# Practical LLM Domain Adaptation: A Comparison of RAG and LoRA
![RAG vs LORA (created by GPT-5)](res/lora_rag.png?raw=true)

<!-- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) -->

## tl;dr

In this small experimental project, I investigate low-cost methods for adapting large language models (LLMs) to specialized domains, addressing a key challenge in applying general-purpose LLMs - such as GPT-4 — in real-world commercial settings, particularly for small or medium-size companies with limited financial resource and technical expertise to invest into LLMs.

Since full fine-tuning of LLMs is prohibitively expensive and often inaccessible to small and medium-sized companies, the project focuses on two alternative domain adaptation techniques: Retrieval Augmented Generation (RAG) and LoRA (Low-Rank Adaptation). The goal is to compare these two approaches in terms of effectiveness and precision in answering questions of a specific target domain. To do this, I use the 'Game of Cthulhu' as a test domain, leveraging its openly available Rulebook as training and retrieval material. I built several RAG- and LoRA-based systems, tasked to answer questions about the game’s rules. I evaluated the systems' performance using a manually crafted dataset of 50 questions, and scored them on Correctness, Topicality, and Fluency.

The results of my experiment, as limited as they are to one toy domain, show that both RAG and LoRA are effective domain adaptation methods that offer clear benefits over general-purpose LLMs for domain-specific tasks, making them a worthwhile investment for small and medium-sized enterprises. However, RAG emerges as the preferred choice due to its better performance (e.g. answer correctness) and ease of setup and maintenance, requiring less specialized expertise compared to LoRA. While LoRA may still be valuable in scenarios demanding deeper integration of domain-specific language and knowledge —such as conversational use of technical jargon— and has very low costs (e.g. ca $2 for fine-tuning a 8B LLM on a small corpus with a A100 GPU) the general recommendation for companies that want to invest into LLMs, is to invest in a RAG-based solution and consider LoRA only when such specific needs arise.


## Disclaimer

This GitHub repository contains all the code required to fully replicate the experiments, along with the datasets used for training, retrieval, and evaluation, the outputs generated by each system, and additional supporting materials.

That said, this remains a small-scale experimental project, and the results should be interpreted with caution. The experiments were limited in scope, focusing on a single, simplified domain and a narrow range of hyperparameter settings. The codebase is not production-ready; it is implemented in simple notebooks, lacks testing, follows no strict coding conventions, and may resemble "spaghetti code" in parts. Additionally, dependencies are not pinned or versioned, which could lead to compatibility issues. However, I have made an effort to document the code thoroughly to support adaptation and reuse. Use at your own discretion!

<!-- Table of Contents -->
## Table of Contents
- [1. Motivation](#1-motivation)
   - [1.1 Retrieval Augmented Generation (RAG)](#11-retrieval-augmented-generation-rag)
   - [1.2 Low-Rank Adaptation (LoRA)](#12-low-rank-adaptation-lora)
- [2. Experimented Systems](#2-experimented-systems)
   - [2.1 Base Models (Baseline)](#21-base-models-baseline)
   - [2.2 LoRA Systems](#22-lora-systems)
     - [2.2.1 LoRA Question Answering (QA) Fine-Tuned Systems](#221-lora-question-answering-qa-fine-tuned-systems)
     - [2.2.2 LoRA Continued Pretraining Systems](#222-lora-continued-pretraining-systems)
     - [2.2.3 LoRA Combined Training](#223-lora-combined-training)
     - [2.2.4 LoRA Stacked](#224-lora-stacked)
   - [2.3 RAG Systems](#23-rag-systems)
     - [2.3.1 Vanilla RAG](#231-vanilla-rag)
     - [2.3.2 RAG with LoRA](#232-rag-with-lora)
- [3. Evaluation Setup](#3-evaluation-setup)
   - [3.1 Evaluation Dataset](#31-evaluation-dataset)
   - [Evaluation Metrics](#evaluation-metrics)
- [4. Evaluation of Experimental Results](#4-evaluation-of-experimental-results)
   - [4.1 Short Qualitative Analysis](#41-short-qualitative-analysis)
   - [4.3 LoRA QA Fine-Tuning vs LoRA Continued Training](#43-lora-qa-fine-tuning-vs-lora-continued-training)
     - [4.3.1 Hyperparameters](#431-hyperparameters)
- [5. Code and Tech Stack](#5-code-and-tech-stack)
- [6. Resource Utilization and Hardware Performance](#6-resource-utilization-and-hardware-performance)
- [References](#references)

## 1. Motivation

The success of LLMs in the last few years is undeniable. However, their concrete usage in commercial settings is still hindered by the fact that foundational LLMs, such as GPT-4, are machineries that posses a 'general' knowledge of the world, but lack knowledge that is 'specialized' to a specific domain. Such domain knowledge is often necessary to make LLM work in commercial settings. LLMs do not posses domain-knowledge because they are trained on vast amount of documents of many different domains. In addition, LLMs have to be retrained whenever one wants to integrate 'new' information, e.g. international news from the lates few days.

The common solution to enable LLMs to acquire domain knowledge is via domain adaptation. The main approach for domain adaptation is to **fine-tune** the LLM using textual documents of the domain as training material. 
For example, an automotive company could fine-tune a LLM using the user manuals of their cars, so that the LLM can start conversing with customers and answer their questions related to a specific model. As another example, a company with complex internal policies could fine-tune a LLM with their policy documents, so that the LLM can conversationally onboard new employees.

However fine-tuning a LLM, i.e. training all the (billions or hundred of billions) of weights of the model, is a very costly and complex process which requires a large number of GPUs and other hardware resources, and multiple teams of expert software engineers and scientists. Only large companies can afford doing that, thus hindering the access to the power of LLMs to small and medium companies.

In addition, in order to fine-tune a LLM, one needs access to the model's architecture and weights. Unfortunately, most of the best-performing LLMs are proprietary, which means that weights and architecture are not available to the general public. There are exceptions, such as the LLama model family from Meta, but this is not the norm. This means that if a company wants to adapt a model to its specific domain, it has to share its domain data with the company owning the LLM. This is unfeasible for many business that want to keep their domain data private.


Two techniques have emerged in the last few years, which enable LLM domain adaptation, without incurring in the large cost of full fine-tuning: Retrieval Augmented Generation (RAG) and LoRA.
These two techniques differ widely on how they work. RAG keeps the LLM model untouched, adding a retrieval component on top of it. LoRA instead alters the LLM architecture and performs further training. However the two techniques have the same goal: allow users to adapt LLMs to a specific domain and to new information in a cost-effective way. This enables a wide set of players (e.g. small and medium companies) to use LLM for their concrete use cases.

The goal of my (small) project is to compare RAG and LoRA as effective solutions for low-cost domain adaptation. I test and compare the two technique on one domain of interest, the Game of Cthulhu. Although this is not the most representative and exemplificative domain for commercial use, the domain provides free accessible documents for LoRA training and RAG retrieval, namely the 'Cthulhu Rulebook', and it is personally a fun domain to apply LLMs to.

Specifically, in this project I did the following:
- I built several RAG-based and LoRA-based systems, which are designed to answer questions regarding the rules of the Game of Cthulhu, that players may have before or during a play session. All systems are built using as input for retrieval/training the Cthulhu Rulebook document.
- I manually constructed a small evaluation dataset, consisting of 50 questions related to the Game of Cthulhu, which can be answered by accessing the Rulebook.
- I evaluated the RAG- and LoRA-based systems on how well they can answer the questions in the evaluation dataset. I used three metrics for the evaluation: Correctness, Topicality and Fluency.

In the rest of this README, I describe more in details the RAG and LoRA techniques. Then, I describe the systems and baseline I built, and the evaluation dataset. Finally, I present the results of the evaluation, both quantitatively and qualitatively.


### 1.1 Retrieval Augmented Generation (RAG)

RAG [1,2] is a technique that allows a LLM to integrate new information (i.e. textual data) that was not contained in the original training data, before answering a user query. The information is integrated at query time, right before answering the query.

A typical RAG agent works as follows. First, the agent analyzes the user textual input and decides if new information is need in order to respond to the user. In such case, the agent searches into a document database containing documents that enclose the new information, and finds the **chunks** (i.e. fragments of text of a pre-defined length) that best match the user query. The agent then builds the context for the LLM, which contains top-k chunks that best match the query, and a special system prompt that encourages the LLM to use these top-k chunks in order to answer the query.

The content of the document database varies depending on the specific goal of the agent. For example, if the agent is supposed to answer queries regarding the latest sport news, it may contain documents collected from the newspapers of the day before. On the other hand, if the agent has to answer queries regarding a company's proprietary knowledge, the database may contain historical internal documents of the company. In all cases, these are documents that the LLM underlying the agent was not pretrained on, and that therefore the LLM would not be able to access if not using a RAG approach.

The chunks of the documents in the database are typically represented as vectors in an embedding space, in order to facilitate and speed-up the retrieval of the top-k chunks. At query time, the user input is also embedded in the space, and the top-k chunks are retrieved using a similarity function between the input vector and that of all the chunks. For this reason the database is called a **vector database**.

RAG agents have several advantages. First, they are a very cost-effective way to integrate new information. The underlying LLM does not need to be finetuned or even retrained, which is typically very costly, especially for very large LLMs. In addition, retraining/finetuning is a very complex process that requires experts knowledge, which is usually not at hand outside big tech companies. Another advantage is that RAG agents use an Information Retrieval (IR) approach to integrate new information. IR is a very established technique used for many years in search engines, which guarantees that the retried top-k chunks are highly precise. Also, RAG agents are relatively simple to implement. 

One of the main disadvantage of RAG agent is that they rely on the LLM context to integrate the new information. The context of an LLM is limited to a defined number of tokens (e.g. GPT3 and GPT4 are limited to respectively 2048 and 8192 tokens). This means that one can only integrate a limited amount of new textual data before answering the user query. In addition, the new information is not integrated 'organically' into the LLM (i.e. during training), but only at the 'last minute': this sometimes results in the LLM having a very shallow understanding of the information and to not be able to truly understand the domain (e.g. incorporating jargon). In addition, RAG systems may be complex to maintain, since they introduce new technical components that may need constant update: namely the vector database, the embedding function and the components needed to integrate the RAG logic. Finally RAG systems usually overfit to the new information, i.e. they rely much more heavily on the new documents than on the data the LLM was trained on, which in some cases poses a problem.

### 1.2 Low-Rank Adaptation (LoRA)

LoRA [3,4,5] is another common approach to integrate new/domain information into a LLM, although it is less popular than RAG.
LoRA in very different from RAG. RAG works at query time and is an external 'addition' to the LLM, i.e. it uses the LLM as-is, without changing it. On the other hand, the core idea of LoRA is to actually 'adapt' the LLM to the new information, by performing an additional step of LLM fine-tuning using the new documents as source.

In short, LoRA works as follow. It introduces new low-dimensional weight matrices, called **adapters**, to the LLM's linear layers, thus essentially extending the LLM's architecture. Before starting the LoRA training, the new documents are collected and preprocessed (i.e. parsed, chunked and tokenized). During LoRA training, only the weights of the adapters are trained with these documents, while the original weights are kept frozen.

LoRA reduces drastically the number of trainable parameters (e.g. of 99%), leading to faster training, lower memory usage and, in turn, lower computation of resources and financial costs. As an example, I was able to fine-tune a LLM with 8 Billion parameters with LoRA on a single cheap GPU on Google Colab within a couple of hours, for a total cost of $2 (see Evaluation section for more details). This opens the possibility of fine-tuning to many more players (small companies, independent researches) than normal fine-tuning, which is very expensive and can only be performed by big tech companies.

Another advantage of LoRA is that it integrates the new information in an 'organic' way, i.e. via training. The hypothesis is that this allow the LLM to truly integrate and more deeply 'understand' the new information. Also, this enables the LLM to understand the jargon of a domain and use it in the answer. For example, for the domain of boardgames, the LLM may use the jargon expression '2D100' instead of 'roll twice a die with 100 faces'.

One of the main disadvantages of LoRA is that every time new documents are available, a new training has to be performed to integrate them. RAG system can instead add the new documents very easily by adding them to the embedding space. Also, one must consider that training a LLM  with LoRA still requires deep expert knowledge, which is expensive to hire or loan for small/medium companies. In addition, LoRA approaches are not always stable, thus requiring careful tuning of the hyperparameters before updating a model.

Another major disadvantage of LoRA, is that it needs access to the architecture and weights of the LLM. This strongly limits the choice of LLM that one can use, since only very few companies share this information publicly.

Finally, once the LoRA adapters are trained, one needs to find an online service or build an on-prem solution to serve the model.



## 2. Experimented systems

I experimented with the following LLM-based systems.

### 2.1 Base models (baseline)
I used two publicly available base LLM as baselines: _Llama-3.1-8B-Instruct_ and _Llama-3.2-1B-Instruct_. Since these models are not fine-tuned to the domain of Game of Cthulhu, they should provide poor performance and serve as a starting point for comparing the other systems. In the experiments the two models are referred respectively as `base-8B-full` and `base-1B-full`

### 2.2 LoRA systems
I experiment with different types of LoRA adapters, and with several values of LoRA hyperparameters, as follows.

#### 2.2.1 LoRA Question Answering (QA) fine-tuned systems
These are systems where the base LLM is augmented with LoRA adapters fine-tuned using a dataset of Question-Answer pairs. The dataset is created semi-automatically using the [Meta Synthetic Data Kit (MSDK)](https://github.com/meta-llama/synthetic-data-kit/tree/main/synthetic_data_kit) applied to the Cthulhu Rulebook. The dataset consists of 11,930 QA pairs, e.g.:
```
Question: "Who takes the role of game moderator in Call of Cthulhu?",
Answer: "One player takes the role of game moderator, known as the Keeper of Arcane Lore."

Question: "What happens when establishing a contact with a non-player character?"
Answer: "A successful roll means the non-player character has heard of the investigator by reputation or has previously met the contact."
```

Using a QA dataset for LoRA fine-tuning is the typical way to apply the LoRA technique. This is often referred as Instruction Fine-Tuning, since we are teaching the LLM to answer to specific instruction (i.e. questions) posed by the user. The hypothesis is that by providing QA pairs, the LLM simultaneously learns the new domain and retains (or acquires) the capacity to react to user questions.

I experimented with two base LLMs: _Llama-3.1-8B-Instruct_ and _Llama-3.2-1B-Instruct_. During LoRA fine-tuning, both LLMs were quantized to 4-bit, so that the process could run smoothly on a single A100 GPU. This is to mimic the setup that one could expect in a small/medium-size company.

I used the following setup for LoRA, following recommendations from the literature [3, 6, 7, 8]:

- **LoRA rank _r_**. This is the dimension of the matrixes of the LoRA adapters. A higher rank allows more capacity and hence increases accuracy, but uses more memory. I experimented with rank values of 16 and 64, to verify this hypothesis. I did not experiment with higher rank values as it uses too much memory.
- **LoRA α**. This is the scaling factor. A higher scaling factor amplifies the effect of LoRA fine-tuning. A lower value pushed the model to rely more on the original parameters of the base model. I fixed α to 32.
- **rLoRA**. I experimented with rLoRA always active. rLoRA takes the square root of the rank when merging the LoRA adapters, instead of the rank, which brings more stability to the fine-tuning process
- **LoRA layers**. I added LoRA adapters to all the linear layers of the base model, including both attention and FFN layers. In the transformer architecture these are: q_proj, v_proj, k_proj, o_proj (attention) and gate_proj, down_proj, up_proj (FFN).

In the experiments, these systems are referred as `lora-qa-full` and `lora-qa-quan`, respectively when the base model is used in its non-quantized or 4-bit-quantized form at inference time. Unless indicated otherwise, these are system on top of _Llama-3.1-8B-Instruct_.

The code that implement these systems is in notebook `2_lora_qa_finetuning.ipynb`. Please refer to this notebook for more information regarding the QA dataset and the setup of the systems.

#### 2.2.2 LoRA Continued Pretraining systems
These are systems where the LoRA adapters are trained using a raw textual document, instead of QA pairs. This is similar to how foundational models are typically pretrained, hence the name 'Continued Pretraining'. The advantage of this approach is that the domain knowledge is explicitly and naturally described in the text, instead of being artificially summarized in a dataset of QA pairs. This should lead the model to better learn the domain. However, there is a risk that after this additional training phase (which does not contain QA pairs),the model has 'forgot' how to answer questions.

The training dataset consists of the raw text of the Cthulhu Rulebook, which is made of 466,075 tokens. The book is split into  chunks of 256 tokens. Each chunk constitute a training example. Chunks have a high overlap (I set the stride to 32 tokens), in order to ensure that all the semantic content of the Rulebook is considered during training. Here are the first two chunks of the dataset, showing the high overlap:
```
Originally written by 
Sandy Petersen
With later revision by 
Lynn Willis
This revised 7th Edition is a collaboration between 
Paul Fricker and Mike Mason
Editorial: Scott Dorward, Badger McInnes, Mike Mason, Charlie Krank
Design Format: Badger McInnes and Mike Mason
Layout: Badger McInnes, Nicholas Nacario, Charlie Krank
Art Direction: Mike Mason, Meghan McLean, Daniel Skomorowski
Cover Illustration: Sam Lamont
Chapter Illustrations: Jonathan Wyke, Paul Carrick, Rob Gould, François Launet, Victor Leza,  
Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz
Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  
Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte
Cristoforo Font created by: Thomas Phinney
Cartography: Steff Worthington
For Lynn Willis
Acknowledgements 
The authors would like to thank the following people for their 
ongo
```
```
Paul Fricker and Mike Mason
Editorial: Scott Dorward, Badger McInnes, Mike Mason, Charlie Krank
Design Format: Badger McInnes and Mike Mason
Layout: Badger McInnes, Nicholas Nacario, Charlie Krank
Art Direction: Mike Mason, Meghan McLean, Daniel Skomorowski
Cover Illustration: Sam Lamont
Chapter Illustrations: Jonathan Wyke, Paul Carrick, Rob Gould, François Launet, Victor Leza,  
Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz
Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  
Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte
Cristoforo Font created by: Thomas Phinney
Cartography: Steff Worthington
For Lynn Willis
Acknowledgements 
The authors would like to thank the following people for their 
ongoing  support  and  assistance:  Charlie  Krank,  Christian 
Grussi, Keary Birch, Alan Bligh, John French, Scott
``` 

As I did for LoRA QA fine-tuned systems, I here also experimented with the two base LLMs _Llama-3.1-8B-Instruct_ and _Llama-3.2-1B-Instruct_, which were quantized to 4-bit during training. 
The LoRA setup is also similar. The only difference is that I experimented with two different sets of LoRA adapters. The first set contains all linear layers. The second set includes all linear layers and also the head of the model, and the initial embedding layer. This latter setup is indicated in the literature to be beneficial for LoRa Continued Pretraining [9].

In the experiments, these systems are referred as `lora-cont-full` and `lora_cont_head-full`, respectively when adapters are applied only to the linear layers, or to the linear layers, the head and the embeddings.  Also, I indicate with `lora-cont-quan` and `lora_cont_head-quan` when I 4-bit quantize the base model at inference time. Unless indicated otherwise, these are system on top of _Llama-3.1-8B-Instruct_.

The code that implement these systems is in notebook `3_lora_cont_pretraining.ipynb`. Please refer to this notebook for more information regarding the QA dataset and the setup of the systems.

#### 2.2.3 LoRA combined training
These are systems where the base LLM is augmented with LoRA adapters fine-tuned using a mix of the Question-Answer pairs and the raw textual document of the Rulebook. In essence, it is an attempt to combine the two LoRA approaches explained before.
The hope is that by training on a combined dataset we can get the best of both worlds. From the one side, the model has a good chance to truly and fully learn domain knowledge and domain jargon (Continued Pretraining). From the other side, the model retains the capability to answer questions on that domain (Instruction Fine Tuning).

Specifically the dataset to train the LoRA adapters consists of the raw textual dataset, to which I appended 2,000 random QA pairs from the QA dataset.

As before, I experimented with the two base LLMs _Llama-3.1-8B-Instruct_ and _Llama-3.2-1B-Instruct_, which were quantized to 4-bit during training. 
The LoRA setup is identical to the setup for the QA-based systems.

In the experiments, these systems are referred as `lora_combo-full` and `lora_combo-quan`.
The code that implements these systems is in notebook `4_lora_combined_training.ipynb`. Please refer to this notebook for more information regarding the dataset and the setup of the systems.

#### 2.2.4 LoRA stacked
Similarly to the 'LoRA combined' system, the LoRA stacked system also aims to combine the QA Fine-Tuning and Continuous Pretraining approaches. However, instead of mixing the datasets and then perform training, we here take the already trained adapters from the two approaches, and simply sum their weights to obtain the final adapter, hence the name 'stacking'. This is a pretty common technique used in LoRA to merge the semantics of different adapters into one adapter [10, 11]. This approach has been mostly used for vision application, but experiments for textual data have also been performed in the literature.

I experimented two setups. These setups should theoretically lead to the same answers. However I wanted to double check that the technical implementation in the Hugging Face library was sound:

  - **Base + qa + continued** \
First merge the QA Adapter (i.e. the adapter tuned with the QA dataset) to the base mode, and then add the Continued Adapter (i.e. the adapter trained with the Continued Pretrained dataset) on top.

  - **Base + continued + qa** \
First merge the Continued Adapter to the base mode, and then add the QA Adapter on top.

In the experiments, the above two systems are referred  as `lora_stack-full` and `lora_stack-quan`.
The code that implement these systems is in notebook `5_evaluation.ipynb`. Please refer to this notebook for more information.

### 2.3 RAG systems

I experimented with two RAG systems, as below.


#### 2.3.1 Vanilla RAG

A simple RAG agent provided with the Cthulhu Rulebook as knowledge base.

The vector database supporting the agent is created as follows:
- I split the Cthulhu Rule Book into chunks of 1000 characters, overlapping of 100 characters.
- I generated an embedding for each chunk using the [jinaai/jina-embeddings-v3](https://huggingface.co/jinaai/jina-embeddings-v3) embedding model. This model is known to perform well on RAG tasks.
- I stored the embeddings in a [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) in-memory vector database

At inference time, the system answers questions using the following typical RAG approach:
- the question is projected in the embedding space, and the top-5 most similar chunks from the vector database are retrieved.
- A LLM is invoked to generate the answer. The LLM is provided a typical RAG context as input, which includes:
  1. a system prompt that recommends the model to base the answer on the top-5 chunks
  2. a user prompt that contains the text of the top-5 chunks

I used `Llama-3.1-8B-Instruct` as LLM in all the experiments.

In the experiments, the above system is referred as `base-full-rag`. The code that implement this system is in notebook `5_evaluation.ipynb`. Please refer to this notebook for more information.


#### 2.3.2 RAG with LoRA

This is an extension of the above RAG system, where the LLM used to answer the question is not a base LLM, but a LLM augmented with any of the LoRA adapters presented before. Specifically I experimented with the following adapters:

- 'LoRA QA fine-tuning' adapters. These systems are referred in the experiments as `lora_qa-full-rag` and `lora_qa-quan-rag`.
- 'LoRA Continued Pretraining' adapters, where only the linear layers are adapted. These systems is referred in the experiments as `lora_cont-full-rag` and  `lora_cont-quan-rag`.
- 'LoRA combined' adapters, where only the linear layers are adapted. These systems are referred in the experiments as `lora_combo-full-rag` and `lora_combo-quan-rag`. 
- 'LoRA stacked' adapters, where only the linear layers are adapted. These systems are referred in the experiments as `lora_stack-full-rag` and `lora_stack-quan-rag`. 

The code that implement these systems is in notebook `5_evaluation.ipynb`. Please refer to this notebook for more information.

## 3. Evaluation setup

### 3.1. Evaluation dataset
I evaluate the systems on a dataset of 50 QA pairs manually derived from the Cthulhu Rulebook. The dataset reflects typical questions that a player could ask about the game, and varies from questions that require a precise answer to open-eneded questions that require the system to reason and summarize the content of the book. The answers provided in the dataset are our gold standard to evaluate the systems.

Examples:

```
{'question': 'Who wrote the Necronomicon chapter?',
 'answer': 'Keith Herber',
 'chapter': 1}

{'question': 'Can you provide a short description of how a Chase takes place',
 'answer': 'A chase take place when the participants have an escape route. The Keeper positions the participants, decided the order of play and how many movements each participant can make in each turn. At the beginning each participant makes a CON roll to adjust their MOV rating. Then, at each round of the chase the participants act in DEX order, and move a number of locations based on their MOV rating. In some cases attacks can also be performed. The Keeper can add hazards and barriers to the chase as well.',
 'chapter': 7}
```
The systems are not required to perfectly match the provided gold standard answer, but should get as close as possible.

## Evaluation metrics

Each answer provided by a system, is manually evaluated on three metrics: _Correctness_, _Topicality_ and _Fluency_ [12].
For each metric a score from 1 to 5 is assigned, where 5 is best and 1 is worst. To compare the systems, I take the average of the scores across all 50 questions in the evaluation dataset. Here are the details on the metrics:

**Correctness [1-5]** \
Correctness refers to how closely the generated text answers the question at hand.
1. The question is not answered at all.
2. The answer is on topic, but largely incorrect.
3. The question is not fully and correctly answered, major aspects are missing or incorrect.
4. Correctly answer most of the question, but some minor aspects are missing or incorrect.
5. Fully and correctly answers the question

**Topicality [1-5]**\
Topicality refers to how close the generated text is to the topic of the question at hand.
1. The answer does not refer to boardgames
2. The answer refers to boardgames
3. The answer refers to the game of Cthulhu
4. The answer refers to the same area of interest of the game of Cthulhu
5. The answer refers to the topic of the question

**Fluency [1-5]**\
Fluency evaluates how natural and grammatically correct the generated text is. A fluent text follows the conventions of the target language, making it easy for humans to read and understand. Specific aspects include: grammaticality, non-redundancy, referential clarity (e.g. pronouns), structure, coherence.
1. Very Poor
2. Poor
3. Barely Acceptable
4. Good
5. Very Good

Note that instead of manually evaluating each of the 50 answer generated by each system, which is very costly, I could have applied automatic metrics which measure the similarity between the generated answer and the gold standard answer, e.g. BLEU and ROUGE [13, 14]. Unfortunately these metrics are very generic (e.g. they look at word overlaps) and are not specific enough do provide meaningful insights of the systems. In the future, one could explore using the LLM-as-a-judge approach, instead of manually scoring.

## 4. Evaluation of experimental results 

The table below shows the results of some of the best performing systems. All systems, apart from `base-1B-full` use _Llama-3.1-8B-Instruct_ as base LLM. In order to keep results as comparable as possible, I set the generation strategy to Greedy, i.e. during generation the model always selects the next most likely token at each step.

| System|Correctness|Topicality|Fluency|
|---|---|---|---|
|base-8B-full (baseline) |1\.44|2\.14|**4\.96**|
|base-1B-full (baseline) |1\.40|2\.02|4\.84|
|base-full-rag        |**4\.00**|**4\.92**|**4\.96**|
|lora\_cont-full-rag  |3\.00|4\.72|4\.74|
|lora\_stack-quan-rag |2\.60|4\.74|4\.00|
|lora\_cont-full      |2\.48|4\.34|4\.76|
|lora\_qa-full         |2\.34|4\.90|4\.82|
|lora\_cont\_head-full|2\.22|3\.82|4\.72|
|lora\_combo-full     |2\.20|4\.84|4\.72|
|lora\_stack-full     |2\.00|4\.78|4\.56|

The table shows that `base-full-rag`, the Vanilla RAG system which uses _Llama-3.2-8B-Instruct_ as LLM, without any LoRA adaptation, is the clear winner of the experiment. It is the only system to achieve high scores across all three metrics. Specifically, the results on Correctness are statistically significantly better than all other systems, using a Wilkoxon sign-rank test [16].

From the one side, these results indicate that the embedding-retrieval component of the RAG system is able to successfully capture the semantics of the domain in the embedding space, and thus always return top-5 chunks which are useful to answers the user question. From the other side, this confirms that a simple RAG prompt containing the retrieved chunks is a sufficient solution to 'guide' the LLM to generate the answer from the context text rather than its historical pretrained knowledge.

The performance of the baseline system, `base-8B-full`, confirms this observation. It achieves 1.4 in Correctness and 2.02 in Topicality. This indicates that a plain general-purpose LLM like _Llama-3.2-8B-Instruct_ is not able to answer domain-specific questions, because its pretrained knowledge base is too general and unfocused.

LoRA-based systems outperform the baseline in Correctness and Topicality, but they perform poorly when compared to RAG-systems. The best LoRA system is `lora_cont-full`, i.e. the LoRA Continued Training system using an unquantized base model at inference time. It achieves an average Correctness score of 2.48, meaning that the answers are on topic, but are usually completely or partially incorrect. Other LoRA-based systems achieve similar levels of Correctness. The main causes of errors are misinterpretation of the question and hallucination in the answer (see qualitative analysis for examples).

LoRA-based system achieve however good scores in Topicality, indicating that the answers usually focus on the topic of the question, or at least the same area of interest. The same stands for Fluency: most of the LoRA-based system produce text that is clear and coherent.

Mixing RAG and LoRA does not bring any advantage. On the contrary RAG-systems using LoRA as LLM surprisingly perform worse than the Vanilla RAG system, across all metrics and especially on Correctness. 
Specifically, `lora_cont-full-rag` drops 1 point of Correctness with respect to Vanilla RAG (from 4.0 to 3.0). `lora_stack-quan-rag` drops even more, to 2.6. 

Regarding the use of quantized or unquantized base models at inference time, I did not notice any significant difference. Therefore, results for quantized base models are here not reported.

The overall conclusion of the experiment is positive. Both domain adaptation methods —RAG and LoRA— prove to be effective, offering clear advantages over a general-purpose LLM when applied to domain-specific tasks. These findings suggest that it could be worthwhile for small and medium-sized enterprises to invest modest resources in adapting off-the-shelf LLMs to their specific domains for internal use.

Between LoRA and RAG, RAG stands out as the clear winner in terms of performance. Additionally, one has to consider that RAG systems are generally easier to set up with respect to LoRA: machine learning experts are needed to train LoRA adapters, while setting up a RAG system can be easily performed by a software engineer. Also, the primary ongoing maintenance of RAG system involves updating the document embeddings when new content is added, a process that can often be automated. In contrast, LoRA requires periodic retraining of adapters, which is more resource-intensive, and setting up a platform to serve the LLM.

Therefore, the overall recommendation for end-users of LLMs is to begin with a RAG-based solution. LoRA should only be considered if specific requirements arise that RAG cannot meet. For example, the need for a model to engage in conversations using domain-specific jargon, where LoRA may have an advantage.

As a final note, it is important to interpret these results with caution. The experiment was conducted within a limited scope, focusing on a single, simplified domain and exploring only a narrow set of hyperparameter configurations.

### 4.1. Short qualitative analysis

Let's look at how `base-8B-full`, `base-full-rag` and `lora_cont-full` answered some of the questions, since these are the top-performing systems in their respective groups, namely baselines, RAGs and LoRAs.\
Remember that these three systems are based on the same base LLM, i.e. _Llama-3.2-8B-Instruct_, thus offering a fare comparison.

**_Question: 'How many dices are needed at a minimum to play the game?'_**

| system | answer|
| -------- | ------- |
| base-full | 'To play the game of dice, you need at least one die. However, most common versions of dice games require two dice.' |
| base-full-rag | 'According to Document 0, a minimum of 5 dice are needed to play the game: a D100, a D4, a D6, an D8, and a D20.' |
| lora_cont-full | 'The  minimum  number  of  players  needed  to  play  many  versions of this game is 2. However, with the various options  and scenarios available, games can be played with as many  as  6  or  more  players [...]' |
| -------- | ------- |

The example shows a typical failure case for `base-8B-full`, where the LLM answers out of topic. Although in this specific scenario this could be corrected by rephrasing the question to _'How many dices are needed at a minimum to play the Game of Chtulhu?'_, I decided not to do that to simulate the fact that a general LLM does not typically have access to domain knowledge. \
The RAG system provides the correct answer, also reporting which chunk it used to construct the answer, which is useful in practical settings where a user wants to have clear evidence that the LLM answer is correct and not a hallucination. \
The LoRA system, while staying on the right topic, fails to understand the question.


**_Question: 'What is a speed roll used for during a chase?'_**

| system | answer|
| -------- | ------- |
| base-full | '"A speed roll is a technique used in various forms of martial arts, particularly in grappling and wrestling, as well as in some combat sports like Brazilian Jiu-Jitsu and submission wrestling. It's also used in some action movies and video games.[...]' |
| base-full-rag | 'A speed roll is used to adjust each participant's MOV (movement) rating for the duration of the chase. It is made by the participants to test their physical condition and endurance (for characters on foot or other self-propelled mode) or their handling of the vehicle (for drivers). The roll determines the MOV rating for the duration of the chase, with success resulting in no change, extreme success resulting in a +1 to MOV, and failure resulting in a -1 to MOV.' |
| lora_cont-full | '"In a chase, a speed roll is used to determine the order of movement. All participants in the chase make a skill roll: CON (for characters) or Drive Auto (for vehicles). Those that roll an Extreme, Hard, or Regular success are allowed to move normally. Those that fail to achieve a level of success are not allowed to move until their next action.  [...]' |
| -------- | ------- |

The example shows again that `base-8B-full` goes out of topi, while the RAG system is able to retrieve and summarize the correct answer.\
Most interestingly, it seems that the LoRA systems, while staying on topic, it hallucinates, making up a wrong answer, despite the fact that the correct answer was present in the training dataset. This may be due to incomplete exposure to the training data (e.g. more epochs of training may be required, or a chunking with smaller stride), or to suboptimal hyperparameter setup (e.g. a higher LoRA ratio may improve the results). Overall, the majority of the failure cases for LoRA-based systems are due similar types of hallucinations, or to a misinterpretation of the question, as shown in the previous example.

**_Question: 'What is the Mythos rating of the Celaeno Fragments?'_**

| system | answer|
| -------- | ------- |
| base-full | 'I couldn't find any information on the ""Celaeno Fragments"" in relation to a specific rating system called ""Mythos."" However, I can provide some context. Celaeno is a figure from Greek mythology, [...]' |
| base-full-rag | 'There is no information about the Celaeno Fragments in the provided context.' |
| lora_cont-full | 'I'm not familiar with the Celaeno Fragments. Can you please provide more information or context regarding this book? That way I can try to help you.' |
| -------- | ------- |

This is an example where all systems fail to provide the correct answer. The failure of the RAG system is likely due to the fact that the Rulebook provides this information in a table-like structure, which is lost when converting from PDF to text. Since more than one examples fail due to this reason, this indicates that better methods should be used than simple PDF-text conversion, when domain documents have rich structure. For example one could use image-base models or ad-hoc table interpretation heuristics.

### 4.3. LoRA QA Fine-Tuning vs LoRA Continued Training

The table below shows a comparison of the two LoRA approaches with different parametrizations.

| System | r | α | batch | lr | Correctness | Topicality | Fluency |
|---|---|---|---|---|---|---|---|
|lora_qa-full       | 16 | 32 | 32 | 1e04 |2\.34|4\.76|4\.72|
|lora_qa-full       | 16 | 32 | 16 | 1e04 |2\.34|4\.90|4\.82|
|lora_qa-full       | 64 | 32 | 16 | 1e04 |2\.32|4\.90|4\.82|
|lora_cont-full     | 64 | 32 | 32 | 1e04 |2\.48|4\.34|4\.76|
|lora_cont_head-full| 64 | 32 | 32 | 1e04 |2\.22|3\.82|4\.72|

The results show that the two approaches (LoRA QA Fine-Tuning and Lora Continued Training) perform similarly. Correctness and Fluency results have small variations, but these are not statistically significant. The only exception is Topicality, where the QA approach is statistically better than Continued Training.

Despite quantitative results are similar, a closer inspection shows that LoRA QA provides much shorter and concise answers when compared to LoRA Continued Training. This is expected, since in LoRA QA we specifically fine-tune the base model to answer questions in a concise way. Instead, in LoRA Continued Training we perform plain pretraining with no specific task in mind. Here are a few examples:

**_Question: 'In which situation is a character allowed to escape a close combat?'_**

| system | answer|
| -------- | ------- |
| lora_qa-full| 'A character is allowed to escape a close combat if their opponent is incapacitated or they have an Extreme advantage.' |
| lora_cont-full | 'A character is allowed to escape a melee attack (close combat) on a  subsequent round if they successfully dodge or fight back. If the character dodges, they may do so on their normal  Dodge skill of 25%. A successful dodge allows the character  to avoid an attack and gain a fighting maneuver of their own on their next action. If  the  character  fights  back,  they  may  do  so  using  a  weapon or a basic fighting maneuver. The target of a fight-ing maneuver may fight back using a basic fighting maneuver of their own. Both sides roll percentile dice and compare them to their Fighting skill. The higher skill wins, just like in a regular fighting attack. If the target is holding a weapon, this may be used to fight back, or a basic Fighting skill can be used. A character can use a weapon to fight back through the fighting maneuver as they can when dodging a melee attack [...]'

**_Question: 'In which of Lovecraft's stories is featured the Miskatonic University?'_**

| system | answer|
| -------- | ------- |
| lora_qa-full| 'in Lovecraft's story 'The Dunwich Horror' (1928)' |
| lora_cont-full | 'The Miskatonic University is featured in many of H.P. Lovecraft’s stories, however it first appears in ""The Picture in  the  House""  (1920),  and  also  in  ""Herbert  West—Reanimator""  (1921-1922). It is in ""The Call of Cthulhu"" (1926) that the Miskatonic  University  is  most  frequently  mentioned.  Other stories  to  feature  the  university  include  ""The  Case  of  Charles Dexter Ward"" and ""The Dreams in the Witch House"". The Miskatonic University is an ancient and (in the modern era) excellent school of learning, with a faculty and student body of over a thousand. Its library is said to be one of the great repositories of tomes in the United States. The university’s collection of manuscripts is second only to that of the Widener Library at Harvard. The university’s collection is strong in rare books as well, with many volumes of the Renaissance era [...]"

#### 4.3.1 Hyperparameters

Surprisingly, the choice of different LoRA hyperparameters does not seem to make a difference. For example, I would have expected that increasing the LoRA α from 16 to 64 would boost Correctness, but quantitative results for LoRA QA suggest otherwise. However, at a closed inspection, I noticed that systems with α=16 tend to at times generate gibberish, while this does nor happen with α=64, for example:

**_Question: 'In which of Lovecraft's stories is featured the Miskatonic University?'_**

| system | answer|
| -------- | ------- |
| lora_qa-full α=64 | 'in Lovecraft's story 'The Dunwich Horror' (1928)' |
| lora_qa-full α=16 | 'The Dunwich Horror, The Shadow Over Innsmouth, The Shadow Out of Time, The Dreams in the Witch House, The Haunter of the Dark, The Colour Out of Space, The Curse of Yig, The Horror at Red Hook, The Lurking Fear, The Picture in the House, The Shadow Over Innsmouth, The Shadow Out of Time, The Dreams in the Witch House, The Haunter of the Dark, The Colour Out of Space, The Curse of Yig, The Horror at Red Hook, The Lurking Fear, The Picture in the House, The Shadow Out of Time, [...]' |

Regarding the use of LoRA adapters in the head and embedding layers when using Continued Training, results seem to contradict previous observations [9], as this is detrimental to both Correctness (without statistical significance) and Topicality (with statistical significance).

## 5. Code and Tech Stack

The code that implements the systems and the evaluation is in the `./notebooks` folder of this repo. Specifically:
- `1_synt_dataset_generation.ipynb`: creation of the QA dataset using the Meta Synthetic Data Kit, for training the 'LoRA QA Fine-Tuning` systems.
- `2_lora_qa_finetuning.ipynb`: fine-tuning of the 'LoRA QA Fine-Tuning` systems.
- `3_lora_cont_pretraining.ipynb`: training of the 'LoRA Continued Training` systems.
- `4_lora_combined_training.ipynb`: training of the 'LoRA Combined Training` systems.
- `5_evaluation.ipynb`: Answer generation for all systems; creation of evaluation dataset; system evaluation.

Please keep in mind that this is a small-scale and experimental project. The code is very far from being production-ready. There is no testing, no attempt to organize the code in a sensible way, no style checks, etc.

Other material in this repo:

- `/config` \
contains the configuration file for the Meta Synthetic Data Kit.
- `/data` \
contains the Cthulhu Rulebook (PDF in `/data/pdf`, text-converted in `/data/output`), and the QA dataset (json format in `/data/generated`, ChatML format in `data/generated-final`).
- `/evaluation/evaluation-dataset` \
contains the evaluation dataset of 50 QA pairs. 
- `/evaluation/evaluation-results` \
contains the answers generated by all systems using at inference time a non-quantized base model.
- `/evaluation/evaluation-results-quantized` \
contains the answers generated by all systems using at inference time a 4-bit quantized base model.
- `/evaluation/manual-annotation` \
contains the manual scoring of the answers generated by the systems.
- `/fine-tuning/profiling` \
memory profiles for some of the LoRA training runs.


I used the following tech stack:
- Python
- [Google Colab](https://colab.research.google.com/) as platform to develop and run the code. For resource intensive tasks I relied on a single L4 or a single A100 GPUs.
- [Llama language models](https://www.llama.com/) as the base LLMs throughout the project.
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) to load and quantize base models, to perform fine-tuning and to generate answers.
- [BitsAndBytes](https://huggingface.co/docs/bitsandbytes/main/en/index) for quantization.
- [Hugging Face Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/en/index) to configure Lora models.
- [Hugging Face Hub](https://huggingface.co/docs/huggingface_hub/en/index) to store the Lora models checkpoints.
- [VLLM](https://docs.vllm.ai/en/latest/) to serve the above LLM to the MSDK.
- [Langchain](https://www.langchain.com/) to implement the RAG agents.
- [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as in-memory vector database for RAG.
- [Meta Synthetic Data Kit (MSDK)](https://github.com/meta-llama/synthetic-data-kit/tree/main/synthetic_data_kit) to generate the QA dataset.


## 6. Resource utilization and hardware performance


For all resource intensive tasks, e.g. training and generation, I used Google Colab with either one L4 GPU or one A100 GPU with 40 GB of VRAM. 

Here below I report statistics regarding resources utilized for training some of the LoRA models.

| Model | Parameters | GPU | Time taken | Memory usage (GB) | Google Compute Units | Cost ($) |
| ------- | -------- | ------- |  -------- | -------- |  -------- | -------- |
| lora_qa-full (1B) | lr=0.0001, b=64, r=16, α=32  | A100 40GB | 25m   | NA | NA | NA |
| lora_qa-full (8B) | lr=0.0001, b=16, r=16, α=32  | A100 40GB | 2h25m | pre-trained model: 7.2<BR> LoRA adapters: 0.16<BR>training (excl model): 13.8  | 16 | $1.6 |
| lora_qa-full (8B) | lr=0.00003, b=32, r=16, α=32 | A100 40GB | 3h11m | pre-trained model: 7.2<BR> LoRA adapters: 0.16<BR>training (excl model): 7.1 | 22 | $2.2 |
| lora_cont-full (8B) | lr=0.0001, b=32, r=64, α=32 w head layer | A100 40GB | 2h50m | pre-trained model: 7.2<BR> LoRA adapters: 0.68<BR>training (excl model): 17.6 | NA | NA |
| lora_cont-full (8B) | lr=0.0001, b=32, r=64, α=32 w/o head layer | A100 40GB | 2h45m | pre-trained model: 7.2<BR> LoRA adapters: 0.64<BR>training (excl model): 16.8 | 19 | $1.9 |
| lora_combo-full (8B) | lr=0.0001, b=32, r=64, α=32 w/o head layer | A100 40GB | 3h08m | pre-trained model: 7.2<BR> LoRA adapters: 0.63<BR>training (excl model): 17.1 | NA | NA |

## References

RAG:
- [1] https://research.ibm.com/blog/retrieval-augmented-generation-RAG
- [2] https://huggingface.co/learn/agents-course/en/unit3/agentic-rag/agentic-rag

LoRA:
- [3] https://arxiv.org/abs/2106.09685
- [4] https://huggingface.co/docs/peft/main/en/conceptual_guides/lora
- [5] https://medium.com/@kednaik/llm-fine-tuning-with-lora-8e06f2227183
- [6] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide
- [7] https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/
- [8] https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2
- [9] https://unsloth.ai/blog/contpretraining
- [10] https://arxiv.org/abs/2411.14064
- [11] https://huggingface.co/docs/diffusers/main/en/using-diffusers/merge_loras 

Evaluation metrics:
- [12] https://medium.com/@sumit.somanchd/
- [13] https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation
- [14] https://medium.com/@sumit.somanchd/testing-evaluating-large-language-models-llms-key-metrics-and-best-practices-part-2-0ac7092c9776
- [15] https://arxiv.org/pdf/2007.12626

Statistical testing
- [16] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
