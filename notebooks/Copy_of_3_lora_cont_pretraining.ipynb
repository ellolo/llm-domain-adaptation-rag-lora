{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1MbBTBbK0GVw",
        "GXuqwV0htgoT",
        "UdDMKV15VZao",
        "bo5SLwuZVcYM",
        "pA4MXwIdxiaq"
      ],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM03TfAu5AqpE3JWyFFlEyW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellolo/cthulhu_fine_tuning/blob/main/notebooks/Copy_of_3_lora_cont_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Continued Pretraining\n",
        "\n",
        "This notebook performs [Continued Pretraining](https://arxiv.org/pdf/2405.09673) of `Llama-3.1-8B-Instruct` and `Llama-3.2-1B-Instruct` on a domain-specific raw text dataset using [Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685). The goal is to obtain a fine-tuned model that answers user questions on that domain.\n",
        "\n",
        "In this noebook, I use the Chtulhu game as the target domain, and the Chtulhu Rulebook as textual fine-tuning dataset. The idea is to use the fine-tuned model while playing the Game of Chtulhu, to answer players' questions regarding the rules of the game.\n",
        "\n",
        "## Continued Pretraining vs. Instruction (QA) fine-tuning\n",
        "\n",
        "The typical approach to adapt a base model to answer questions related to a target domain is to perform Question Answering fine-tuning, or, more boradly Instruction Fine Tuning (IFT, see [this paper](https://arxiv.org/pdf/2405.09673) for a survey): the base model is fed a dataset of Question Answering pairs related to the domain, thus learning to answer similar questions. In addition, in order to spare memory, this fine-tuning is performed using Lora. I have implemented this approach in `2_lora_qa_finetuning.ipynb`.\n",
        "\n",
        "A different approach, presented in this Notebook, is to feed the base model with raw text documents that enclose the domain knowledge. The advantage is here that the domain knolwedge is explicitly and naturally described in the text, instead of being artifically summarized in a dataset of QA pairs, as done in IFT. This should lead the model to better learn the domain. However, there is a risk that after this additional training phase (which does not contain QA pairs),the model has 'forgot' how to answer questions.\n",
        "\n",
        "\n",
        "## Stack\n",
        "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) to load and quantize the base models, and to perform fine-tuning\n",
        "- [BitsAndBytes](https://huggingface.co/docs/bitsandbytes/main/en/index) for quantization.\n",
        "- [Hugging Face Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/en/index) to configure the Lora model.\n",
        "- [Hugging Face Hub](https://huggingface.co/docs/huggingface_hub/en/index) to store the Lora models checkpoints.\n",
        "\n",
        "I recommend to use a machine with at least a **A100** Nvidia GPU. Although Lora is designed to save VRAM, a decent GPU is stil needed.\n",
        "\n",
        "## Details\n",
        "At a high-level, I do the following (more details are provided in the rest of the notebook):\n",
        "\n",
        "1. Install dependencies and clone this github repo. I use the repo to read the Chtulhu Rulebook, and also to store performance data of the fine-tuning process.\n",
        "2. Load the base model (either `Llama-3.1-8B-Instruct` or `Llama-3.2-1B-Instruct`) and quantize it to 4-bits to save VRAM. When quantized, these models are small enough to perform Lora fine-tuning on a A100 GPU.\n",
        "3. Setup and configure the Lora model. This means that we create low-dimensional Lora adapters on top of specific layers of the base model, and fine-tune the weights of these adapters, leaving the weights of the original base model untouched, thus saving time and VRAM. I add Lora adapeters to all linear layers of the Llama models (both attention and FFN layers) including the head and the initial embedding. I set Lora rank to 64, Lora α to 32, and always activate rLora.\n",
        "4. Prepare the dataset for training. The dataset consists of **chunks** of 256 tokens extracted from the Chtulhu Rulebook. More details are provided in the following sections.\n",
        "5. Fine-tune the Lora model. I track the training loss as the main metric. I also track and store VRAM snapshots to monitor the memory usage."
      ],
      "metadata": {
        "id": "59h8yaqcSGfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install dependencies and clone github repo\n",
        "\n",
        "\n",
        "The github repo contains the dataset for fine-tuning."
      ],
      "metadata": {
        "id": "1MbBTBbK0GVw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86dv3HdX6xax"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install -U 'transformers[torch]' peft datasets bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get HF token from secrets and login in HF hub, so that we can download models from HF Hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN=userdata.get('HF_TOKEN_WRITE')\n",
        "if HF_TOKEN:\n",
        "  login(HF_TOKEN, new_session=True)\n",
        "else:\n",
        "  login()"
      ],
      "metadata": {
        "id": "V8q9TEI668wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone github repository.\n",
        "\n",
        "# Past here the SSH key stored on my personal laptopat: ~/dev/llm_cthulhu_fine_tuning/keys\n",
        "# This is not safe, but unfortunaly using Colab secrets did not work.\n",
        "\n",
        "! mkdir -p /root/.ssh\n",
        "with open(\"/root/.ssh/id_rsa\", mode=\"w\") as fp:\n",
        "    fp.write(\"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\n",
        "NhAAAAAwEAAQAAAgEApUPD7gz0vInJz1dYlIzxWNa2fuvnUiT/TrVJNk+WUK5+KgLFgK6g\n",
        "jYWDITJm9gLMVDQwhTtKosHzgnzVvP4PgYlGf5jRcqUrarWiXejYIG9zoKHyi33X1gi2zo\n",
        "FKAATnycO+XHdGTz3JOrrmlAsvkIKokji3rqWFblDFJ3u0aSeyzHxNs3xJNFGpihBWaAYy\n",
        "GVQaV14JVdU3PfmyaxmzD77NE53Z/fVk0k72SjNJ/7Ql50OaXpsGXTkySMGCXrihnzajF2\n",
        "tVJwdxnWKT9Z2/8akLsrg8LhiVcRvWrrElgo63azLrTIaKyv+D827qh4k1pEvmZ+aKjPc0\n",
        "2Ota9l6JEIhYhnOhtsQwU9Mjcq501Ce4L/qBsDfRFx+qx+vCxZAMc80+Rapbat5jfsNpbp\n",
        "YhgtXzmTnlIGO0Fc0mX/IKyGTUL/RyV4B0OP6vIAanLS01WIMbkAXfkEdgu1MKvKF8hQGl\n",
        "GryB1NfEPDBE3jyCljxhBuyFqN5Kp/EVeFiw38AwQAe4+u5VDgy3ZHqcRT/H0UryYjC6S0\n",
        "nymFB/ObOijWw03W6YKEaOeqE/HNww7CO3MtuAbgwUqNYZF+zOe1v58Mm7xZVzk8+hirok\n",
        "wQlTl4CYWX/ql0+Jwbz2IpDiX4iCWcOMo40cJmZlYVR4jL54sETNCL91FbkmjP3/YP5Qn6\n",
        "kAAAdIVX2ag1V9moMAAAAHc3NoLXJzYQAAAgEApUPD7gz0vInJz1dYlIzxWNa2fuvnUiT/\n",
        "TrVJNk+WUK5+KgLFgK6gjYWDITJm9gLMVDQwhTtKosHzgnzVvP4PgYlGf5jRcqUrarWiXe\n",
        "jYIG9zoKHyi33X1gi2zoFKAATnycO+XHdGTz3JOrrmlAsvkIKokji3rqWFblDFJ3u0aSey\n",
        "zHxNs3xJNFGpihBWaAYyGVQaV14JVdU3PfmyaxmzD77NE53Z/fVk0k72SjNJ/7Ql50OaXp\n",
        "sGXTkySMGCXrihnzajF2tVJwdxnWKT9Z2/8akLsrg8LhiVcRvWrrElgo63azLrTIaKyv+D\n",
        "827qh4k1pEvmZ+aKjPc02Ota9l6JEIhYhnOhtsQwU9Mjcq501Ce4L/qBsDfRFx+qx+vCxZ\n",
        "AMc80+Rapbat5jfsNpbpYhgtXzmTnlIGO0Fc0mX/IKyGTUL/RyV4B0OP6vIAanLS01WIMb\n",
        "kAXfkEdgu1MKvKF8hQGlGryB1NfEPDBE3jyCljxhBuyFqN5Kp/EVeFiw38AwQAe4+u5VDg\n",
        "y3ZHqcRT/H0UryYjC6S0nymFB/ObOijWw03W6YKEaOeqE/HNww7CO3MtuAbgwUqNYZF+zO\n",
        "e1v58Mm7xZVzk8+hirokwQlTl4CYWX/ql0+Jwbz2IpDiX4iCWcOMo40cJmZlYVR4jL54sE\n",
        "TNCL91FbkmjP3/YP5Qn6kAAAADAQABAAACAEaWEH/C4d8LPPqFlIxyPH0UzAKe0IC505/y\n",
        "9y+uw4V3WeSopWGmdGWt0kmiBO7rWAlY9yZYojKtA0xG9GWR396UWtuR0leUq1wa8xwIIR\n",
        "ONdsXzlaw1ljPRKf8+onQqpDN9mvdUbF/ZBHNEs8okku62l7hIaE+8W6a38dVA1VgagBgt\n",
        "uWRBX+TsQiz5eGZayxgdX1jUjcku1bbvSODMq7m8ZUwNHjgFkUfwOOqNSHxiHdROgAcLUK\n",
        "cNkGgZ2oyJcGKXzAXrLoYKfGDb41VDSOG3MYtmfDG2B1I1sTaQ6/P87+Nl7rETQAGfK+UU\n",
        "CTDVjmc7kc/r3F6EEXra31GeJA0OrsWt8o5lLO40DQLQVnta3xxQHNVIMlSOjgEVTd7MrC\n",
        "a81v07L5dLTEAj73guKFHYnRT/Ixd1AKYrX7OhMeVGN9sJn7y7FYD4m+gF3T+et2VOWw0R\n",
        "RuQyjoCopnlH+Lo24yJk/XSsQL3IVHasTCaThMz/km7AH7KktxK7/SNx7PwOuOU6076IMd\n",
        "37eawrIg5ecoEAp1MKfMJUG2jcN1Xl7RFpJaDuCu7qsFfjiSEPnVh6yQUkyMBuzni8mLna\n",
        "588TxVp9V3P6y9XZyDMeBN671u3Ro8HCxEJEQxTZoFgxdpaUQ0kO4oy7FrJxd22GkldMyi\n",
        "r856D7O+NORiFPWy//AAABABJLjG39AtIOBh+4mkIpT0FdWWqyTBwJdb+t3DyP+vIpLxmg\n",
        "U0dsTKVcAudwjr9Dv9XfA/BejcNhMvlzB9ob0NUi7tKTg8Xo2JhZQvpnQkBEavlTLm6rgN\n",
        "l3egQf8tIHCLfZ5+aF+mwODibIRK7kBTWlc7ln586JIonhgWzm0g3sD23LG0YW/2cCgi0r\n",
        "qtZQ6lUsJGFJGOUrZnomHnS9woCxS+BeEH4uE8lzXr5/0NvxmAGhHX9kt2fmwZWicrfqnu\n",
        "Crd5RDp9Dpe/iQzyvJssiSPvUFEwTOQEmO5Ce/kNPA0hKpBWYRoQpkVdnPsgEOFiZIqmbR\n",
        "vUu6I6zqxsNA09YAAAEBAOHLauNS72FUrybBrDoq5cvdTsg9yayMbuNsVGLHX/GOZWCkLF\n",
        "b9E7200i28rrz4Ue6QwsOTuabGT18Y2cTVQmiTaQzOLCu9IA7p90r14zb18V0KXyrV4Tlm\n",
        "YpfY+gqyWbiSgaTIHgZFW6qT+/kHKi8Wj/yx3NVtJFYefMcHl71wF90au+l9u+CGQkyZ1w\n",
        "yYXFIe9RmJt6uuLjoT55wdit63OdfGxTDIzdaLW3FHlxcgk7NmIYslHhLBBnSbM+olKGjh\n",
        "R7dkOzTuB2S76hOOLMSkFGUozYcscgLKlFGriZJjRzMzlnTyplH8WOp8NHUB3syy9/sSrJ\n",
        "rGDMftPsTff9MAAAEBALtfb4XvNoa/PhryNEycwbq9w0rrp8JL45ia6r7fgs6YJSReHwfr\n",
        "Lb/PtXDFay+dVNZwnCzQT1Ha9y40KhXpDEGFWBGwvcNnFhm1ZTl4TqPlSnFdTS+P6PE+vY\n",
        "Fsva2mLxNEBLjQPQPPpE1aAmrRsH3hG1OAbCa7F5BufTiz7WsHnvwSZpXgmudhkAZZGQYb\n",
        "6/0X7pUJBkm0N4XIjl6dreTMxL6iERx1eQM/S1k97tqH1RUqrLxdcEWvEygB1WpqyiEFrM\n",
        "q41tWqMTkadBD5lyg91y2OHcV5a5R+/9fLKUxVrkKvMziRlMU8uNPD6pggf5P0g/uv3t/g\n",
        "fAVid9m6cRMAAAARcm9vdEBjMjhlMjE0Y2Q1OGYBAg==\n",
        "-----END OPENSSH PRIVATE KEY-----\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# <COPY FROM LOCAL DISK AT: ~/dev/llm_cthulhu_fine_tuning/keys>\n",
        "! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "! chmod go-rwx /root/.ssh/id_rsa\n",
        "! git clone git@github.com:ellolo/cthulhu_fine_tuning.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYOAJ3nj69Y7",
        "outputId": "7ed2a6fa-e1fc-46ca-a789-d5cb1dac0fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-4f820feb\n",
            "Cloning into 'cthulhu_fine_tuning'...\n",
            "remote: Enumerating objects: 3462, done.\u001b[K\n",
            "remote: Counting objects: 100% (237/237), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 3462 (delta 133), reused 160 (delta 58), pack-reused 3225 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3462/3462), 134.57 MiB | 8.08 MiB/s, done.\n",
            "Resolving deltas: 100% (2161/2161), done.\n",
            "Updating files: 100% (2823/2823), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load quantized pretrained model\n",
        "\n",
        "The process is the same as in `2_lora_qa_finetuning.ipynb`, as follows.\n",
        "\n",
        "I experimented with the following base models: `Llama-3.1-8B-Instruct` and  `Llama-3.2-1B-Instruct`. These models are small enough to be finetuned with LoRA on a A100 GPU.\n",
        "It could be interesting to experiment also with base models that are not tuned to chat (e.g. `Llama-3.2-1B`) as proposed in [2], with bigger models, and with models other open-sourced models outside Llama family.\n",
        "\n",
        "The base model is loaded into the GPU with 4-bit BitsAndBytes quantization.  Using quantization saves GPU memory, which allowed me to run LoRA fine-tuning with a 8B model on a single A100 GPU.\n",
        "\n",
        "- **4-bit vs 8-bit quantization** \\\\\n",
        "Two types of quantization are typically used, 8-bit and 4-bit. The latter has slighly lower precision on existing benchmarks, but allows to significantly shrink memory. Using 4-bit instead of 8-bit allowed me to use a bigger model (Llama 8B instead of 3B). This is good, as some empirical evidences [7] show that a 4-bit 8B is more accurate than a a 8-bit 3B model.\n",
        "\n",
        "- **BitsAndBytes** \\\\\n",
        "There are many available approaches to quantize a model, as described in [4, 5]. For PEFT fine-tuning and for working on CUDA/Nvidia, Hugging Face recommends to use BitsAndBytes [9], since it is the most tested for PEFT.\n",
        "\n",
        "- **Flesh attention** \\\\\n",
        "I load the model with flash-attention when the architecture of the GPU allows. Flesh attention should increase memory performance and stability [13, 14, 15].\n",
        "\n",
        "GPU memory used for base models during my experiments, measured with nvidia-smi:\n",
        "\n",
        "| Model | Quantization | VRAM (GB) |\n",
        "|-------|--------------|----------|\n",
        "| Llama-3.2-1B | none | 5.99 |\n",
        "| Llama-3.2-1B | 4-bit | 1.60 |\n",
        "| Llama-3.1-8B | 4-bit | 7.27 |\n",
        "\n",
        "## References\n",
        "\n",
        "Fine-tuning in Hugging Face using PEFT:\n",
        "\n",
        "- [1] https://huggingface.co/docs/transformers/main/en/peft\n",
        "\n",
        "Best base models for LoRA:\n",
        "- [2] https://docs.unsloth.ai/get-started/fine-tuning-guide/what-model-should-i-use\n",
        "\n",
        "Llama 3.2 architecture:\n",
        "- [3] https://www.analyticsvidhya.com/blog/2024/09/llama-3-2-models/\n",
        "\n",
        "Quantization quick intro:\n",
        "- [4] https://huggingface.co/docs/transformers/main/en/quantization/concept_guide\n",
        "- [5] https://huggingface.co/docs/transformers/en/quantization/overview\n",
        "- [6] https://huggingface.co/blog/merve/quantization\n",
        "\n",
        "4-bit vs 8-bit quantization:\n",
        "- [7] https://www.reddit.com/r/LocalLLaMA/comments/13mxq66/13b_4bit_or_7b_8bits/\n",
        "- [8] https://medium.com/@shikharstruck/shrinking-elephants-a-funny-guide-to-4-bit-and-8-bit-quantization-for-llms-with-lora-ddf9f1a62070\n",
        "\n",
        "BitsAndBytes:\n",
        "- [9] https://huggingface.co/docs/transformers/main/en/quantization/selecting\n",
        "- [10] https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "- [11] https://huggingface.co/docs/transformers/en/quantization/bitsandbytes\n",
        "- [12] https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig\n",
        "\n",
        "Flesh attention:\n",
        "- [13] https://huggingface.co/docs/trl/en/sft_trainer#using-flash-attention-and-flash-attention-2\n",
        "- [14] https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html\n",
        "- [15] https://medium.com/data-science-in-your-pocket/what-is-flash-attention-f5dc22522a77"
      ],
      "metadata": {
        "id": "MKO4Bus9xtFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set base model\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # \"meta-llama/Llama-3.2-1B-Instruct\" # \"meta-llama/Llama-3.2-1B\""
      ],
      "metadata": {
        "id": "oJl6jqW20M-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure BitsAndBytes\n",
        "\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                     # weights stored in 4 bits (saves memory)\n",
        "    bnb_4bit_quant_type=\"nf4\",             # format used for storing in 4 bits\n",
        "    bnb_4bit_use_double_quant=True,        # double quantize (saves memory)\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # perform operations in 16bit instead of 32bit (speed up fine-tuning)\n",
        ")"
      ],
      "metadata": {
        "id": "S0BmIOBdzoKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load quantized model on GPU\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "SKIP_FLASH_ATTENTION = True # force skipping flash attention\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "mem_start = torch.cuda.memory_allocated(device)\n",
        "\n",
        "# check if we can use flash-attention\n",
        "try:\n",
        "  if torch.cuda.get_device_capability()[0] >= 8 and not SKIP_FLASH_ATTENTION:\n",
        "      print(\"Using flash-attention\")\n",
        "      attn_implementation = \"flash_attention_2\"\n",
        "      !pip install -U flash_attn\n",
        "  else:\n",
        "      print(\"Using native attention\")\n",
        "      attn_implementation = \"eager\"\n",
        "except:\n",
        "  print(\"Using native attention (no GPU found)\")\n",
        "  attn_implementation = \"eager\"\n",
        "attn_implementation = \"eager\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    attn_implementation=attn_implementation,\n",
        ").to(device)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "mem_model_load = torch.cuda.memory_allocated(device)\n",
        "mem_used_model = round((mem_model_load - mem_start) / 1024/1024/1024, 4)\n",
        "print(f\"Using device:{device}\")\n",
        "print(f\"Memory used for original model (GB): {mem_used_model}\")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ABvhVB77zrXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Setup LoRA model\n",
        "\n",
        "The process is very similar to `2_lora_qa_finetuning.ipynb`. The only difference is that we try to apply the Lora adapters also to the `lm_head` and the `embed_tokens` layers.\n",
        "\n",
        "I experimented with different Llora hyperparameter setups, largely following existing recommendations from the literature [1, 2, 3, 6]:\n",
        "- **r**: 64 \\\\\n",
        "Literature suggests to keep the Llora rank _r_ between 8 and 128. I set this to 64, based on the results of the experiments obtained in `2_lora_qa_finetuning.ipynb`.\n",
        "- **α**: 32 \\\\\n",
        "Literature suggests to set the scaling factor _α_ to _α=r_ or _α=2*r_. A higher scaling factor amplifies the effect of the fine-tuning, i.e. the Llora. A lower value pushed the model to rely mor eon the original parameters.\n",
        "- **rLlora**: active \\\\\n",
        "rLlora takes the square root of _r_ when merging the Llora adapter, instead of _r_, which brings more stability to the fine-tuning process.\n",
        "\n",
        "- **layers with Llora**: all linear layers, all linear layers + lm_head \\\\\n",
        "Empirical experiment [6] hypothesize that best performance for continued pretraining are achieved when applying Llora not only to all linear layers, but also to the `lm_head` and `embed_tokens` layers. I experimented both with and without these latter layers, to verify this hypothesis.\n",
        "\n",
        "- **batch size**: 16, 64\n",
        "A small batch size allows to save VRAM, but I was able to push to 64 examples on the A100, so to get more stability and faster training time.\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Llora intro:\n",
        "- [1] https://arxiv.org/abs/2106.09685\n",
        "- [2] https://medium.com/@kednaik/llm-fine-tuning-with-lora-8e06f2227183\n",
        "\n",
        "Hyperparameters setting for Llora:\n",
        "- [3] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "- [4] https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/\n",
        "- [5] https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2\n",
        "\n",
        "Hyperparameters setting for Continued Pretraing with Llora\n",
        "- [6] https://unsloth.ai/blog/contpretraining"
      ],
      "metadata": {
        "id": "GXuqwV0htgoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Llora hyperparameters\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "def get_linear_layers(model, exclude_lm_head = True):\n",
        "    \"\"\"\n",
        "    Returns the short sematic name of all layers that are linear,\n",
        "    (excluding the final head layer if needed).\n",
        "\n",
        "    Args:\n",
        "        model: HF model to get linear layers from.\n",
        "        exclude_lm_head (boolean, optional): True if lm_head layer must be included\n",
        "    Returns:\n",
        "        list: List of linear layers of the model.\n",
        "    \"\"\"\n",
        "    linear_layers = set()\n",
        "    for name, module in model.named_modules():\n",
        "      module_type = str(type(module))\n",
        "      if \"Linear\" in module_type:\n",
        "        linear_layers.add(name.split(\".\")[-1])\n",
        "    if exclude_lm_head and 'lm_head' in linear_layers:\n",
        "      linear_layers.remove(\"lm_head\")\n",
        "    return list(linear_layers)\n",
        "\n",
        "INCLUDE_HEAD_AND_EMB = False # decide if to include lm_nead and embed_tokens layers\n",
        "\n",
        "if INCLUDE_HEAD_AND_EMB:\n",
        "  layers_for_lora = get_linear_layers(model, exclude_lm_head=False)\n",
        "  layers_for_lora.append(\"embed_tokens\")\n",
        "else:\n",
        "  layers_for_lora = get_linear_layers(model, exclude_lm_head=True)\n",
        "\n",
        "print(f\"Linear for LoRA: {layers_for_lora}\")\n",
        "\n",
        "LORA_R = 64 # 16, 32, 64, 128, 256\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,                                 # rank (dim) of the adapters A and B. Higher value means more parameters to train (more expressivity but also more mem). Increasing does not bring substantial increase in performance.\n",
        "    lora_alpha=LORA_ALPHA,                    # magnitude of adapter in changing the orginal params (W' = W + alpha/r * (A*B)). Usually alpha=2*r. Typicall set between 4 and 64. High a leans more to the adapter, low a leans more on original weights. Should be bigger for smaller models.\n",
        "    target_modules= layers_for_lora,          # to which layers to apply lora adapters. This is the param that influences performance the most. According to QLoRA paper should use adapters to all linear layers to match full finetuning accuracy\n",
        "    lora_dropout=0.0,                         # setting to 0 allows faster training, but risks some overfitting\n",
        "    bias=\"none\",\n",
        "    use_rslora=True,                          # stabilizes adapters. changes the Lora equation to: W' = W + alpha/SQRT(r) * (A*B), so that for large r, the Lora adapters still have a significant impact on W'\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "GUC-Yw4TyyTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66cb696-90eb-4643-daf0-04b9253ab4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear for LoRA: ['o_proj', 'v_proj', 'up_proj', 'q_proj', 'k_proj', 'down_proj', 'gate_proj']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters to the selected layers of the base model's architecture.\n",
        "\n",
        "from peft import PeftModelForCausalLM\n",
        "\n",
        "model: PeftModelForCausalLM = get_peft_model(model, lora_config)\n",
        "\n",
        "mem_lora_model_load = torch.cuda.memory_allocated(device)\n",
        "mem_used_lora_model = round((mem_lora_model_load - mem_model_load) / 1024/1024/1024, 4)\n",
        "print(f\"Memory used for original model (GB): {mem_used_model}\")\n",
        "print(f\"Memory used for LoRA model (GB): {mem_used_lora_model}\")\n",
        "print(model)\n",
        "print(\"*\"*20)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "uGp_5pnrz56f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "f6e2a5a7-8af0-44e4-8215-58a8352f5dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory used for original model (GB): 7.2716\n",
            "Memory used for LoRA model (GB): 0.6265\n",
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(128256, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (rotary_emb): LlamaRotaryEmbedding()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "********************\n",
            "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prepare textual dataset\n",
        "\n",
        "We perform continuous pretraining on the text of the [Chtulhu Rulebook](https://archive.org/details/call-of-cthulhu-core-rulebook-by-chaosium-inc.-z-lib.org). The text (located at `data/output`) is converted from the original PDF (located at `data/pdf`) as described in `1_synt_dataset_generation.ipynb`. The text consists of 466,075 tokens, for a total of 1,690,260 characters.\n",
        "\n",
        "The text is then split into **chunks** of 256 tokens. Each chunk constitue a training example. Chunks have a high overlap, in order to ensure that all the semantic content of the Rulebook is considered during training. More details on how chunking is performed are reported below.\n",
        "\n",
        "\n",
        "Specifically, I do the following:\n",
        "\n",
        "1. Load the text of the Rulebook. Since we are doing continued pretraining, we do not need to split into training and test set.\n",
        "\n",
        "2. Tokenize the text [4, 5, 6] and peform **chunking** (see details below). Each chunk constitutes a training example for the Lora model.\n",
        "\n",
        "3. Initialize the Data Collator that will generate batches during fine-tuning. This is done exactly as in `2_lora_qa_finetuning.ipynb`. \\\\\n",
        " I use the `DataCollatorForLanguageModeling` which performs the following for each batch (see [7] for a complete guide):\n",
        " - **Padding**: finds the example in the batch with the longest 'input_ids'  token sequence _n_. Pads the 'input_ids' token sequence of the other examples in the batch up to _n_, using the tokenizer's pad token. This results in all token sequences in the batch to have the same length.\n",
        " - **Labels**: create a new 'label sequence' for each example in the batch. This is the field used by the Trainer to compute the loss. The label sequence is created as follows. First, it copies the token ids from the 'input_ids' sequence into the label sequence. Then, it replaces pad token ids with -100 so that pad tokens are not used for loss computation. \\\\\n",
        "Note that the label for token $u_i$ should be token $u_i+1$ since we want to predict the _next_ token $u_i+1$ from the context $u_0...u_i$. Therefore we we would expect the Data Collator to not simply 'copy' the input_ids into the label sequence, but to shift the label of one position. In Hugging Face this shifting is actualy done by the trainer itself [8], therefore just copying is ok.\n",
        "\n",
        "### How chunking is done\n",
        "\n",
        "I construct the dataset for continuous pretraining with a procedure that is very similar to that used by big LLM players to pretrain fundational model (e.g. GPT or Llama).\n",
        "\n",
        "Specifically, I take the raw text of the Rulebook, tokenize it, and split it into chunks [15, 16, 17, 18]. Each chunk has a fixed number of tokens (`CONTEXT_LENGTH`, which I set empirically to 256). Each chunk starts a fixed number of tokens after the start of the previous chunk (`STRIDE` which I treat as a hyperparameter). In practice:\n",
        "  - first chunk => start at token index `1`, ends at token `CONTEXT_LENGTH`\n",
        "  - second chunk => start at token index `STRIDE`, ends at token `CONTEXT_LENGTH + STIRED`\n",
        "  - third chunk => start at token index `2*STRIDE`, ends at token `2*STRIDE + CONTEXT_LENGTH`\n",
        "  - k chunk  => start at token index `K*STRIDE`, ends at token `K*STRIDE + CONTEXT_LENGTH`\n",
        "\n",
        "This means that when we use a `STRIDE` of 1, we extract a total of `N-CONTEXT_LENGTH`, chunks, where N is the total number of tokens in the dataset.\n",
        "Example:\n",
        "```\n",
        "TOKENS = [\"the\", \"book\", \"is\", \"on\", \"the\", \"table\", \"and\", \"is\", \"red\"]\n",
        "CONTEXT_LENGHT = 5\n",
        "STRIDE = 1\n",
        "Chunks are:\n",
        "    [\"the\", \"book\", \"is\", \"on\", \"the\",]\n",
        "    [\"book\", \"is\", \"on\", \"the\", \"table\"]\n",
        "    [\"is\", \"on\", \"the\", \"table\", \"and\"]\n",
        "    [\"on\", \"the\", \"table\", \"and\", \"is\"]\n",
        "    [\"the\", \"table\", \"and\", \"is\", \"red\"]\n",
        "```\n",
        "This allows every token in the dataset to be predicted with full context.\n",
        "For example, if we had instead used `STRIDE=5` we would have had:\n",
        "```\n",
        "    [\"the\", \"book\", \"is\", \"on\", \"the\"]\n",
        "    [\"table\", \"and\", \"is\", \"red\"]\n",
        "```\n",
        "In this case, the token \"table\" would never had a chance to be predicted,since it never has a context. Therefore we want `STRIDE` to be always smaller than `CONTEXT_LENGTH`.\n",
        "\n",
        "\n",
        "### Note about Pad token in Llama.\n",
        "\n",
        "`DataCollatorForLanguageModeling` requires the Tokenizer to have a pad token, or it fails. Since Llama does not have a pad token, I set it manually to one of the unused 'reserved tokens' (see file tokenizer_config.json in Hugging Face). Specificially I used the aptly named `<|finetune_right_pad_id|>` [10].\n",
        "I could have alternaitely used the `eos_token` or a brand new token, but these have the following disadvantages:\n",
        "-  eos_token: this would result in the collator to 'mask' the eos_token (i.e.\n",
        "set all the occurrences of eos_token, also those that actually mean 'end of sentence') to -100. This may result in the model never to emit eos_token, end therefore endlessly generate [11, 12].\n",
        "- new token: this would change the token vocabulary size. We would therefore need to resize the embedding layers, which is inconvenient and may not work with quantized models [13, 14].\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Tokenization in Hugging Face:\n",
        "- [4] https://huggingface.co/learn/llm-course/chapter2/4?fw=pt\n",
        "- [5] https://huggingface.co/learn/llm-course/chapter3/2?fw=pt\n",
        "- [6] https://huggingface.co/learn/llm-course/chapter2/5?fw=pt#attention-masks\n",
        "\n",
        "Data Collators:\n",
        "- [7] https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2/\n",
        "- [8] https://huggingface.co/learn/llm-course/en/chapter7/6#initializing-a-new-model\n",
        "- [9] https://discuss.huggingface.co/t/how-labelled-data-is-processed-transformers-trainer/80644/6\n",
        "\n",
        "Pad token in Llama:\n",
        "- [10] http://github.com/unslothai/unsloth/issues/416\n",
        "- [11] https://github.com/huggingface/transformers/issues/25773\n",
        "- [12] https://github.com/huggingface/transformers/issues/22794\n",
        "- [13] https://medium.com/@coldstart_coder/adding-custom-tokens-to-huggingface-models-1981f114efc1\n",
        "- [14] https://www.youtube.com/watch?v=MB0ZJ5Y-07s           \n",
        "\n",
        "Prepare dataset for continued pretraining:\n",
        "- [15] https://huggingface.co/learn/llm-course/chapter7/6?fw=pt#preparing-the-dataset\n",
        "- [16] https://medium.com/@williamzebrowski7/llm-foundations-constructing-and-training-decoder-only-transformers-bfcc429b43a2\n",
        "- [17] https://github.com/karpathy/nanoGPT/blob/master/train.py#L116\n",
        "- [18] https://stackoverflow.com/questions/76342339/how-can-i-handle-overflowing-tokens-in-huggingface-transformer-model\n"
      ],
      "metadata": {
        "id": "rbJr1M0LuR0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load textual dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning\n",
        "\n",
        "dataset_file = \"data/output/cthulhu.txt\"\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files = dataset_file,\n",
        "    sample_by=\"document\",\n",
        "    split = \"train\")\n",
        "\n",
        "print(f\"dataset size: {len(dataset)}\")\n",
        "print(f\"Number of chars: {len(dataset[0]['text'])}\")"
      ],
      "metadata": {
        "id": "-OTuplaO_pLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and chunk text\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "  \"\"\"\n",
        "  Tokenizes the text in the 'text' field of a dataset example.\n",
        "\n",
        "  Args:\n",
        "      example (dict): A dictionary containing the text data in the 'text' key.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing the tokenized input IDs under the 'input_ids' key.\n",
        "  \"\"\"\n",
        "  return tokenizer(example[\"text\"])\n",
        "\n",
        "\n",
        "def create_chunks(example, context_length, stride=1):\n",
        "  \"\"\"\n",
        "  Splits tokenized input IDs into overlapping chunks.\n",
        "\n",
        "  Args:\n",
        "      example (dict): A dictionary containing the tokenized input IDs under the 'input_ids' key.\n",
        "      context_length (int): The desired length of each chunk, in number of tokens.\n",
        "      stride (int, optional): The number of tokens to shift for the start of the next chunk. Defaults to 1.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing a list of tokenized chunks under the 'input_ids' key\n",
        "            and a list of chunk lengths under the 'length' key.\n",
        "  \"\"\"\n",
        "  chunks = []\n",
        "  lengths = []\n",
        "  for i in range(0, len(example[\"input_ids\"][0]) - context_length + 1, stride):\n",
        "    chunk = example[\"input_ids\"][0][i : i + context_length]\n",
        "    chunks.append(chunk)\n",
        "    lengths.append(len(chunk))\n",
        "  return {\"input_ids\": chunks, \"length\": lengths}\n",
        "\n",
        "\n",
        "CONTEXT_LENGTH = 256  # nuber of tokens in each chunk\n",
        "STRIDE = 32 # start the next chunk STRIDE tokens later\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# tokenize whole document\n",
        "dataset_tokenized = dataset.map(tokenize, batched=False, remove_columns=dataset.column_names)\n",
        "\n",
        "# split document into chunks\n",
        "dataset_chunked = dataset_tokenized.map(\n",
        "    create_chunks,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_tokenized.column_names,\n",
        "    fn_kwargs={\"context_length\": CONTEXT_LENGTH, \"stride\": STRIDE})\n",
        "\n",
        "# show dataset statistics\n",
        "print(f\"Number of tokens: {len(dataset_tokenized['input_ids'][0])}\")\n",
        "print(f\"Number of chunks: {len(dataset_chunked)}\")\n",
        "lengths = dataset_chunked[\"length\"]\n",
        "d = defaultdict(int)\n",
        "for l in lengths:\n",
        "  d[l] += 1\n",
        "print(\"Distribution of chunk lengths:\")\n",
        "for k, v in d.items():\n",
        "  print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "LCAaODvd6_FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's double check we chuncked correctly (only works for STRIDE=1).\n",
        "# we take the first three chunks, and the first CONTENT_LENGHT+2 tokens of the\n",
        "# original tokenized document and make sure there is a match.\n",
        "# We do the same with the last three chunks.\n",
        "for i in range(3):\n",
        "  print(dataset_chunked[i][\"input_ids\"])\n",
        "  print(dataset_tokenized[0][\"input_ids\"][i:CONTEXT_LENGTH+i])\n",
        "  print(\"-\"*20)\n",
        "for i in range(3):\n",
        "  print(dataset_chunked[-i-1][\"input_ids\"])\n",
        "  length = len(dataset_tokenized[0][\"input_ids\"])\n",
        "  print(dataset_tokenized[0][\"input_ids\"][length-i-CONTEXT_LENGTH:length-i])\n",
        "  print(\"-\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AlTq2S-h9ACK",
        "outputId": "61a04f7a-1d36-47fa-8ff2-eb6427968e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[128000, 200, 200, 38363, 5439, 555, 4815, 50, 13634, 32284, 268, 271, 2409, 3010, 24493, 555, 4815, 43, 59360, 67742, 271, 2028, 32828, 220, 22, 339, 14398, 374, 264, 20632, 1990, 4815, 26368, 2939, 5327, 323, 11519, 29927, 271, 9597, 532, 25, 10016, 25225, 1637, 11, 11717, 1414, 4584, 644, 4978, 11, 11519, 29927, 11, 25972, 93070, 271, 21103, 15392, 25, 11717, 1414, 4584, 644, 4978, 323, 11519, 29927, 271, 2224, 25, 11717, 1414, 4584, 644, 4978, 11, 40796, 452, 582, 3370, 11, 25972, 93070, 271, 9470, 19438, 25, 11519, 29927, 11, 80134, 4584, 49862, 11, 15469, 4923, 121827, 29384, 271, 31996, 39154, 367, 25, 8388, 33794, 546, 271, 26072, 39154, 811, 25, 24150, 24100, 441, 11, 7043, 30474, 875, 11, 4997, 85431, 11, 85807, 5034, 359, 295, 11, 33412, 2009, 4458, 11, 2355, 54567, 56728, 11, 11519, 31421, 11, 40796, 19197, 1601, 11, 23245, 34297, 11, 735, 96893, 54090, 89, 271, 86225, 39154, 811, 25, 220, 32532, 99666, 11, 2895, 6374, 11, 11517, 473, 952, 11, 6621, 38672, 66, 15130, 4341, 11, 7043, 30474, 875, 11, 10016, 34221, 11, 2355, 66091, 24100, 441, 11, 33412, 2009, 4458, 11, 8388, 33794, 546, 11, 23519, 5455, 426, 2639, 309, 11, 92664, 329, 11, 23245, 4802, 10333, 11, 8529, 43089, 11, 35281, 386, 16977, 271, 34, 2889, 1073, 18812, 9757, 3549, 555, 25, 11355, 2405, 258, 3520, 271, 12878, 5814, 25, 3441, 544, 36636, 1626, 783, 271, 2520, 45638, 67742, 271, 200, 91336, 84127, 720, 791, 12283, 1053, 1093, 311, 9901, 279, 2768, 1274, 369, 872, 720, 6497]\n",
            "[128000, 200, 200, 38363, 5439, 555, 4815, 50, 13634, 32284, 268, 271, 2409, 3010, 24493, 555, 4815, 43, 59360, 67742, 271, 2028, 32828, 220, 22, 339, 14398, 374, 264, 20632, 1990, 4815, 26368, 2939, 5327, 323, 11519, 29927, 271, 9597, 532, 25, 10016, 25225, 1637, 11, 11717, 1414, 4584, 644, 4978, 11, 11519, 29927, 11, 25972, 93070, 271, 21103, 15392, 25, 11717, 1414, 4584, 644, 4978, 323, 11519, 29927, 271, 2224, 25, 11717, 1414, 4584, 644, 4978, 11, 40796, 452, 582, 3370, 11, 25972, 93070, 271, 9470, 19438, 25, 11519, 29927, 11, 80134, 4584, 49862, 11, 15469, 4923, 121827, 29384, 271, 31996, 39154, 367, 25, 8388, 33794, 546, 271, 26072, 39154, 811, 25, 24150, 24100, 441, 11, 7043, 30474, 875, 11, 4997, 85431, 11, 85807, 5034, 359, 295, 11, 33412, 2009, 4458, 11, 2355, 54567, 56728, 11, 11519, 31421, 11, 40796, 19197, 1601, 11, 23245, 34297, 11, 735, 96893, 54090, 89, 271, 86225, 39154, 811, 25, 220, 32532, 99666, 11, 2895, 6374, 11, 11517, 473, 952, 11, 6621, 38672, 66, 15130, 4341, 11, 7043, 30474, 875, 11, 10016, 34221, 11, 2355, 66091, 24100, 441, 11, 33412, 2009, 4458, 11, 8388, 33794, 546, 11, 23519, 5455, 426, 2639, 309, 11, 92664, 329, 11, 23245, 4802, 10333, 11, 8529, 43089, 11, 35281, 386, 16977, 271, 34, 2889, 1073, 18812, 9757, 3549, 555, 25, 11355, 2405, 258, 3520, 271, 12878, 5814, 25, 3441, 544, 36636, 1626, 783, 271, 2520, 45638, 67742, 271, 200, 91336, 84127, 720, 791, 12283, 1053, 1093, 311, 9901, 279, 2768, 1274, 369, 872, 720, 6497]\n",
            "--------------------\n",
            "[26368, 2939, 5327, 323, 11519, 29927, 271, 9597, 532, 25, 10016, 25225, 1637, 11, 11717, 1414, 4584, 644, 4978, 11, 11519, 29927, 11, 25972, 93070, 271, 21103, 15392, 25, 11717, 1414, 4584, 644, 4978, 323, 11519, 29927, 271, 2224, 25, 11717, 1414, 4584, 644, 4978, 11, 40796, 452, 582, 3370, 11, 25972, 93070, 271, 9470, 19438, 25, 11519, 29927, 11, 80134, 4584, 49862, 11, 15469, 4923, 121827, 29384, 271, 31996, 39154, 367, 25, 8388, 33794, 546, 271, 26072, 39154, 811, 25, 24150, 24100, 441, 11, 7043, 30474, 875, 11, 4997, 85431, 11, 85807, 5034, 359, 295, 11, 33412, 2009, 4458, 11, 2355, 54567, 56728, 11, 11519, 31421, 11, 40796, 19197, 1601, 11, 23245, 34297, 11, 735, 96893, 54090, 89, 271, 86225, 39154, 811, 25, 220, 32532, 99666, 11, 2895, 6374, 11, 11517, 473, 952, 11, 6621, 38672, 66, 15130, 4341, 11, 7043, 30474, 875, 11, 10016, 34221, 11, 2355, 66091, 24100, 441, 11, 33412, 2009, 4458, 11, 8388, 33794, 546, 11, 23519, 5455, 426, 2639, 309, 11, 92664, 329, 11, 23245, 4802, 10333, 11, 8529, 43089, 11, 35281, 386, 16977, 271, 34, 2889, 1073, 18812, 9757, 3549, 555, 25, 11355, 2405, 258, 3520, 271, 12878, 5814, 25, 3441, 544, 36636, 1626, 783, 271, 2520, 45638, 67742, 271, 200, 91336, 84127, 720, 791, 12283, 1053, 1093, 311, 9901, 279, 2768, 1274, 369, 872, 720, 6497, 287, 220, 1862, 220, 323, 220, 13291, 25, 220, 25972, 220, 93070, 11, 220, 9052, 720, 6600, 1892, 72, 11, 6706, 661, 88088, 11, 26349, 2563, 1108, 11, 3842, 8753, 11, 10016]\n",
            "[200, 200, 38363, 5439, 555, 4815, 50, 13634, 32284, 268, 271, 2409, 3010, 24493, 555, 4815, 43, 59360, 67742, 271, 2028, 32828, 220, 22, 339, 14398, 374, 264, 20632, 1990, 4815, 26368, 2939, 5327, 323, 11519, 29927, 271, 9597, 532, 25, 10016, 25225, 1637, 11, 11717, 1414, 4584, 644, 4978, 11, 11519, 29927, 11, 25972, 93070, 271, 21103, 15392, 25, 11717, 1414, 4584, 644, 4978, 323, 11519, 29927, 271, 2224, 25, 11717, 1414, 4584, 644, 4978, 11, 40796, 452, 582, 3370, 11, 25972, 93070, 271, 9470, 19438, 25, 11519, 29927, 11, 80134, 4584, 49862, 11, 15469, 4923, 121827, 29384, 271, 31996, 39154, 367, 25, 8388, 33794, 546, 271, 26072, 39154, 811, 25, 24150, 24100, 441, 11, 7043, 30474, 875, 11, 4997, 85431, 11, 85807, 5034, 359, 295, 11, 33412, 2009, 4458, 11, 2355, 54567, 56728, 11, 11519, 31421, 11, 40796, 19197, 1601, 11, 23245, 34297, 11, 735, 96893, 54090, 89, 271, 86225, 39154, 811, 25, 220, 32532, 99666, 11, 2895, 6374, 11, 11517, 473, 952, 11, 6621, 38672, 66, 15130, 4341, 11, 7043, 30474, 875, 11, 10016, 34221, 11, 2355, 66091, 24100, 441, 11, 33412, 2009, 4458, 11, 8388, 33794, 546, 11, 23519, 5455, 426, 2639, 309, 11, 92664, 329, 11, 23245, 4802, 10333, 11, 8529, 43089, 11, 35281, 386, 16977, 271, 34, 2889, 1073, 18812, 9757, 3549, 555, 25, 11355, 2405, 258, 3520, 271, 12878, 5814, 25, 3441, 544, 36636, 1626, 783, 271, 2520, 45638, 67742, 271, 200, 91336, 84127, 720, 791, 12283, 1053, 1093, 311, 9901, 279, 2768, 1274, 369, 872, 720, 6497, 287]\n",
            "--------------------\n",
            "[644, 4978, 323, 11519, 29927, 271, 2224, 25, 11717, 1414, 4584, 644, 4978, 11, 40796, 452, 582, 3370, 11, 25972, 93070, 271, 9470, 19438, 25, 11519, 29927, 11, 80134, 4584, 49862, 11, 15469, 4923, 121827, 29384, 271, 31996, 39154, 367, 25, 8388, 33794, 546, 271, 26072, 39154, 811, 25, 24150, 24100, 441, 11, 7043, 30474, 875, 11, 4997, 85431, 11, 85807, 5034, 359, 295, 11, 33412, 2009, 4458, 11, 2355, 54567, 56728, 11, 11519, 31421, 11, 40796, 19197, 1601, 11, 23245, 34297, 11, 735, 96893, 54090, 89, 271, 86225, 39154, 811, 25, 220, 32532, 99666, 11, 2895, 6374, 11, 11517, 473, 952, 11, 6621, 38672, 66, 15130, 4341, 11, 7043, 30474, 875, 11, 10016, 34221, 11, 2355, 66091, 24100, 441, 11, 33412, 2009, 4458, 11, 8388, 33794, 546, 11, 23519, 5455, 426, 2639, 309, 11, 92664, 329, 11, 23245, 4802, 10333, 11, 8529, 43089, 11, 35281, 386, 16977, 271, 34, 2889, 1073, 18812, 9757, 3549, 555, 25, 11355, 2405, 258, 3520, 271, 12878, 5814, 25, 3441, 544, 36636, 1626, 783, 271, 2520, 45638, 67742, 271, 200, 91336, 84127, 720, 791, 12283, 1053, 1093, 311, 9901, 279, 2768, 1274, 369, 872, 720, 6497, 287, 220, 1862, 220, 323, 220, 13291, 25, 220, 25972, 220, 93070, 11, 220, 9052, 720, 6600, 1892, 72, 11, 6706, 661, 88088, 11, 26349, 2563, 1108, 11, 3842, 8753, 11, 10016, 25225, 1637, 11, 720, 50988, 220, 8847, 1293, 11, 220, 25028, 220, 91990, 60382, 11, 220, 13678, 220, 21293, 11, 720, 20830, 35407, 11, 16768, 5929, 11, 12471, 7379, 11166, 11, 43582]\n",
            "[200, 38363, 5439, 555, 4815, 50, 13634, 32284, 268, 271, 2409, 3010, 24493, 555, 4815, 43, 59360, 67742, 271, 2028, 32828, 220, 22, 339, 14398, 374, 264, 20632, 1990, 4815, 26368, 2939, 5327, 323, 11519, 29927, 271, 9597, 532, 25, 10016, 25225, 1637, 11, 11717, 1414, 4584, 644, 4978, 11, 11519, 29927, 11, 25972, 93070, 271, 21103, 15392, 25, 11717, 1414, 4584, 644, 4978, 323, 11519, 29927, 271, 2224, 25, 11717, 1414, 4584, 644, 4978, 11, 40796, 452, 582, 3370, 11, 25972, 93070, 271, 9470, 19438, 25, 11519, 29927, 11, 80134, 4584, 49862, 11, 15469, 4923, 121827, 29384, 271, 31996, 39154, 367, 25, 8388, 33794, 546, 271, 26072, 39154, 811, 25, 24150, 24100, 441, 11, 7043, 30474, 875, 11, 4997, 85431, 11, 85807, 5034, 359, 295, 11, 33412, 2009, 4458, 11, 2355, 54567, 56728, 11, 11519, 31421, 11, 40796, 19197, 1601, 11, 23245, 34297, 11, 735, 96893, 54090, 89, 271, 86225, 39154, 811, 25, 220, 32532, 99666, 11, 2895, 6374, 11, 11517, 473, 952, 11, 6621, 38672, 66, 15130, 4341, 11, 7043, 30474, 875, 11, 10016, 34221, 11, 2355, 66091, 24100, 441, 11, 33412, 2009, 4458, 11, 8388, 33794, 546, 11, 23519, 5455, 426, 2639, 309, 11, 92664, 329, 11, 23245, 4802, 10333, 11, 8529, 43089, 11, 35281, 386, 16977, 271, 34, 2889, 1073, 18812, 9757, 3549, 555, 25, 11355, 2405, 258, 3520, 271, 12878, 5814, 25, 3441, 544, 36636, 1626, 783, 271, 2520, 45638, 67742, 271, 200, 91336, 84127, 720, 791, 12283, 1053, 1093, 311, 9901, 279, 2768, 1274, 369, 872, 720, 6497, 287, 220]\n",
            "--------------------\n",
            "[271, 32974, 3833, 445, 2319, 81, 271, 32974, 942, 29770, 271, 32974, 942, 435, 55526, 271, 52, 11253, 26316, 83, 31031, 271, 52, 657, 40310, 271, 20552, 3228, 358, 14946, 271, 52, 36722, 474, 271, 52, 906, 5124, 372, 11252, 271, 73092, 1189, 50940, 16172, 49120, 271, 19697, 4815, 54, 5809, 13929, 28280, 271, 54, 5809, 5124, 6043, 3018, 86394, 1553, 271, 54, 5809, 9126, 12404, 7830, 271, 54, 1037, 328, 34869, 271, 54, 12173, 39874, 8611, 84, 271, 54, 38377, 435, 5809, 25611, 728, 271, 29784, 1466, 38866, 271, 29784, 1466, 7948, 88873, 271, 36154, 818, 47051, 399, 271, 36154, 818, 473, 714, 88, 271, 36154, 818, 445, 5809, 37235, 4469, 271, 906, 2668, 4650, 28769, 271, 54, 408, 88, 97260, 3315, 271, 54, 288, 31676, 285, 271, 54, 288, 3258, 24298, 271, 54, 288, 3258, 35393, 616, 271, 54, 288, 3258, 33491, 352, 271, 54, 1216, 3817, 469, 2221, 271, 54, 83518, 18878, 271, 53477, 11490, 1254, 2718, 271, 10149, 35674, 271, 10149, 66309, 271, 10149, 12036, 79, 6181, 271, 45887, 29492, 271, 45887, 56859, 271, 45887, 356, 426, 12359, 16014, 271, 45887, 25028, 271, 45887, 7462, 5730, 14767, 271, 45887, 70480, 1293, 271, 45887, 386, 553, 271, 45887, 432, 5809, 3279, 3252, 783, 271, 45887, 1443, 5100, 271, 45887, 60920, 370, 271, 45887, 328, 4010, 86, 5302, 271, 45887, 350, 5809, 35552, 60808, 271, 45887, 220, 1901, 5809, 220, 32196, 11, 220, 24200, 51, 6018, 2078, 11865, 11, 2160, 5809, 271, 10149, 372, 5034, 37, 49389, 271, 17400, 64117, 271, 84028, 18668, 271]\n",
            "[3228, 358, 14946, 271, 52, 36722, 474, 271, 52, 906, 5124, 372, 11252, 271, 73092, 1189, 50940, 16172, 49120, 271, 19697, 4815, 54, 5809, 13929, 28280, 271, 54, 5809, 5124, 6043, 3018, 86394, 1553, 271, 54, 5809, 9126, 12404, 7830, 271, 54, 1037, 328, 34869, 271, 54, 12173, 39874, 8611, 84, 271, 54, 38377, 435, 5809, 25611, 728, 271, 29784, 1466, 38866, 271, 29784, 1466, 7948, 88873, 271, 36154, 818, 47051, 399, 271, 36154, 818, 473, 714, 88, 271, 36154, 818, 445, 5809, 37235, 4469, 271, 906, 2668, 4650, 28769, 271, 54, 408, 88, 97260, 3315, 271, 54, 288, 31676, 285, 271, 54, 288, 3258, 24298, 271, 54, 288, 3258, 35393, 616, 271, 54, 288, 3258, 33491, 352, 271, 54, 1216, 3817, 469, 2221, 271, 54, 83518, 18878, 271, 53477, 11490, 1254, 2718, 271, 10149, 35674, 271, 10149, 66309, 271, 10149, 12036, 79, 6181, 271, 45887, 29492, 271, 45887, 56859, 271, 45887, 356, 426, 12359, 16014, 271, 45887, 25028, 271, 45887, 7462, 5730, 14767, 271, 45887, 70480, 1293, 271, 45887, 386, 553, 271, 45887, 432, 5809, 3279, 3252, 783, 271, 45887, 1443, 5100, 271, 45887, 60920, 370, 271, 45887, 328, 4010, 86, 5302, 271, 45887, 350, 5809, 35552, 60808, 271, 45887, 220, 1901, 5809, 220, 32196, 11, 220, 24200, 51, 6018, 2078, 11865, 11, 2160, 5809, 271, 10149, 372, 5034, 37, 49389, 271, 17400, 64117, 271, 84028, 18668, 271, 54, 337, 76413, 426, 4202, 271, 54, 337, 3516, 72, 42609, 271, 54, 647, 34321, 271, 41326, 220, 845, 25, 18395, 4794, 16213, 200, 200, 200, 200]\n",
            "--------------------\n",
            "[54071, 23063, 16347, 78, 271, 32974, 17200, 271, 32974, 41817, 438, 689, 271, 100224, 94005, 587, 271, 100224, 622, 50054, 271, 100224, 56497, 271, 100224, 8560, 276, 271, 100224, 13358, 283, 3018, 271, 32974, 3833, 445, 2319, 81, 271, 32974, 942, 29770, 271, 32974, 942, 435, 55526, 271, 52, 11253, 26316, 83, 31031, 271, 52, 657, 40310, 271, 20552, 3228, 358, 14946, 271, 52, 36722, 474, 271, 52, 906, 5124, 372, 11252, 271, 73092, 1189, 50940, 16172, 49120, 271, 19697, 4815, 54, 5809, 13929, 28280, 271, 54, 5809, 5124, 6043, 3018, 86394, 1553, 271, 54, 5809, 9126, 12404, 7830, 271, 54, 1037, 328, 34869, 271, 54, 12173, 39874, 8611, 84, 271, 54, 38377, 435, 5809, 25611, 728, 271, 29784, 1466, 38866, 271, 29784, 1466, 7948, 88873, 271, 36154, 818, 47051, 399, 271, 36154, 818, 473, 714, 88, 271, 36154, 818, 445, 5809, 37235, 4469, 271, 906, 2668, 4650, 28769, 271, 54, 408, 88, 97260, 3315, 271, 54, 288, 31676, 285, 271, 54, 288, 3258, 24298, 271, 54, 288, 3258, 35393, 616, 271, 54, 288, 3258, 33491, 352, 271, 54, 1216, 3817, 469, 2221, 271, 54, 83518, 18878, 271, 53477, 11490, 1254, 2718, 271, 10149, 35674, 271, 10149, 66309, 271, 10149, 12036, 79, 6181, 271, 45887, 29492, 271, 45887, 56859, 271, 45887, 356, 426, 12359, 16014, 271, 45887, 25028, 271, 45887, 7462, 5730, 14767, 271, 45887, 70480, 1293, 271, 45887, 386, 553, 271, 45887, 432, 5809, 3279, 3252, 783, 271, 45887, 1443, 5100, 271, 45887, 60920, 370, 271, 45887, 328, 4010, 86, 5302, 271, 45887, 350, 5809]\n",
            "[20552, 3228, 358, 14946, 271, 52, 36722, 474, 271, 52, 906, 5124, 372, 11252, 271, 73092, 1189, 50940, 16172, 49120, 271, 19697, 4815, 54, 5809, 13929, 28280, 271, 54, 5809, 5124, 6043, 3018, 86394, 1553, 271, 54, 5809, 9126, 12404, 7830, 271, 54, 1037, 328, 34869, 271, 54, 12173, 39874, 8611, 84, 271, 54, 38377, 435, 5809, 25611, 728, 271, 29784, 1466, 38866, 271, 29784, 1466, 7948, 88873, 271, 36154, 818, 47051, 399, 271, 36154, 818, 473, 714, 88, 271, 36154, 818, 445, 5809, 37235, 4469, 271, 906, 2668, 4650, 28769, 271, 54, 408, 88, 97260, 3315, 271, 54, 288, 31676, 285, 271, 54, 288, 3258, 24298, 271, 54, 288, 3258, 35393, 616, 271, 54, 288, 3258, 33491, 352, 271, 54, 1216, 3817, 469, 2221, 271, 54, 83518, 18878, 271, 53477, 11490, 1254, 2718, 271, 10149, 35674, 271, 10149, 66309, 271, 10149, 12036, 79, 6181, 271, 45887, 29492, 271, 45887, 56859, 271, 45887, 356, 426, 12359, 16014, 271, 45887, 25028, 271, 45887, 7462, 5730, 14767, 271, 45887, 70480, 1293, 271, 45887, 386, 553, 271, 45887, 432, 5809, 3279, 3252, 783, 271, 45887, 1443, 5100, 271, 45887, 60920, 370, 271, 45887, 328, 4010, 86, 5302, 271, 45887, 350, 5809, 35552, 60808, 271, 45887, 220, 1901, 5809, 220, 32196, 11, 220, 24200, 51, 6018, 2078, 11865, 11, 2160, 5809, 271, 10149, 372, 5034, 37, 49389, 271, 17400, 64117, 271, 84028, 18668, 271, 54, 337, 76413, 426, 4202, 271, 54, 337, 3516, 72, 42609, 271, 54, 647, 34321, 271, 41326, 220, 845, 25, 18395, 4794, 16213, 200, 200, 200]\n",
            "--------------------\n",
            "[71, 271, 1305, 2159, 426, 14468, 18615, 271, 51, 3433, 86962, 271, 51, 3433, 58253, 271, 51, 3433, 622, 32666, 1126, 271, 22170, 764, 3205, 30969, 271, 53893, 4168, 32945, 33680, 271, 54071, 23063, 16347, 78, 271, 32974, 17200, 271, 32974, 41817, 438, 689, 271, 100224, 94005, 587, 271, 100224, 622, 50054, 271, 100224, 56497, 271, 100224, 8560, 276, 271, 100224, 13358, 283, 3018, 271, 32974, 3833, 445, 2319, 81, 271, 32974, 942, 29770, 271, 32974, 942, 435, 55526, 271, 52, 11253, 26316, 83, 31031, 271, 52, 657, 40310, 271, 20552, 3228, 358, 14946, 271, 52, 36722, 474, 271, 52, 906, 5124, 372, 11252, 271, 73092, 1189, 50940, 16172, 49120, 271, 19697, 4815, 54, 5809, 13929, 28280, 271, 54, 5809, 5124, 6043, 3018, 86394, 1553, 271, 54, 5809, 9126, 12404, 7830, 271, 54, 1037, 328, 34869, 271, 54, 12173, 39874, 8611, 84, 271, 54, 38377, 435, 5809, 25611, 728, 271, 29784, 1466, 38866, 271, 29784, 1466, 7948, 88873, 271, 36154, 818, 47051, 399, 271, 36154, 818, 473, 714, 88, 271, 36154, 818, 445, 5809, 37235, 4469, 271, 906, 2668, 4650, 28769, 271, 54, 408, 88, 97260, 3315, 271, 54, 288, 31676, 285, 271, 54, 288, 3258, 24298, 271, 54, 288, 3258, 35393, 616, 271, 54, 288, 3258, 33491, 352, 271, 54, 1216, 3817, 469, 2221, 271, 54, 83518, 18878, 271, 53477, 11490, 1254, 2718, 271, 10149, 35674, 271, 10149, 66309, 271, 10149, 12036, 79, 6181, 271, 45887, 29492, 271, 45887, 56859, 271, 45887, 356, 426, 12359, 16014, 271, 45887, 25028, 271, 45887, 7462, 5730, 14767, 271]\n",
            "[271, 20552, 3228, 358, 14946, 271, 52, 36722, 474, 271, 52, 906, 5124, 372, 11252, 271, 73092, 1189, 50940, 16172, 49120, 271, 19697, 4815, 54, 5809, 13929, 28280, 271, 54, 5809, 5124, 6043, 3018, 86394, 1553, 271, 54, 5809, 9126, 12404, 7830, 271, 54, 1037, 328, 34869, 271, 54, 12173, 39874, 8611, 84, 271, 54, 38377, 435, 5809, 25611, 728, 271, 29784, 1466, 38866, 271, 29784, 1466, 7948, 88873, 271, 36154, 818, 47051, 399, 271, 36154, 818, 473, 714, 88, 271, 36154, 818, 445, 5809, 37235, 4469, 271, 906, 2668, 4650, 28769, 271, 54, 408, 88, 97260, 3315, 271, 54, 288, 31676, 285, 271, 54, 288, 3258, 24298, 271, 54, 288, 3258, 35393, 616, 271, 54, 288, 3258, 33491, 352, 271, 54, 1216, 3817, 469, 2221, 271, 54, 83518, 18878, 271, 53477, 11490, 1254, 2718, 271, 10149, 35674, 271, 10149, 66309, 271, 10149, 12036, 79, 6181, 271, 45887, 29492, 271, 45887, 56859, 271, 45887, 356, 426, 12359, 16014, 271, 45887, 25028, 271, 45887, 7462, 5730, 14767, 271, 45887, 70480, 1293, 271, 45887, 386, 553, 271, 45887, 432, 5809, 3279, 3252, 783, 271, 45887, 1443, 5100, 271, 45887, 60920, 370, 271, 45887, 328, 4010, 86, 5302, 271, 45887, 350, 5809, 35552, 60808, 271, 45887, 220, 1901, 5809, 220, 32196, 11, 220, 24200, 51, 6018, 2078, 11865, 11, 2160, 5809, 271, 10149, 372, 5034, 37, 49389, 271, 17400, 64117, 271, 84028, 18668, 271, 54, 337, 76413, 426, 4202, 271, 54, 337, 3516, 72, 42609, 271, 54, 647, 34321, 271, 41326, 220, 845, 25, 18395, 4794, 16213, 200, 200]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set pad token for Llama tokenizer\n",
        "\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<|finetune_right_pad_id|>\"})\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "print(\"*\"*40)\n",
        "print(f\"Tokenizer vocab size before adding pad token: {len(tokenizer)}\")\n",
        "print(f\"Tokenizer vocab size after adding pad token: {len(tokenizer)}\")\n",
        "print('Tokenizer pad token:', tokenizer.pad_token)\n",
        "print('Tokenizer pad token id:', tokenizer.pad_token_id)\n",
        "print('Model pad token id:', model.config.pad_token_id)\n",
        "print(\"*\"*40)\n",
        "\n",
        "\n",
        "# inizialize data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cyYT3RkubWx",
        "outputId": "88fa48c2-6a7a-40b3-d200-1a1405f9ee43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Tokenizer vocab size before adding pad token: 128256\n",
            "Tokenizer vocab size after adding pad token: 128256\n",
            "Tokenizer pad token: <|finetune_right_pad_id|>\n",
            "Tokenizer pad token id: 128004\n",
            "Model pad token id: 128004\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check that the data collator produces output as we expect\n",
        "\n",
        "import torch\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_chunked,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=2\n",
        "    )\n",
        "\n",
        "def occurrence_token_id(tensor, token_id):\n",
        "  mask = token_id == tensor\n",
        "  return mask.sum().item()\n",
        "\n",
        "i = 0\n",
        "for batch in dataloader:\n",
        "      print()\n",
        "      print(f\"Sequences length: {len(batch['input_ids'][0])}\")\n",
        "      print(f\"Padding tokens in sequence 1: {occurrence_token_id(batch['input_ids'][0], tokenizer.pad_token_id)}\")\n",
        "      print(f\"Padding tokens in sequence 2: {occurrence_token_id(batch['input_ids'][1], tokenizer.pad_token_id)}\")\n",
        "      print(f\"Sequence 1 decoded:\\n {tokenizer.decode(batch['input_ids'][0])}\")\n",
        "      print(f\"\\nSequence 2 decoded:\\n {tokenizer.decode(batch['input_ids'][1])}\")\n",
        "      print(f\"Actual input to Trainer:\")\n",
        "      print(batch)\n",
        "      print(\"*\"*50)\n",
        "      if i == 2:\n",
        "        break\n",
        "      i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_RdRytl3vKYN",
        "outputId": "62940599-7b87-4236-b9f5-faf6c79d04be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequences length: 256\n",
            "Padding tokens in sequence 1: 0\n",
            "Padding tokens in sequence 2: 0\n",
            "Sequence 1 decoded:\n",
            " <|begin_of_text|>\f\fOriginally written by \n",
            "\n",
            "Sandy Petersen\n",
            "\n",
            "With later revision by \n",
            "\n",
            "Lynn Willis\n",
            "\n",
            "This revised 7th Edition is a collaboration between \n",
            "\n",
            "Paul Fricker and Mike Mason\n",
            "\n",
            "Editorial: Scott Dorward, Badger McInnes, Mike Mason, Charlie Krank\n",
            "\n",
            "Design Format: Badger McInnes and Mike Mason\n",
            "\n",
            "Layout: Badger McInnes, Nicholas Nacario, Charlie Krank\n",
            "\n",
            "Art Direction: Mike Mason, Meghan McLean, Daniel Skomorowski\n",
            "\n",
            "Cover Illustration: Sam Lamont\n",
            "\n",
            "Chapter Illustrations: Jonathan Wyke, Paul Carrick, Rob Gould, François Launet, Victor Leza,  \n",
            "Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz\n",
            "\n",
            "Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  \n",
            "Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte\n",
            "\n",
            "Cristoforo Font created by: Thomas Phinney\n",
            "\n",
            "Cartography: Steff Worthington\n",
            "\n",
            "For Lynn Willis\n",
            "\n",
            "\fAcknowledgements \n",
            "The authors would like to thank the following people for their \n",
            "ongo\n",
            "\n",
            "Sequence 2 decoded:\n",
            " Paul Fricker and Mike Mason\n",
            "\n",
            "Editorial: Scott Dorward, Badger McInnes, Mike Mason, Charlie Krank\n",
            "\n",
            "Design Format: Badger McInnes and Mike Mason\n",
            "\n",
            "Layout: Badger McInnes, Nicholas Nacario, Charlie Krank\n",
            "\n",
            "Art Direction: Mike Mason, Meghan McLean, Daniel Skomorowski\n",
            "\n",
            "Cover Illustration: Sam Lamont\n",
            "\n",
            "Chapter Illustrations: Jonathan Wyke, Paul Carrick, Rob Gould, François Launet, Victor Leza,  \n",
            "Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz\n",
            "\n",
            "Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  \n",
            "Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte\n",
            "\n",
            "Cristoforo Font created by: Thomas Phinney\n",
            "\n",
            "Cartography: Steff Worthington\n",
            "\n",
            "For Lynn Willis\n",
            "\n",
            "\fAcknowledgements \n",
            "The authors would like to thank the following people for their \n",
            "ongoing  support  and  assistance:  Charlie  Krank,  Christian \n",
            "Grussi, Keary Birch, Alan Bligh, John French, Scott\n",
            "Actual input to Trainer:\n",
            "{'input_ids': tensor([[128000,    200,    200,  38363,   5439,    555,   4815,     50,  13634,\n",
            "          32284,    268,    271,   2409,   3010,  24493,    555,   4815,     43,\n",
            "          59360,  67742,    271,   2028,  32828,    220,     22,    339,  14398,\n",
            "            374,    264,  20632,   1990,   4815,  26368,   2939,   5327,    323,\n",
            "          11519,  29927,    271,   9597,    532,     25,  10016,  25225,   1637,\n",
            "             11,  11717,   1414,   4584,    644,   4978,     11,  11519,  29927,\n",
            "             11,  25972,  93070,    271,  21103,  15392,     25,  11717,   1414,\n",
            "           4584,    644,   4978,    323,  11519,  29927,    271,   2224,     25,\n",
            "          11717,   1414,   4584,    644,   4978,     11,  40796,    452,    582,\n",
            "           3370,     11,  25972,  93070,    271,   9470,  19438,     25,  11519,\n",
            "          29927,     11,  80134,   4584,  49862,     11,  15469,   4923, 121827,\n",
            "          29384,    271,  31996,  39154,    367,     25,   8388,  33794,    546,\n",
            "            271,  26072,  39154,    811,     25,  24150,  24100,    441,     11,\n",
            "           7043,  30474,    875,     11,   4997,  85431,     11,  85807,   5034,\n",
            "            359,    295,     11,  33412,   2009,   4458,     11,   2355,  54567,\n",
            "          56728,     11,  11519,  31421,     11,  40796,  19197,   1601,     11,\n",
            "          23245,  34297,     11,    735,  96893,  54090,     89,    271,  86225,\n",
            "          39154,    811,     25,    220,  32532,  99666,     11,   2895,   6374,\n",
            "             11,  11517,    473,    952,     11,   6621,  38672,     66,  15130,\n",
            "           4341,     11,   7043,  30474,    875,     11,  10016,  34221,     11,\n",
            "           2355,  66091,  24100,    441,     11,  33412,   2009,   4458,     11,\n",
            "           8388,  33794,    546,     11,  23519,   5455,    426,   2639,    309,\n",
            "             11,  92664,    329,     11,  23245,   4802,  10333,     11,   8529,\n",
            "          43089,     11,  35281,    386,  16977,    271,     34,   2889,   1073,\n",
            "          18812,   9757,   3549,    555,     25,  11355,   2405,    258,   3520,\n",
            "            271,  12878,   5814,     25,   3441,    544,  36636,   1626,    783,\n",
            "            271,   2520,  45638,  67742,    271,    200,  91336,  84127,    720,\n",
            "            791,  12283,   1053,   1093,    311,   9901,    279,   2768,   1274,\n",
            "            369,    872,    720,   6497],\n",
            "        [ 26368,   2939,   5327,    323,  11519,  29927,    271,   9597,    532,\n",
            "             25,  10016,  25225,   1637,     11,  11717,   1414,   4584,    644,\n",
            "           4978,     11,  11519,  29927,     11,  25972,  93070,    271,  21103,\n",
            "          15392,     25,  11717,   1414,   4584,    644,   4978,    323,  11519,\n",
            "          29927,    271,   2224,     25,  11717,   1414,   4584,    644,   4978,\n",
            "             11,  40796,    452,    582,   3370,     11,  25972,  93070,    271,\n",
            "           9470,  19438,     25,  11519,  29927,     11,  80134,   4584,  49862,\n",
            "             11,  15469,   4923, 121827,  29384,    271,  31996,  39154,    367,\n",
            "             25,   8388,  33794,    546,    271,  26072,  39154,    811,     25,\n",
            "          24150,  24100,    441,     11,   7043,  30474,    875,     11,   4997,\n",
            "          85431,     11,  85807,   5034,    359,    295,     11,  33412,   2009,\n",
            "           4458,     11,   2355,  54567,  56728,     11,  11519,  31421,     11,\n",
            "          40796,  19197,   1601,     11,  23245,  34297,     11,    735,  96893,\n",
            "          54090,     89,    271,  86225,  39154,    811,     25,    220,  32532,\n",
            "          99666,     11,   2895,   6374,     11,  11517,    473,    952,     11,\n",
            "           6621,  38672,     66,  15130,   4341,     11,   7043,  30474,    875,\n",
            "             11,  10016,  34221,     11,   2355,  66091,  24100,    441,     11,\n",
            "          33412,   2009,   4458,     11,   8388,  33794,    546,     11,  23519,\n",
            "           5455,    426,   2639,    309,     11,  92664,    329,     11,  23245,\n",
            "           4802,  10333,     11,   8529,  43089,     11,  35281,    386,  16977,\n",
            "            271,     34,   2889,   1073,  18812,   9757,   3549,    555,     25,\n",
            "          11355,   2405,    258,   3520,    271,  12878,   5814,     25,   3441,\n",
            "            544,  36636,   1626,    783,    271,   2520,  45638,  67742,    271,\n",
            "            200,  91336,  84127,    720,    791,  12283,   1053,   1093,    311,\n",
            "           9901,    279,   2768,   1274,    369,    872,    720,   6497,    287,\n",
            "            220,   1862,    220,    323,    220,  13291,     25,    220,  25972,\n",
            "            220,  93070,     11,    220,   9052,    720,   6600,   1892,     72,\n",
            "             11,   6706,    661,  88088,     11,  26349,   2563,   1108,     11,\n",
            "           3842,   8753,     11,  10016]]), 'length': tensor([256, 256]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[128000,    200,    200,  38363,   5439,    555,   4815,     50,  13634,\n",
            "          32284,    268,    271,   2409,   3010,  24493,    555,   4815,     43,\n",
            "          59360,  67742,    271,   2028,  32828,    220,     22,    339,  14398,\n",
            "            374,    264,  20632,   1990,   4815,  26368,   2939,   5327,    323,\n",
            "          11519,  29927,    271,   9597,    532,     25,  10016,  25225,   1637,\n",
            "             11,  11717,   1414,   4584,    644,   4978,     11,  11519,  29927,\n",
            "             11,  25972,  93070,    271,  21103,  15392,     25,  11717,   1414,\n",
            "           4584,    644,   4978,    323,  11519,  29927,    271,   2224,     25,\n",
            "          11717,   1414,   4584,    644,   4978,     11,  40796,    452,    582,\n",
            "           3370,     11,  25972,  93070,    271,   9470,  19438,     25,  11519,\n",
            "          29927,     11,  80134,   4584,  49862,     11,  15469,   4923, 121827,\n",
            "          29384,    271,  31996,  39154,    367,     25,   8388,  33794,    546,\n",
            "            271,  26072,  39154,    811,     25,  24150,  24100,    441,     11,\n",
            "           7043,  30474,    875,     11,   4997,  85431,     11,  85807,   5034,\n",
            "            359,    295,     11,  33412,   2009,   4458,     11,   2355,  54567,\n",
            "          56728,     11,  11519,  31421,     11,  40796,  19197,   1601,     11,\n",
            "          23245,  34297,     11,    735,  96893,  54090,     89,    271,  86225,\n",
            "          39154,    811,     25,    220,  32532,  99666,     11,   2895,   6374,\n",
            "             11,  11517,    473,    952,     11,   6621,  38672,     66,  15130,\n",
            "           4341,     11,   7043,  30474,    875,     11,  10016,  34221,     11,\n",
            "           2355,  66091,  24100,    441,     11,  33412,   2009,   4458,     11,\n",
            "           8388,  33794,    546,     11,  23519,   5455,    426,   2639,    309,\n",
            "             11,  92664,    329,     11,  23245,   4802,  10333,     11,   8529,\n",
            "          43089,     11,  35281,    386,  16977,    271,     34,   2889,   1073,\n",
            "          18812,   9757,   3549,    555,     25,  11355,   2405,    258,   3520,\n",
            "            271,  12878,   5814,     25,   3441,    544,  36636,   1626,    783,\n",
            "            271,   2520,  45638,  67742,    271,    200,  91336,  84127,    720,\n",
            "            791,  12283,   1053,   1093,    311,   9901,    279,   2768,   1274,\n",
            "            369,    872,    720,   6497],\n",
            "        [ 26368,   2939,   5327,    323,  11519,  29927,    271,   9597,    532,\n",
            "             25,  10016,  25225,   1637,     11,  11717,   1414,   4584,    644,\n",
            "           4978,     11,  11519,  29927,     11,  25972,  93070,    271,  21103,\n",
            "          15392,     25,  11717,   1414,   4584,    644,   4978,    323,  11519,\n",
            "          29927,    271,   2224,     25,  11717,   1414,   4584,    644,   4978,\n",
            "             11,  40796,    452,    582,   3370,     11,  25972,  93070,    271,\n",
            "           9470,  19438,     25,  11519,  29927,     11,  80134,   4584,  49862,\n",
            "             11,  15469,   4923, 121827,  29384,    271,  31996,  39154,    367,\n",
            "             25,   8388,  33794,    546,    271,  26072,  39154,    811,     25,\n",
            "          24150,  24100,    441,     11,   7043,  30474,    875,     11,   4997,\n",
            "          85431,     11,  85807,   5034,    359,    295,     11,  33412,   2009,\n",
            "           4458,     11,   2355,  54567,  56728,     11,  11519,  31421,     11,\n",
            "          40796,  19197,   1601,     11,  23245,  34297,     11,    735,  96893,\n",
            "          54090,     89,    271,  86225,  39154,    811,     25,    220,  32532,\n",
            "          99666,     11,   2895,   6374,     11,  11517,    473,    952,     11,\n",
            "           6621,  38672,     66,  15130,   4341,     11,   7043,  30474,    875,\n",
            "             11,  10016,  34221,     11,   2355,  66091,  24100,    441,     11,\n",
            "          33412,   2009,   4458,     11,   8388,  33794,    546,     11,  23519,\n",
            "           5455,    426,   2639,    309,     11,  92664,    329,     11,  23245,\n",
            "           4802,  10333,     11,   8529,  43089,     11,  35281,    386,  16977,\n",
            "            271,     34,   2889,   1073,  18812,   9757,   3549,    555,     25,\n",
            "          11355,   2405,    258,   3520,    271,  12878,   5814,     25,   3441,\n",
            "            544,  36636,   1626,    783,    271,   2520,  45638,  67742,    271,\n",
            "            200,  91336,  84127,    720,    791,  12283,   1053,   1093,    311,\n",
            "           9901,    279,   2768,   1274,    369,    872,    720,   6497,    287,\n",
            "            220,   1862,    220,    323,    220,  13291,     25,    220,  25972,\n",
            "            220,  93070,     11,    220,   9052,    720,   6600,   1892,     72,\n",
            "             11,   6706,    661,  88088,     11,  26349,   2563,   1108,     11,\n",
            "           3842,   8753,     11,  10016]])}\n",
            "**************************************************\n",
            "\n",
            "Sequences length: 256\n",
            "Padding tokens in sequence 1: 0\n",
            "Padding tokens in sequence 2: 0\n",
            "Sequence 1 decoded:\n",
            " Innes and Mike Mason\n",
            "\n",
            "Layout: Badger McInnes, Nicholas Nacario, Charlie Krank\n",
            "\n",
            "Art Direction: Mike Mason, Meghan McLean, Daniel Skomorowski\n",
            "\n",
            "Cover Illustration: Sam Lamont\n",
            "\n",
            "Chapter Illustrations: Jonathan Wyke, Paul Carrick, Rob Gould, François Launet, Victor Leza,  \n",
            "Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz\n",
            "\n",
            "Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  \n",
            "Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte\n",
            "\n",
            "Cristoforo Font created by: Thomas Phinney\n",
            "\n",
            "Cartography: Steff Worthington\n",
            "\n",
            "For Lynn Willis\n",
            "\n",
            "\fAcknowledgements \n",
            "The authors would like to thank the following people for their \n",
            "ongoing  support  and  assistance:  Charlie  Krank,  Christian \n",
            "Grussi, Keary Birch, Alan Bligh, John French, Scott Dorward, \n",
            "Matthew  Sanderson,  Dean  Engelhardt,  Matt  Anderson, \n",
            "Tim Vincent, Kevin White, Garrie Hall, Pedro\n",
            "\n",
            "Sequence 2 decoded:\n",
            "  Daniel Skomorowski\n",
            "\n",
            "Cover Illustration: Sam Lamont\n",
            "\n",
            "Chapter Illustrations: Jonathan Wyke, Paul Carrick, Rob Gould, François Launet, Victor Leza,  \n",
            "Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz\n",
            "\n",
            "Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  \n",
            "Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte\n",
            "\n",
            "Cristoforo Font created by: Thomas Phinney\n",
            "\n",
            "Cartography: Steff Worthington\n",
            "\n",
            "For Lynn Willis\n",
            "\n",
            "\fAcknowledgements \n",
            "The authors would like to thank the following people for their \n",
            "ongoing  support  and  assistance:  Charlie  Krank,  Christian \n",
            "Grussi, Keary Birch, Alan Bligh, John French, Scott Dorward, \n",
            "Matthew  Sanderson,  Dean  Engelhardt,  Matt  Anderson, \n",
            "Tim Vincent, Kevin White, Garrie Hall, Pedro Ziviani, Dan \n",
            "Kramer, Scott David Aniolowski, Brian Courtemache, Brian \n",
            "Sammons, Chad Bowser, Tom Lynch,\n",
            "Actual input to Trainer:\n",
            "{'input_ids': tensor([[   644,   4978,    323,  11519,  29927,    271,   2224,     25,  11717,\n",
            "           1414,   4584,    644,   4978,     11,  40796,    452,    582,   3370,\n",
            "             11,  25972,  93070,    271,   9470,  19438,     25,  11519,  29927,\n",
            "             11,  80134,   4584,  49862,     11,  15469,   4923, 121827,  29384,\n",
            "            271,  31996,  39154,    367,     25,   8388,  33794,    546,    271,\n",
            "          26072,  39154,    811,     25,  24150,  24100,    441,     11,   7043,\n",
            "          30474,    875,     11,   4997,  85431,     11,  85807,   5034,    359,\n",
            "            295,     11,  33412,   2009,   4458,     11,   2355,  54567,  56728,\n",
            "             11,  11519,  31421,     11,  40796,  19197,   1601,     11,  23245,\n",
            "          34297,     11,    735,  96893,  54090,     89,    271,  86225,  39154,\n",
            "            811,     25,    220,  32532,  99666,     11,   2895,   6374,     11,\n",
            "          11517,    473,    952,     11,   6621,  38672,     66,  15130,   4341,\n",
            "             11,   7043,  30474,    875,     11,  10016,  34221,     11,   2355,\n",
            "          66091,  24100,    441,     11,  33412,   2009,   4458,     11,   8388,\n",
            "          33794,    546,     11,  23519,   5455,    426,   2639,    309,     11,\n",
            "          92664,    329,     11,  23245,   4802,  10333,     11,   8529,  43089,\n",
            "             11,  35281,    386,  16977,    271,     34,   2889,   1073,  18812,\n",
            "           9757,   3549,    555,     25,  11355,   2405,    258,   3520,    271,\n",
            "          12878,   5814,     25,   3441,    544,  36636,   1626,    783,    271,\n",
            "           2520,  45638,  67742,    271,    200,  91336,  84127,    720,    791,\n",
            "          12283,   1053,   1093,    311,   9901,    279,   2768,   1274,    369,\n",
            "            872,    720,   6497,    287,    220,   1862,    220,    323,    220,\n",
            "          13291,     25,    220,  25972,    220,  93070,     11,    220,   9052,\n",
            "            720,   6600,   1892,     72,     11,   6706,    661,  88088,     11,\n",
            "          26349,   2563,   1108,     11,   3842,   8753,     11,  10016,  25225,\n",
            "           1637,     11,    720,  50988,    220,   8847,   1293,     11,    220,\n",
            "          25028,    220,  91990,  60382,     11,    220,  13678,    220,  21293,\n",
            "             11,    720,  20830,  35407,     11,  16768,   5929,     11,  12471,\n",
            "           7379,  11166,     11,  43582],\n",
            "        [ 15469,   4923, 121827,  29384,    271,  31996,  39154,    367,     25,\n",
            "           8388,  33794,    546,    271,  26072,  39154,    811,     25,  24150,\n",
            "          24100,    441,     11,   7043,  30474,    875,     11,   4997,  85431,\n",
            "             11,  85807,   5034,    359,    295,     11,  33412,   2009,   4458,\n",
            "             11,   2355,  54567,  56728,     11,  11519,  31421,     11,  40796,\n",
            "          19197,   1601,     11,  23245,  34297,     11,    735,  96893,  54090,\n",
            "             89,    271,  86225,  39154,    811,     25,    220,  32532,  99666,\n",
            "             11,   2895,   6374,     11,  11517,    473,    952,     11,   6621,\n",
            "          38672,     66,  15130,   4341,     11,   7043,  30474,    875,     11,\n",
            "          10016,  34221,     11,   2355,  66091,  24100,    441,     11,  33412,\n",
            "           2009,   4458,     11,   8388,  33794,    546,     11,  23519,   5455,\n",
            "            426,   2639,    309,     11,  92664,    329,     11,  23245,   4802,\n",
            "          10333,     11,   8529,  43089,     11,  35281,    386,  16977,    271,\n",
            "             34,   2889,   1073,  18812,   9757,   3549,    555,     25,  11355,\n",
            "           2405,    258,   3520,    271,  12878,   5814,     25,   3441,    544,\n",
            "          36636,   1626,    783,    271,   2520,  45638,  67742,    271,    200,\n",
            "          91336,  84127,    720,    791,  12283,   1053,   1093,    311,   9901,\n",
            "            279,   2768,   1274,    369,    872,    720,   6497,    287,    220,\n",
            "           1862,    220,    323,    220,  13291,     25,    220,  25972,    220,\n",
            "          93070,     11,    220,   9052,    720,   6600,   1892,     72,     11,\n",
            "           6706,    661,  88088,     11,  26349,   2563,   1108,     11,   3842,\n",
            "           8753,     11,  10016,  25225,   1637,     11,    720,  50988,    220,\n",
            "           8847,   1293,     11,    220,  25028,    220,  91990,  60382,     11,\n",
            "            220,  13678,    220,  21293,     11,    720,  20830,  35407,     11,\n",
            "          16768,   5929,     11,  12471,   7379,  11166,     11,  43582,   1901,\n",
            "            344,  47547,     11,  11824,    720,     42,  47469,     11,  10016,\n",
            "           6941,   1556,  25947,  29384,     11,  17520,   6377,    880,   1815,\n",
            "             11,  17520,    720,  24903,  24483,     11,  43130,  18943,    805,\n",
            "             11,   8529,  38206,     11]]), 'length': tensor([256, 256]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   644,   4978,    323,  11519,  29927,    271,   2224,     25,  11717,\n",
            "           1414,   4584,    644,   4978,     11,  40796,    452,    582,   3370,\n",
            "             11,  25972,  93070,    271,   9470,  19438,     25,  11519,  29927,\n",
            "             11,  80134,   4584,  49862,     11,  15469,   4923, 121827,  29384,\n",
            "            271,  31996,  39154,    367,     25,   8388,  33794,    546,    271,\n",
            "          26072,  39154,    811,     25,  24150,  24100,    441,     11,   7043,\n",
            "          30474,    875,     11,   4997,  85431,     11,  85807,   5034,    359,\n",
            "            295,     11,  33412,   2009,   4458,     11,   2355,  54567,  56728,\n",
            "             11,  11519,  31421,     11,  40796,  19197,   1601,     11,  23245,\n",
            "          34297,     11,    735,  96893,  54090,     89,    271,  86225,  39154,\n",
            "            811,     25,    220,  32532,  99666,     11,   2895,   6374,     11,\n",
            "          11517,    473,    952,     11,   6621,  38672,     66,  15130,   4341,\n",
            "             11,   7043,  30474,    875,     11,  10016,  34221,     11,   2355,\n",
            "          66091,  24100,    441,     11,  33412,   2009,   4458,     11,   8388,\n",
            "          33794,    546,     11,  23519,   5455,    426,   2639,    309,     11,\n",
            "          92664,    329,     11,  23245,   4802,  10333,     11,   8529,  43089,\n",
            "             11,  35281,    386,  16977,    271,     34,   2889,   1073,  18812,\n",
            "           9757,   3549,    555,     25,  11355,   2405,    258,   3520,    271,\n",
            "          12878,   5814,     25,   3441,    544,  36636,   1626,    783,    271,\n",
            "           2520,  45638,  67742,    271,    200,  91336,  84127,    720,    791,\n",
            "          12283,   1053,   1093,    311,   9901,    279,   2768,   1274,    369,\n",
            "            872,    720,   6497,    287,    220,   1862,    220,    323,    220,\n",
            "          13291,     25,    220,  25972,    220,  93070,     11,    220,   9052,\n",
            "            720,   6600,   1892,     72,     11,   6706,    661,  88088,     11,\n",
            "          26349,   2563,   1108,     11,   3842,   8753,     11,  10016,  25225,\n",
            "           1637,     11,    720,  50988,    220,   8847,   1293,     11,    220,\n",
            "          25028,    220,  91990,  60382,     11,    220,  13678,    220,  21293,\n",
            "             11,    720,  20830,  35407,     11,  16768,   5929,     11,  12471,\n",
            "           7379,  11166,     11,  43582],\n",
            "        [ 15469,   4923, 121827,  29384,    271,  31996,  39154,    367,     25,\n",
            "           8388,  33794,    546,    271,  26072,  39154,    811,     25,  24150,\n",
            "          24100,    441,     11,   7043,  30474,    875,     11,   4997,  85431,\n",
            "             11,  85807,   5034,    359,    295,     11,  33412,   2009,   4458,\n",
            "             11,   2355,  54567,  56728,     11,  11519,  31421,     11,  40796,\n",
            "          19197,   1601,     11,  23245,  34297,     11,    735,  96893,  54090,\n",
            "             89,    271,  86225,  39154,    811,     25,    220,  32532,  99666,\n",
            "             11,   2895,   6374,     11,  11517,    473,    952,     11,   6621,\n",
            "          38672,     66,  15130,   4341,     11,   7043,  30474,    875,     11,\n",
            "          10016,  34221,     11,   2355,  66091,  24100,    441,     11,  33412,\n",
            "           2009,   4458,     11,   8388,  33794,    546,     11,  23519,   5455,\n",
            "            426,   2639,    309,     11,  92664,    329,     11,  23245,   4802,\n",
            "          10333,     11,   8529,  43089,     11,  35281,    386,  16977,    271,\n",
            "             34,   2889,   1073,  18812,   9757,   3549,    555,     25,  11355,\n",
            "           2405,    258,   3520,    271,  12878,   5814,     25,   3441,    544,\n",
            "          36636,   1626,    783,    271,   2520,  45638,  67742,    271,    200,\n",
            "          91336,  84127,    720,    791,  12283,   1053,   1093,    311,   9901,\n",
            "            279,   2768,   1274,    369,    872,    720,   6497,    287,    220,\n",
            "           1862,    220,    323,    220,  13291,     25,    220,  25972,    220,\n",
            "          93070,     11,    220,   9052,    720,   6600,   1892,     72,     11,\n",
            "           6706,    661,  88088,     11,  26349,   2563,   1108,     11,   3842,\n",
            "           8753,     11,  10016,  25225,   1637,     11,    720,  50988,    220,\n",
            "           8847,   1293,     11,    220,  25028,    220,  91990,  60382,     11,\n",
            "            220,  13678,    220,  21293,     11,    720,  20830,  35407,     11,\n",
            "          16768,   5929,     11,  12471,   7379,  11166,     11,  43582,   1901,\n",
            "            344,  47547,     11,  11824,    720,     42,  47469,     11,  10016,\n",
            "           6941,   1556,  25947,  29384,     11,  17520,   6377,    880,   1815,\n",
            "             11,  17520,    720,  24903,  24483,     11,  43130,  18943,    805,\n",
            "             11,   8529,  38206,     11]])}\n",
            "**************************************************\n",
            "\n",
            "Sequences length: 256\n",
            "Padding tokens in sequence 1: 0\n",
            "Padding tokens in sequence 2: 0\n",
            "Sequence 1 decoded:\n",
            " , Victor Leza,  \n",
            "Charles Wong, Mike Perry, Nicholas Cloister, Antonio Luis, Kalli Schulz\n",
            "\n",
            "Interior Illustrations:  Rachel Kahn, Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  \n",
            "Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte\n",
            "\n",
            "Cristoforo Font created by: Thomas Phinney\n",
            "\n",
            "Cartography: Steff Worthington\n",
            "\n",
            "For Lynn Willis\n",
            "\n",
            "\fAcknowledgements \n",
            "The authors would like to thank the following people for their \n",
            "ongoing  support  and  assistance:  Charlie  Krank,  Christian \n",
            "Grussi, Keary Birch, Alan Bligh, John French, Scott Dorward, \n",
            "Matthew  Sanderson,  Dean  Engelhardt,  Matt  Anderson, \n",
            "Tim Vincent, Kevin White, Garrie Hall, Pedro Ziviani, Dan \n",
            "Kramer, Scott David Aniolowski, Brian Courtemache, Brian \n",
            "Sammons, Chad Bowser, Tom Lynch, Andrew Leman of the \n",
            "HPLHS, and of course Sandy Petersen, without whom none \n",
            "of this would have happened! \n",
            "\n",
            "Dedications\n",
            "\n",
            "\n",
            "Sequence 2 decoded:\n",
            "  Grilla, Chris Huth, Loïc Muzy, Paul Carrick, Scott Neil,  \n",
            "Jonathan Wyke, Victor Leza, Sam Lamont, Celeste Burcham, Caryad, Antonio Mainez, Tom Sullivan, Marco Morte\n",
            "\n",
            "Cristoforo Font created by: Thomas Phinney\n",
            "\n",
            "Cartography: Steff Worthington\n",
            "\n",
            "For Lynn Willis\n",
            "\n",
            "\fAcknowledgements \n",
            "The authors would like to thank the following people for their \n",
            "ongoing  support  and  assistance:  Charlie  Krank,  Christian \n",
            "Grussi, Keary Birch, Alan Bligh, John French, Scott Dorward, \n",
            "Matthew  Sanderson,  Dean  Engelhardt,  Matt  Anderson, \n",
            "Tim Vincent, Kevin White, Garrie Hall, Pedro Ziviani, Dan \n",
            "Kramer, Scott David Aniolowski, Brian Courtemache, Brian \n",
            "Sammons, Chad Bowser, Tom Lynch, Andrew Leman of the \n",
            "HPLHS, and of course Sandy Petersen, without whom none \n",
            "of this would have happened! \n",
            "\n",
            "Dedications\n",
            "To my father, who introduced me to Lovecraft and to science \n",
            "fiction in general. From one of his books I read my first Love-\n",
            "craftian story\n",
            "Actual input to Trainer:\n",
            "{'input_ids': tensor([[   11, 33412,  2009,  4458,    11,  2355, 54567, 56728,    11, 11519,\n",
            "         31421,    11, 40796, 19197,  1601,    11, 23245, 34297,    11,   735,\n",
            "         96893, 54090,    89,   271, 86225, 39154,   811,    25,   220, 32532,\n",
            "         99666,    11,  2895,  6374,    11, 11517,   473,   952,    11,  6621,\n",
            "         38672,    66, 15130,  4341,    11,  7043, 30474,   875,    11, 10016,\n",
            "         34221,    11,  2355, 66091, 24100,   441,    11, 33412,  2009,  4458,\n",
            "            11,  8388, 33794,   546,    11, 23519,  5455,   426,  2639,   309,\n",
            "            11, 92664,   329,    11, 23245,  4802, 10333,    11,  8529, 43089,\n",
            "            11, 35281,   386, 16977,   271,    34,  2889,  1073, 18812,  9757,\n",
            "          3549,   555,    25, 11355,  2405,   258,  3520,   271, 12878,  5814,\n",
            "            25,  3441,   544, 36636,  1626,   783,   271,  2520, 45638, 67742,\n",
            "           271,   200, 91336, 84127,   720,   791, 12283,  1053,  1093,   311,\n",
            "          9901,   279,  2768,  1274,   369,   872,   720,  6497,   287,   220,\n",
            "          1862,   220,   323,   220, 13291,    25,   220, 25972,   220, 93070,\n",
            "            11,   220,  9052,   720,  6600,  1892,    72,    11,  6706,   661,\n",
            "         88088,    11, 26349,  2563,  1108,    11,  3842,  8753,    11, 10016,\n",
            "         25225,  1637,    11,   720, 50988,   220,  8847,  1293,    11,   220,\n",
            "         25028,   220, 91990, 60382,    11,   220, 13678,   220, 21293,    11,\n",
            "           720, 20830, 35407,    11, 16768,  5929,    11, 12471,  7379, 11166,\n",
            "            11, 43582,  1901,   344, 47547,    11, 11824,   720,    42, 47469,\n",
            "            11, 10016,  6941,  1556, 25947, 29384,    11, 17520,  6377,   880,\n",
            "          1815,    11, 17520,   720, 24903, 24483,    11, 43130, 18943,   805,\n",
            "            11,  8529, 38206,    11, 13929,   445, 16357,   315,   279,   720,\n",
            "            39,  2989, 12228,    11,   323,   315,  3388, 39485, 32284,   268,\n",
            "            11,  2085,  8884,  7000,   720,  1073,   420,  1053,   617,  7077,\n",
            "             0,  4815,    35, 34383,   811,   198],\n",
            "        [ 2895,  6374,    11, 11517,   473,   952,    11,  6621, 38672,    66,\n",
            "         15130,  4341,    11,  7043, 30474,   875,    11, 10016, 34221,    11,\n",
            "          2355, 66091, 24100,   441,    11, 33412,  2009,  4458,    11,  8388,\n",
            "         33794,   546,    11, 23519,  5455,   426,  2639,   309,    11, 92664,\n",
            "           329,    11, 23245,  4802, 10333,    11,  8529, 43089,    11, 35281,\n",
            "           386, 16977,   271,    34,  2889,  1073, 18812,  9757,  3549,   555,\n",
            "            25, 11355,  2405,   258,  3520,   271, 12878,  5814,    25,  3441,\n",
            "           544, 36636,  1626,   783,   271,  2520, 45638, 67742,   271,   200,\n",
            "         91336, 84127,   720,   791, 12283,  1053,  1093,   311,  9901,   279,\n",
            "          2768,  1274,   369,   872,   720,  6497,   287,   220,  1862,   220,\n",
            "           323,   220, 13291,    25,   220, 25972,   220, 93070,    11,   220,\n",
            "          9052,   720,  6600,  1892,    72,    11,  6706,   661, 88088,    11,\n",
            "         26349,  2563,  1108,    11,  3842,  8753,    11, 10016, 25225,  1637,\n",
            "            11,   720, 50988,   220,  8847,  1293,    11,   220, 25028,   220,\n",
            "         91990, 60382,    11,   220, 13678,   220, 21293,    11,   720, 20830,\n",
            "         35407,    11, 16768,  5929,    11, 12471,  7379, 11166,    11, 43582,\n",
            "          1901,   344, 47547,    11, 11824,   720,    42, 47469,    11, 10016,\n",
            "          6941,  1556, 25947, 29384,    11, 17520,  6377,   880,  1815,    11,\n",
            "         17520,   720, 24903, 24483,    11, 43130, 18943,   805,    11,  8529,\n",
            "         38206,    11, 13929,   445, 16357,   315,   279,   720,    39,  2989,\n",
            "         12228,    11,   323,   315,  3388, 39485, 32284,   268,    11,  2085,\n",
            "          8884,  7000,   720,  1073,   420,  1053,   617,  7077,     0,  4815,\n",
            "            35, 34383,   811,   198,  1271,   856,  7126,    11,   889, 11784,\n",
            "           757,   311, 10919,  7868,   323,   311,  8198,   720, 58162,   304,\n",
            "          4689,    13,  5659,   832,   315,   813,  6603,   358,  1373,   856,\n",
            "          1176, 10919,  7058,  7868,  1122,  3446]]), 'length': tensor([256, 256]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   11, 33412,  2009,  4458,    11,  2355, 54567, 56728,    11, 11519,\n",
            "         31421,    11, 40796, 19197,  1601,    11, 23245, 34297,    11,   735,\n",
            "         96893, 54090,    89,   271, 86225, 39154,   811,    25,   220, 32532,\n",
            "         99666,    11,  2895,  6374,    11, 11517,   473,   952,    11,  6621,\n",
            "         38672,    66, 15130,  4341,    11,  7043, 30474,   875,    11, 10016,\n",
            "         34221,    11,  2355, 66091, 24100,   441,    11, 33412,  2009,  4458,\n",
            "            11,  8388, 33794,   546,    11, 23519,  5455,   426,  2639,   309,\n",
            "            11, 92664,   329,    11, 23245,  4802, 10333,    11,  8529, 43089,\n",
            "            11, 35281,   386, 16977,   271,    34,  2889,  1073, 18812,  9757,\n",
            "          3549,   555,    25, 11355,  2405,   258,  3520,   271, 12878,  5814,\n",
            "            25,  3441,   544, 36636,  1626,   783,   271,  2520, 45638, 67742,\n",
            "           271,   200, 91336, 84127,   720,   791, 12283,  1053,  1093,   311,\n",
            "          9901,   279,  2768,  1274,   369,   872,   720,  6497,   287,   220,\n",
            "          1862,   220,   323,   220, 13291,    25,   220, 25972,   220, 93070,\n",
            "            11,   220,  9052,   720,  6600,  1892,    72,    11,  6706,   661,\n",
            "         88088,    11, 26349,  2563,  1108,    11,  3842,  8753,    11, 10016,\n",
            "         25225,  1637,    11,   720, 50988,   220,  8847,  1293,    11,   220,\n",
            "         25028,   220, 91990, 60382,    11,   220, 13678,   220, 21293,    11,\n",
            "           720, 20830, 35407,    11, 16768,  5929,    11, 12471,  7379, 11166,\n",
            "            11, 43582,  1901,   344, 47547,    11, 11824,   720,    42, 47469,\n",
            "            11, 10016,  6941,  1556, 25947, 29384,    11, 17520,  6377,   880,\n",
            "          1815,    11, 17520,   720, 24903, 24483,    11, 43130, 18943,   805,\n",
            "            11,  8529, 38206,    11, 13929,   445, 16357,   315,   279,   720,\n",
            "            39,  2989, 12228,    11,   323,   315,  3388, 39485, 32284,   268,\n",
            "            11,  2085,  8884,  7000,   720,  1073,   420,  1053,   617,  7077,\n",
            "             0,  4815,    35, 34383,   811,   198],\n",
            "        [ 2895,  6374,    11, 11517,   473,   952,    11,  6621, 38672,    66,\n",
            "         15130,  4341,    11,  7043, 30474,   875,    11, 10016, 34221,    11,\n",
            "          2355, 66091, 24100,   441,    11, 33412,  2009,  4458,    11,  8388,\n",
            "         33794,   546,    11, 23519,  5455,   426,  2639,   309,    11, 92664,\n",
            "           329,    11, 23245,  4802, 10333,    11,  8529, 43089,    11, 35281,\n",
            "           386, 16977,   271,    34,  2889,  1073, 18812,  9757,  3549,   555,\n",
            "            25, 11355,  2405,   258,  3520,   271, 12878,  5814,    25,  3441,\n",
            "           544, 36636,  1626,   783,   271,  2520, 45638, 67742,   271,   200,\n",
            "         91336, 84127,   720,   791, 12283,  1053,  1093,   311,  9901,   279,\n",
            "          2768,  1274,   369,   872,   720,  6497,   287,   220,  1862,   220,\n",
            "           323,   220, 13291,    25,   220, 25972,   220, 93070,    11,   220,\n",
            "          9052,   720,  6600,  1892,    72,    11,  6706,   661, 88088,    11,\n",
            "         26349,  2563,  1108,    11,  3842,  8753,    11, 10016, 25225,  1637,\n",
            "            11,   720, 50988,   220,  8847,  1293,    11,   220, 25028,   220,\n",
            "         91990, 60382,    11,   220, 13678,   220, 21293,    11,   720, 20830,\n",
            "         35407,    11, 16768,  5929,    11, 12471,  7379, 11166,    11, 43582,\n",
            "          1901,   344, 47547,    11, 11824,   720,    42, 47469,    11, 10016,\n",
            "          6941,  1556, 25947, 29384,    11, 17520,  6377,   880,  1815,    11,\n",
            "         17520,   720, 24903, 24483,    11, 43130, 18943,   805,    11,  8529,\n",
            "         38206,    11, 13929,   445, 16357,   315,   279,   720,    39,  2989,\n",
            "         12228,    11,   323,   315,  3388, 39485, 32284,   268,    11,  2085,\n",
            "          8884,  7000,   720,  1073,   420,  1053,   617,  7077,     0,  4815,\n",
            "            35, 34383,   811,   198,  1271,   856,  7126,    11,   889, 11784,\n",
            "           757,   311, 10919,  7868,   323,   311,  8198,   720, 58162,   304,\n",
            "          4689,    13,  5659,   832,   315,   813,  6603,   358,  1373,   856,\n",
            "          1176, 10919,  7058,  7868,  1122,  3446]])}\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Execute continued pretraining\n",
        "\n",
        "The process for training the model is very similar to `2_lora_qa_finetuning.ipynb`. The only relevant difference is that the learning rate for the adapters in the `lm_head` and `embed_tokens` layers is lower than for other layers (details below).\n",
        "\n",
        "In details, I did the following:\n",
        "\n",
        "1. Set fine-tuning parameters:\n",
        "  -  **Learning rate** [1, 2, 3]: For all hidden layers, I used 1e-4. When including also `lm_head` and `embed_tokens`, their learning rate is set to 5e-5 as suggested in [4]. To enable different learning rates, I initalized a dedicated AdamW optimizer instance, as suggested in [5]. \\\\\n",
        "    I used a simple Linear Scheduler [6], which warms up the learning rate until `warmup_ratio` steps, then linearly decreases it.\n",
        "  - **Epochs** [7]: I set this to 1, i.e. each chunk of the text is seem once by the trainer.\n",
        "  - **Batch size** [8]: I set the batch size as big as possible based on VRAM availability, i.e. to 64.\n",
        "\n",
        "2. Randomly shuffle the dataset\n",
        "\n",
        "3. Setup logic for storing training loss and learning rate schedule into local log file. \\\\\n",
        "Instructs Trainer to store the learning rate and all the other logs, at each log steps. Log steps are defined via the `log_steps` argument of the Trainer.This is implemented via the `SaveLrCallback` callback, that gets executed by the Trainer after every log step. The log file is stored under the \"profiling\" path.\n",
        "\n",
        "4. Execute fine-tuning.\n",
        "\n",
        "5. Save the following:\n",
        "  - to Hugging Face Hub => best model and last checkpoint model. I use my personal HF namespace `mpenna77`\n",
        "  - to Github => PyTorch memory profiling files [12] and training loss/learning rate schedule\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Learning rate for Llora:\n",
        "- [1] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#learning-rate\n",
        "- [2] https://www.determined.ai/blog/lora-parameters\n",
        "- [3] https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2?_gl=1*1xhtjl9*_gcl_au*ODk3ODg5MjE3LjE3NDc5MDcwODY.#sensitivity-of-lora-to-learning-rate\n",
        "- [4] https://unsloth.ai/blog/contpretraining\n",
        "- [5] https://www.kaggle.com/code/rhtsingh/guide-to-huggingface-schedulers-differential-lrs\n",
        "- [6] https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup\n",
        "\n",
        "Epochs for Llora:\n",
        "- [7] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#epochs\n",
        "\n",
        "Batch size for Llora:\n",
        "- [8] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#effective-batch-size\n",
        "\n",
        "Use PEFT and Trainer in Hugging Face:\n",
        "- [9] https://huggingface.co/docs/peft/quicktour\n",
        "- [10] https://huggingface.co/docs/transformers/v4.52.2/en/trainer\n",
        "- [11] https://huggingface.co/learn/llm-course/chapter3/2?fw=pt\n",
        "\n",
        "Memory profiling in PyTorch:\n",
        "- [12] https://zdevito.github.io/2022/12/09/memory-traces.html\n",
        "\n",
        "Storing\n",
        "- https://discuss.huggingface.co/t/how-to-write-custom-trainercallback-functions-with-custom-arguments/151063"
      ],
      "metadata": {
        "id": "UdDMKV15VZao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# set hyperparameters\n",
        "LR = 1e-4\n",
        "LOW_LR = 5e-5\n",
        "BATCH_SIZE = 32 # 16\n",
        "NUM_SAVES = 5   # how many checkpoints to save\n",
        "\n",
        "# setting run name\n",
        "if INCLUDE_HEAD_AND_EMB:\n",
        "  run_name = f\"{model_name.split('/')[-1]}-lr{LR}-b{BATCH_SIZE}-r{LORA_R}-a{LORA_ALPHA}-continuous-lora-cthulhu\"\n",
        "else:\n",
        "  run_name = f\"{model_name.split('/')[-1]}-lr{LR}-b{BATCH_SIZE}-r{LORA_R}-a{LORA_ALPHA}-continuous-no_head-lora-cthulhu\"\n",
        "print(run_name)\n",
        "\n",
        "# creating output directories\n",
        "%cd /content/cthulhu_fine_tuning\n",
        "\n",
        "out_dir = f\"fine-tuning/models/{run_name}\"           # temp local dir where checkpoints are saved\n",
        "profiling_dir = f\"fine-tuning/profiling/{run_name}\"  # local dir where logs and mem profile are stored. will be pushed to github\n",
        "log_file=f\"{profiling_dir}/logs.txt\"                 # log file storing for each log step various metrics\n",
        "Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
        "Path(profiling_dir).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "# Create HF repo for this model\n",
        "api = HfApi(token=userdata.get('HF_TOKEN_WRITE'))\n",
        "api.create_repo(repo_id=f\"mpenna77/{run_name}\", private=True, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "bEC-WGgZUWBo",
        "outputId": "a4cea520-f75e-40d9-d858-28253d858d3d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu\n",
            "/content/cthulhu_fine_tuning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RepoUrl('https://huggingface.co/mpenna77/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu', endpoint='https://huggingface.co', repo_type='model', repo_id='mpenna77/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting learning rates separately for different layers.\n",
        "\n",
        "from torch.optim.adamw import AdamW\n",
        "\n",
        "params_low_lr = [\n",
        "    \"base_model.model.model.embed_tokens.lora_embedding_A.default\",\n",
        "    \"base_model.model.model.embed_tokens.lora_embedding_B.default\",\n",
        "    \"base_model.model.lm_head.lora_A.default.weight\",\n",
        "    \"base_model.model.lm_head.lora_B.default.weight\"\n",
        "    ]\n",
        "\n",
        "model_params = model.named_parameters()\n",
        "grouped_parameters = [\n",
        "    {'params': [p for n, p in model_params if not any(nd in n for nd in params_low_lr)], 'lr': LR},\n",
        "    {'params': [p for n, p in model_params if any(nd in n for nd in params_low_lr)], 'lr': LOW_LR}\n",
        "]\n",
        "\n",
        "adam_optimizer = AdamW(grouped_parameters) # this optimizer will be used during training (see later)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4Wt_6P67ezc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle dataset\n",
        "generator = np.random.default_rng(seed=42)\n",
        "dataset_chunked = dataset_chunked.shuffle(generator=generator)"
      ],
      "metadata": {
        "id": "PNfhjomreyqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insruct Trainer to log learning rates in local log file at every log event.\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class SaveLrCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A TrainerCallback that logs learning rates to a local file at every log event.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_file):\n",
        "        \"\"\"\n",
        "        Initializes the SaveLrCallback.\n",
        "\n",
        "        Args:\n",
        "            output_file (str): The path to the file where logs will be written.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.output_file = output_file\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "      \"\"\"\n",
        "      Writes the logs to the output file at every log event.\n",
        "\n",
        "      Args:\n",
        "          args (TrainingArguments): The training arguments.\n",
        "          state (TrainerState): The current state of the trainer.\n",
        "          control (TrainerControl): The control object for the training loop.\n",
        "          logs (Dict[str, float], optional): The logs to write. Defaults to None.\n",
        "          **kwargs: Additional keyword arguments.\n",
        "\n",
        "      Returns:\n",
        "          TrainerControl: The control object for the training loop.\n",
        "      \"\"\"\n",
        "      lr_scheduler = kwargs.pop(\"lr_scheduler\")\n",
        "      optimizer = kwargs.pop(\"optimizer\")\n",
        "      if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        for k, group in enumerate(optimizer.param_groups()):\n",
        "          logs[f\"lr_param_group_{k}\"] = group[\"lr\"]\n",
        "      else:\n",
        "        for k, v in enumerate(lr_scheduler.get_last_lr()):\n",
        "          logs[f\"lr_param_group_{k}\"] = v\n",
        "      print(logs)\n",
        "      with open(self.output_file, \"a\") as f:\n",
        "          f.write(str(logs) + \"\\n\")\n",
        "      return super().on_log(args, state, control, **kwargs)"
      ],
      "metadata": {
        "id": "AsioaIR0TFVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define every how many steps we want to save checkpoints\n",
        "\n",
        "num_examples = len(dataset_chunked)\n",
        "num_steps = num_examples // BATCH_SIZE\n",
        "save_steps = num_steps // NUM_SAVES\n",
        "\n",
        "print(f\"Number of examples: {num_examples}\")\n",
        "print(f\"Number of steps: {num_steps}\")\n",
        "print(f\"save_steps: {save_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bTCgxG9xFCc",
        "outputId": "38f96706-3a81-47a3-c9f1-78c0ce505383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples: 14557\n",
            "Number of steps: 454\n",
            "save_steps: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute fine tuning\n",
        "\n",
        "import time\n",
        "from typing import Dict, Optional\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback, DataCollatorWithPadding\n",
        "from transformers.integrations import flash_attention\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    gradient_accumulation_steps = 1,         # after how many steps (batches) to sum up the gradient and then update params\n",
        "    lr_scheduler_type = \"linear\",            # learning rate is warmed up linearly until warmup_ratio steps, then linearly decreases\n",
        "    warmup_ratio = 0.05,                     # firs x% of training steps will be used to slowly ramp-up learning rate (default is 0.1). Makes training more stable.\n",
        "    weight_decay=0.3,                        # keep between 1.0 and 0.3\n",
        "    per_device_train_batch_size=BATCH_SIZE,  # depends on VRAM availability\n",
        "    num_train_epochs=1,                      # 1 should be sufficient. More could overfit and decrease creativity.\n",
        "    save_strategy=\"steps\",                   # set to \"epoch\" if storing checkpoints every epoch, or \"step\" if at every batch\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=5,\n",
        "    report_to=\"none\",                        # Disable 'Weights and Biases' reporting\n",
        "    label_names=[\"labels\"],                  # see: https://github.com/unslothai/unsloth/issues/1788\n",
        "    push_to_hub = True,\n",
        "    hub_model_id = run_name,\n",
        "    hub_strategy = \"all_checkpoints\",        # saves to the hub all checkpoints, including the last step\n",
        "    hub_token = HF_TOKEN,\n",
        "    #max_steps = 10,\n",
        ")\n",
        "\n",
        "\n",
        "time_start_training = time.perf_counter()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_chunked,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    optimizers=(adam_optimizer, None) # sets the learning rates defined in adapt_optmizer. Note that the scheduler (set here to None) will be set automatically using the TrainingArguments (see: https://github.com/huggingface/transformers/blob/0725cd6953803b8aacfc85288cbfb83dea30c469/src/transformers/trainer.py#L1741)\n",
        ")\n",
        "\n",
        "# add callback to log learning rates\n",
        "trainer.add_callback(SaveLrCallback(log_file))\n",
        "\n",
        "# start capturing VRAM snapshots\n",
        "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
        "\n",
        "results = trainer.train()\n",
        "\n",
        "# end capturing VRAM snapshots\n",
        "time_end_training = time.perf_counter()\n",
        "\n",
        "# push tokenizer files to the hub\n",
        "tokenizer.push_to_hub(run_name, private=True, token=HF_TOKEN)\n"
      ],
      "metadata": {
        "id": "beCySXDzVdxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e874ad4b-d59b-41aa-dd73-7e1bc1e1211b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-19-311516262.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/455 2:44:40, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.708900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.550100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.296300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.032500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.929900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.890400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.885300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.882700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.826700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.795200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.762100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.617500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.580700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.563300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.510600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.543900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.449900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.472300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.407800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.392300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.334600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.309000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.371900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.326600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>1.282700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.196600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.181400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.205500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.124800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.084600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>1.038800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.095400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>1.085700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.984000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>1.028400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.997100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.979400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.928400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.907400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.806200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.793100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.845200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.834500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.814900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.804800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.768600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.750100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.672300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.651300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.671700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.649100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.644100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.559700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.568700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.559800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.542400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.497100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.485500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.479600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.457700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.437200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.433800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.453700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.428800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.450200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>0.395900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.421900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.393600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.386400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.357200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.388100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.391200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>0.385600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 2.7089, 'grad_norm': 2.071607828140259, 'learning_rate': 1.739130434782609e-05, 'epoch': 0.01098901098901099, 'lr_param_group_0': 2.173913043478261e-05, 'lr_param_group_1': 1.0869565217391305e-05}\n",
            "{'loss': 2.5501, 'grad_norm': 1.4521268606185913, 'learning_rate': 3.91304347826087e-05, 'epoch': 0.02197802197802198, 'lr_param_group_0': 4.347826086956522e-05, 'lr_param_group_1': 2.173913043478261e-05}\n",
            "{'loss': 2.4125, 'grad_norm': 1.3611598014831543, 'learning_rate': 6.086956521739131e-05, 'epoch': 0.03296703296703297, 'lr_param_group_0': 6.521739130434783e-05, 'lr_param_group_1': 3.260869565217392e-05}\n",
            "{'loss': 2.2963, 'grad_norm': 1.3198211193084717, 'learning_rate': 8.260869565217392e-05, 'epoch': 0.04395604395604396, 'lr_param_group_0': 8.695652173913044e-05, 'lr_param_group_1': 4.347826086956522e-05}\n",
            "{'loss': 2.2084, 'grad_norm': 1.441917061805725, 'learning_rate': 9.976851851851852e-05, 'epoch': 0.054945054945054944, 'lr_param_group_0': 9.953703703703704e-05, 'lr_param_group_1': 4.976851851851852e-05}\n",
            "{'loss': 2.1275, 'grad_norm': 1.2170544862747192, 'learning_rate': 9.861111111111112e-05, 'epoch': 0.06593406593406594, 'lr_param_group_0': 9.837962962962963e-05, 'lr_param_group_1': 4.9189814814814815e-05}\n",
            "{'loss': 2.0818, 'grad_norm': 1.201651692390442, 'learning_rate': 9.745370370370371e-05, 'epoch': 0.07692307692307693, 'lr_param_group_0': 9.722222222222223e-05, 'lr_param_group_1': 4.8611111111111115e-05}\n",
            "{'loss': 2.0325, 'grad_norm': 1.161786437034607, 'learning_rate': 9.62962962962963e-05, 'epoch': 0.08791208791208792, 'lr_param_group_0': 9.606481481481482e-05, 'lr_param_group_1': 4.803240740740741e-05}\n",
            "{'loss': 1.9299, 'grad_norm': 1.2177150249481201, 'learning_rate': 9.513888888888888e-05, 'epoch': 0.0989010989010989, 'lr_param_group_0': 9.490740740740742e-05, 'lr_param_group_1': 4.745370370370371e-05}\n",
            "{'loss': 1.8904, 'grad_norm': 1.182674765586853, 'learning_rate': 9.398148148148148e-05, 'epoch': 0.10989010989010989, 'lr_param_group_0': 9.375e-05, 'lr_param_group_1': 4.6875e-05}\n",
            "{'loss': 1.8853, 'grad_norm': 1.078943133354187, 'learning_rate': 9.282407407407407e-05, 'epoch': 0.12087912087912088, 'lr_param_group_0': 9.25925925925926e-05, 'lr_param_group_1': 4.62962962962963e-05}\n",
            "{'loss': 1.8827, 'grad_norm': 1.2566813230514526, 'learning_rate': 9.166666666666667e-05, 'epoch': 0.13186813186813187, 'lr_param_group_0': 9.143518518518519e-05, 'lr_param_group_1': 4.5717592592592594e-05}\n",
            "{'loss': 1.8267, 'grad_norm': 1.191741943359375, 'learning_rate': 9.050925925925925e-05, 'epoch': 0.14285714285714285, 'lr_param_group_0': 9.027777777777779e-05, 'lr_param_group_1': 4.5138888888888894e-05}\n",
            "{'loss': 1.7952, 'grad_norm': 1.1647205352783203, 'learning_rate': 8.935185185185185e-05, 'epoch': 0.15384615384615385, 'lr_param_group_0': 8.912037037037037e-05, 'lr_param_group_1': 4.456018518518519e-05}\n",
            "{'loss': 1.7621, 'grad_norm': 1.3271949291229248, 'learning_rate': 8.819444444444445e-05, 'epoch': 0.16483516483516483, 'lr_param_group_0': 8.796296296296297e-05, 'lr_param_group_1': 4.3981481481481486e-05}\n",
            "{'loss': 1.7526, 'grad_norm': 1.320763111114502, 'learning_rate': 8.703703703703704e-05, 'epoch': 0.17582417582417584, 'lr_param_group_0': 8.680555555555556e-05, 'lr_param_group_1': 4.340277777777778e-05}\n",
            "{'loss': 1.62, 'grad_norm': 1.373268723487854, 'learning_rate': 8.587962962962964e-05, 'epoch': 0.18681318681318682, 'lr_param_group_0': 8.564814814814816e-05, 'lr_param_group_1': 4.282407407407408e-05}\n",
            "{'loss': 1.6175, 'grad_norm': 1.23393976688385, 'learning_rate': 8.472222222222222e-05, 'epoch': 0.1978021978021978, 'lr_param_group_0': 8.449074074074074e-05, 'lr_param_group_1': 4.224537037037037e-05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 1.656, 'grad_norm': 1.4674347639083862, 'learning_rate': 8.356481481481482e-05, 'epoch': 0.2087912087912088, 'lr_param_group_0': 8.333333333333334e-05, 'lr_param_group_1': 4.166666666666667e-05}\n",
            "{'loss': 1.6101, 'grad_norm': 1.329586386680603, 'learning_rate': 8.240740740740741e-05, 'epoch': 0.21978021978021978, 'lr_param_group_0': 8.217592592592593e-05, 'lr_param_group_1': 4.1087962962962965e-05}\n",
            "{'loss': 1.5807, 'grad_norm': 1.3742815256118774, 'learning_rate': 8.125000000000001e-05, 'epoch': 0.23076923076923078, 'lr_param_group_0': 8.101851851851853e-05, 'lr_param_group_1': 4.0509259259259265e-05}\n",
            "{'loss': 1.5633, 'grad_norm': 1.5650655031204224, 'learning_rate': 8.00925925925926e-05, 'epoch': 0.24175824175824176, 'lr_param_group_0': 7.986111111111112e-05, 'lr_param_group_1': 3.993055555555556e-05}\n",
            "{'loss': 1.5106, 'grad_norm': 1.3469758033752441, 'learning_rate': 7.89351851851852e-05, 'epoch': 0.25274725274725274, 'lr_param_group_0': 7.870370370370372e-05, 'lr_param_group_1': 3.935185185185186e-05}\n",
            "{'loss': 1.5439, 'grad_norm': 1.373941421508789, 'learning_rate': 7.777777777777778e-05, 'epoch': 0.26373626373626374, 'lr_param_group_0': 7.75462962962963e-05, 'lr_param_group_1': 3.877314814814815e-05}\n",
            "{'loss': 1.4499, 'grad_norm': 1.4737673997879028, 'learning_rate': 7.662037037037038e-05, 'epoch': 0.27472527472527475, 'lr_param_group_0': 7.638888888888889e-05, 'lr_param_group_1': 3.8194444444444444e-05}\n",
            "{'loss': 1.4723, 'grad_norm': 1.3908895254135132, 'learning_rate': 7.546296296296297e-05, 'epoch': 0.2857142857142857, 'lr_param_group_0': 7.523148148148149e-05, 'lr_param_group_1': 3.7615740740740744e-05}\n",
            "{'loss': 1.4078, 'grad_norm': 1.5286415815353394, 'learning_rate': 7.430555555555557e-05, 'epoch': 0.2967032967032967, 'lr_param_group_0': 7.407407407407407e-05, 'lr_param_group_1': 3.7037037037037037e-05}\n",
            "{'loss': 1.3923, 'grad_norm': 1.5102784633636475, 'learning_rate': 7.314814814814815e-05, 'epoch': 0.3076923076923077, 'lr_param_group_0': 7.291666666666667e-05, 'lr_param_group_1': 3.6458333333333336e-05}\n",
            "{'loss': 1.3346, 'grad_norm': 1.501734733581543, 'learning_rate': 7.199074074074075e-05, 'epoch': 0.31868131868131866, 'lr_param_group_0': 7.175925925925926e-05, 'lr_param_group_1': 3.587962962962963e-05}\n",
            "{'loss': 1.309, 'grad_norm': 1.5563037395477295, 'learning_rate': 7.083333333333334e-05, 'epoch': 0.32967032967032966, 'lr_param_group_0': 7.060185185185186e-05, 'lr_param_group_1': 3.530092592592593e-05}\n",
            "{'loss': 1.3719, 'grad_norm': 1.8463562726974487, 'learning_rate': 6.967592592592594e-05, 'epoch': 0.34065934065934067, 'lr_param_group_0': 6.944444444444444e-05, 'lr_param_group_1': 3.472222222222222e-05}\n",
            "{'loss': 1.3266, 'grad_norm': 1.69161856174469, 'learning_rate': 6.851851851851852e-05, 'epoch': 0.3516483516483517, 'lr_param_group_0': 6.828703703703704e-05, 'lr_param_group_1': 3.414351851851852e-05}\n",
            "{'loss': 1.2827, 'grad_norm': 1.5964750051498413, 'learning_rate': 6.736111111111112e-05, 'epoch': 0.3626373626373626, 'lr_param_group_0': 6.712962962962963e-05, 'lr_param_group_1': 3.3564814814814815e-05}\n",
            "{'loss': 1.1966, 'grad_norm': 1.8138540983200073, 'learning_rate': 6.620370370370371e-05, 'epoch': 0.37362637362637363, 'lr_param_group_0': 6.597222222222223e-05, 'lr_param_group_1': 3.2986111111111115e-05}\n",
            "{'loss': 1.1814, 'grad_norm': 1.6339483261108398, 'learning_rate': 6.50462962962963e-05, 'epoch': 0.38461538461538464, 'lr_param_group_0': 6.481481481481482e-05, 'lr_param_group_1': 3.240740740740741e-05}\n",
            "{'loss': 1.2055, 'grad_norm': 1.7250884771347046, 'learning_rate': 6.388888888888888e-05, 'epoch': 0.3956043956043956, 'lr_param_group_0': 6.365740740740742e-05, 'lr_param_group_1': 3.182870370370371e-05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 1.1248, 'grad_norm': 1.6980574131011963, 'learning_rate': 6.273148148148148e-05, 'epoch': 0.4065934065934066, 'lr_param_group_0': 6.25e-05, 'lr_param_group_1': 3.125e-05}\n",
            "{'loss': 1.0846, 'grad_norm': 1.8243016004562378, 'learning_rate': 6.157407407407407e-05, 'epoch': 0.4175824175824176, 'lr_param_group_0': 6.13425925925926e-05, 'lr_param_group_1': 3.06712962962963e-05}\n",
            "{'loss': 1.0388, 'grad_norm': 1.759268045425415, 'learning_rate': 6.041666666666667e-05, 'epoch': 0.42857142857142855, 'lr_param_group_0': 6.018518518518519e-05, 'lr_param_group_1': 3.0092592592592593e-05}\n",
            "{'loss': 1.0954, 'grad_norm': 1.8132466077804565, 'learning_rate': 5.925925925925926e-05, 'epoch': 0.43956043956043955, 'lr_param_group_0': 5.902777777777778e-05, 'lr_param_group_1': 2.951388888888889e-05}\n",
            "{'loss': 1.0857, 'grad_norm': 1.9019651412963867, 'learning_rate': 5.810185185185185e-05, 'epoch': 0.45054945054945056, 'lr_param_group_0': 5.787037037037037e-05, 'lr_param_group_1': 2.8935185185185186e-05}\n",
            "{'loss': 0.984, 'grad_norm': 1.8968383073806763, 'learning_rate': 5.6944444444444445e-05, 'epoch': 0.46153846153846156, 'lr_param_group_0': 5.6712962962962965e-05, 'lr_param_group_1': 2.8356481481481483e-05}\n",
            "{'loss': 1.0284, 'grad_norm': 1.8323601484298706, 'learning_rate': 5.578703703703704e-05, 'epoch': 0.4725274725274725, 'lr_param_group_0': 5.555555555555556e-05, 'lr_param_group_1': 2.777777777777778e-05}\n",
            "{'loss': 0.9971, 'grad_norm': 2.007112503051758, 'learning_rate': 5.462962962962963e-05, 'epoch': 0.4835164835164835, 'lr_param_group_0': 5.439814814814815e-05, 'lr_param_group_1': 2.7199074074074076e-05}\n",
            "{'loss': 0.9794, 'grad_norm': 1.8704272508621216, 'learning_rate': 5.3472222222222224e-05, 'epoch': 0.4945054945054945, 'lr_param_group_0': 5.3240740740740744e-05, 'lr_param_group_1': 2.6620370370370372e-05}\n",
            "{'loss': 0.9284, 'grad_norm': 1.9471749067306519, 'learning_rate': 5.231481481481482e-05, 'epoch': 0.5054945054945055, 'lr_param_group_0': 5.208333333333334e-05, 'lr_param_group_1': 2.604166666666667e-05}\n",
            "{'loss': 0.9074, 'grad_norm': 1.9228503704071045, 'learning_rate': 5.115740740740741e-05, 'epoch': 0.5164835164835165, 'lr_param_group_0': 5.092592592592593e-05, 'lr_param_group_1': 2.5462962962962965e-05}\n",
            "{'loss': 0.8062, 'grad_norm': 1.9799954891204834, 'learning_rate': 5e-05, 'epoch': 0.5274725274725275, 'lr_param_group_0': 4.976851851851852e-05, 'lr_param_group_1': 2.488425925925926e-05}\n",
            "{'loss': 0.7931, 'grad_norm': 1.939818024635315, 'learning_rate': 4.8842592592592595e-05, 'epoch': 0.5384615384615384, 'lr_param_group_0': 4.8611111111111115e-05, 'lr_param_group_1': 2.4305555555555558e-05}\n",
            "{'loss': 0.8452, 'grad_norm': 1.9071022272109985, 'learning_rate': 4.768518518518519e-05, 'epoch': 0.5494505494505495, 'lr_param_group_0': 4.745370370370371e-05, 'lr_param_group_1': 2.3726851851851854e-05}\n",
            "{'loss': 0.8345, 'grad_norm': 1.9442048072814941, 'learning_rate': 4.652777777777778e-05, 'epoch': 0.5604395604395604, 'lr_param_group_0': 4.62962962962963e-05, 'lr_param_group_1': 2.314814814814815e-05}\n",
            "{'loss': 0.8149, 'grad_norm': 1.9342652559280396, 'learning_rate': 4.5370370370370374e-05, 'epoch': 0.5714285714285714, 'lr_param_group_0': 4.5138888888888894e-05, 'lr_param_group_1': 2.2569444444444447e-05}\n",
            "{'loss': 0.8048, 'grad_norm': 2.2791597843170166, 'learning_rate': 4.4212962962962966e-05, 'epoch': 0.5824175824175825, 'lr_param_group_0': 4.3981481481481486e-05, 'lr_param_group_1': 2.1990740740740743e-05}\n",
            "{'loss': 0.7686, 'grad_norm': 1.903957724571228, 'learning_rate': 4.305555555555556e-05, 'epoch': 0.5934065934065934, 'lr_param_group_0': 4.282407407407408e-05, 'lr_param_group_1': 2.141203703703704e-05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.7501, 'grad_norm': 1.9011123180389404, 'learning_rate': 4.1898148148148145e-05, 'epoch': 0.6043956043956044, 'lr_param_group_0': 4.166666666666667e-05, 'lr_param_group_1': 2.0833333333333336e-05}\n",
            "{'loss': 0.6477, 'grad_norm': 1.9786354303359985, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.6153846153846154, 'lr_param_group_0': 4.0509259259259265e-05, 'lr_param_group_1': 2.0254629629629632e-05}\n",
            "{'loss': 0.6723, 'grad_norm': 1.9542311429977417, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.6263736263736264, 'lr_param_group_0': 3.935185185185186e-05, 'lr_param_group_1': 1.967592592592593e-05}\n",
            "{'loss': 0.6513, 'grad_norm': 2.2112927436828613, 'learning_rate': 3.8425925925925924e-05, 'epoch': 0.6373626373626373, 'lr_param_group_0': 3.8194444444444444e-05, 'lr_param_group_1': 1.9097222222222222e-05}\n",
            "{'loss': 0.6717, 'grad_norm': 1.9635757207870483, 'learning_rate': 3.726851851851852e-05, 'epoch': 0.6483516483516484, 'lr_param_group_0': 3.7037037037037037e-05, 'lr_param_group_1': 1.8518518518518518e-05}\n",
            "{'loss': 0.6636, 'grad_norm': 1.869895100593567, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.6593406593406593, 'lr_param_group_0': 3.587962962962963e-05, 'lr_param_group_1': 1.7939814814814815e-05}\n",
            "{'loss': 0.6491, 'grad_norm': 2.045090436935425, 'learning_rate': 3.49537037037037e-05, 'epoch': 0.6703296703296703, 'lr_param_group_0': 3.472222222222222e-05, 'lr_param_group_1': 1.736111111111111e-05}\n",
            "{'loss': 0.6441, 'grad_norm': 2.093724489212036, 'learning_rate': 3.3796296296296295e-05, 'epoch': 0.6813186813186813, 'lr_param_group_0': 3.3564814814814815e-05, 'lr_param_group_1': 1.6782407407407408e-05}\n",
            "{'loss': 0.5626, 'grad_norm': 1.909276008605957, 'learning_rate': 3.263888888888889e-05, 'epoch': 0.6923076923076923, 'lr_param_group_0': 3.240740740740741e-05, 'lr_param_group_1': 1.6203703703703704e-05}\n",
            "{'loss': 0.5597, 'grad_norm': 1.8168187141418457, 'learning_rate': 3.148148148148148e-05, 'epoch': 0.7032967032967034, 'lr_param_group_0': 3.125e-05, 'lr_param_group_1': 1.5625e-05}\n",
            "{'loss': 0.5687, 'grad_norm': 2.0822160243988037, 'learning_rate': 3.0324074074074077e-05, 'epoch': 0.7142857142857143, 'lr_param_group_0': 3.0092592592592593e-05, 'lr_param_group_1': 1.5046296296296297e-05}\n",
            "{'loss': 0.5598, 'grad_norm': 2.0639281272888184, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.7252747252747253, 'lr_param_group_0': 2.8935185185185186e-05, 'lr_param_group_1': 1.4467592592592593e-05}\n",
            "{'loss': 0.5479, 'grad_norm': 2.1143510341644287, 'learning_rate': 2.8009259259259263e-05, 'epoch': 0.7362637362637363, 'lr_param_group_0': 2.777777777777778e-05, 'lr_param_group_1': 1.388888888888889e-05}\n",
            "{'loss': 0.5424, 'grad_norm': 2.18941330909729, 'learning_rate': 2.6851851851851855e-05, 'epoch': 0.7472527472527473, 'lr_param_group_0': 2.6620370370370372e-05, 'lr_param_group_1': 1.3310185185185186e-05}\n",
            "{'loss': 0.5052, 'grad_norm': 2.1160359382629395, 'learning_rate': 2.5694444444444445e-05, 'epoch': 0.7582417582417582, 'lr_param_group_0': 2.5462962962962965e-05, 'lr_param_group_1': 1.2731481481481482e-05}\n",
            "{'loss': 0.5352, 'grad_norm': 2.2498152256011963, 'learning_rate': 2.4537037037037038e-05, 'epoch': 0.7692307692307693, 'lr_param_group_0': 2.4305555555555558e-05, 'lr_param_group_1': 1.2152777777777779e-05}\n",
            "{'loss': 0.4971, 'grad_norm': 2.386279821395874, 'learning_rate': 2.337962962962963e-05, 'epoch': 0.7802197802197802, 'lr_param_group_0': 2.314814814814815e-05, 'lr_param_group_1': 1.1574074074074075e-05}\n",
            "{'loss': 0.4855, 'grad_norm': 2.1686148643493652, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.7912087912087912, 'lr_param_group_0': 2.1990740740740743e-05, 'lr_param_group_1': 1.0995370370370372e-05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.4796, 'grad_norm': 2.2216808795928955, 'learning_rate': 2.1064814814814816e-05, 'epoch': 0.8021978021978022, 'lr_param_group_0': 2.0833333333333336e-05, 'lr_param_group_1': 1.0416666666666668e-05}\n",
            "{'loss': 0.4577, 'grad_norm': 2.2457292079925537, 'learning_rate': 1.990740740740741e-05, 'epoch': 0.8131868131868132, 'lr_param_group_0': 1.967592592592593e-05, 'lr_param_group_1': 9.837962962962964e-06}\n",
            "{'loss': 0.448, 'grad_norm': 1.9606224298477173, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.8241758241758241, 'lr_param_group_0': 1.8518518518518518e-05, 'lr_param_group_1': 9.259259259259259e-06}\n",
            "{'loss': 0.4372, 'grad_norm': 1.9174219369888306, 'learning_rate': 1.7592592592592595e-05, 'epoch': 0.8351648351648352, 'lr_param_group_0': 1.736111111111111e-05, 'lr_param_group_1': 8.680555555555556e-06}\n",
            "{'loss': 0.4338, 'grad_norm': 2.0316741466522217, 'learning_rate': 1.6435185185185187e-05, 'epoch': 0.8461538461538461, 'lr_param_group_0': 1.6203703703703704e-05, 'lr_param_group_1': 8.101851851851852e-06}\n",
            "{'loss': 0.4537, 'grad_norm': 2.327354669570923, 'learning_rate': 1.527777777777778e-05, 'epoch': 0.8571428571428571, 'lr_param_group_0': 1.5046296296296297e-05, 'lr_param_group_1': 7.523148148148148e-06}\n",
            "{'loss': 0.4288, 'grad_norm': 2.258723497390747, 'learning_rate': 1.412037037037037e-05, 'epoch': 0.8681318681318682, 'lr_param_group_0': 1.388888888888889e-05, 'lr_param_group_1': 6.944444444444445e-06}\n",
            "{'loss': 0.4502, 'grad_norm': 1.9061285257339478, 'learning_rate': 1.2962962962962962e-05, 'epoch': 0.8791208791208791, 'lr_param_group_0': 1.2731481481481482e-05, 'lr_param_group_1': 6.365740740740741e-06}\n",
            "{'loss': 0.3959, 'grad_norm': 1.865344762802124, 'learning_rate': 1.1805555555555555e-05, 'epoch': 0.8901098901098901, 'lr_param_group_0': 1.1574074074074075e-05, 'lr_param_group_1': 5.787037037037038e-06}\n",
            "{'loss': 0.4125, 'grad_norm': 2.0411360263824463, 'learning_rate': 1.0648148148148148e-05, 'epoch': 0.9010989010989011, 'lr_param_group_0': 1.0416666666666668e-05, 'lr_param_group_1': 5.208333333333334e-06}\n",
            "{'loss': 0.4219, 'grad_norm': 2.106222629547119, 'learning_rate': 9.490740740740741e-06, 'epoch': 0.9120879120879121, 'lr_param_group_0': 9.259259259259259e-06, 'lr_param_group_1': 4.6296296296296296e-06}\n",
            "{'loss': 0.3936, 'grad_norm': 1.9310988187789917, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.9230769230769231, 'lr_param_group_0': 8.101851851851852e-06, 'lr_param_group_1': 4.050925925925926e-06}\n",
            "{'loss': 0.3864, 'grad_norm': 2.0117924213409424, 'learning_rate': 7.1759259259259266e-06, 'epoch': 0.9340659340659341, 'lr_param_group_0': 6.944444444444445e-06, 'lr_param_group_1': 3.4722222222222224e-06}\n",
            "{'loss': 0.3572, 'grad_norm': 1.9447946548461914, 'learning_rate': 6.0185185185185185e-06, 'epoch': 0.945054945054945, 'lr_param_group_0': 5.787037037037038e-06, 'lr_param_group_1': 2.893518518518519e-06}\n",
            "{'loss': 0.3881, 'grad_norm': 1.7943028211593628, 'learning_rate': 4.861111111111111e-06, 'epoch': 0.9560439560439561, 'lr_param_group_0': 4.6296296296296296e-06, 'lr_param_group_1': 2.3148148148148148e-06}\n",
            "{'loss': 0.3912, 'grad_norm': 1.962872862815857, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.967032967032967, 'lr_param_group_0': 3.4722222222222224e-06, 'lr_param_group_1': 1.7361111111111112e-06}\n",
            "{'loss': 0.3773, 'grad_norm': 1.9066197872161865, 'learning_rate': 2.546296296296296e-06, 'epoch': 0.978021978021978, 'lr_param_group_0': 2.3148148148148148e-06, 'lr_param_group_1': 1.1574074074074074e-06}\n",
            "{'loss': 0.3838, 'grad_norm': 1.8971236944198608, 'learning_rate': 1.388888888888889e-06, 'epoch': 0.989010989010989, 'lr_param_group_0': 1.1574074074074074e-06, 'lr_param_group_1': 5.787037037037037e-07}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3856, 'grad_norm': 2.0090320110321045, 'learning_rate': 2.3148148148148148e-07, 'epoch': 1.0, 'lr_param_group_0': 0.0, 'lr_param_group_1': 0.0}\n",
            "{'train_runtime': 9903.1511, 'train_samples_per_second': 1.47, 'train_steps_per_second': 0.046, 'total_flos': 1.7155806379533926e+17, 'train_loss': 1.0691619705367874, 'epoch': 1.0, 'lr_param_group_0': 0.0, 'lr_param_group_1': 0.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/mpenna77/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu/commit/5b517a2b81ed92db6f34b498f0e0c435e2774ab0', commit_message='Upload tokenizer', commit_description='', oid='5b517a2b81ed92db6f34b498f0e0c435e2774ab0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mpenna77/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu', endpoint='https://huggingface.co', repo_type='model', repo_id='mpenna77/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving VRAM snapshots to Github\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning\n",
        "\n",
        "profiling_file = f\"{profiling_dir}/profile.pkl\"\n",
        "profiling_viz_file = f\"{profiling_dir}/trace.html\"\n",
        "\n",
        "# saving memory profile file to local file system\n",
        "Path(profiling_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "torch.cuda.memory._dump_snapshot(profiling_file)\n",
        "torch.cuda.memory._record_memory_history(enabled=None)\n",
        "\n",
        "# generating memory profile visualuzation files in local file system\n",
        "! wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/cuda/_memory_viz.py\n",
        "! python _memory_viz.py trace_plot {profiling_file} -o {profiling_viz_file}\n",
        "\n",
        "\n",
        "# pushing profiling files to github\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add {profiling_dir}\n",
        "! git commit -m \"added new profiling files\"\n",
        "! git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmAF802mvNXB",
        "outputId": "2c7ca050-13c4-4d34-ede8-ee4252bb74b2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cthulhu_fine_tuning\n",
            "--2025-06-25 15:55:26--  https://raw.githubusercontent.com/pytorch/pytorch/master/torch/cuda/_memory_viz.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25848 (25K) [text/plain]\n",
            "Saving to: ‘_memory_viz.py’\n",
            "\n",
            "_memory_viz.py      100%[===================>]  25.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-25 15:55:26 (146 MB/s) - ‘_memory_viz.py’ saved [25848/25848]\n",
            "\n",
            "[main 4c00657] added new profiling files\n",
            " 3 files changed, 104 insertions(+)\n",
            " create mode 100644 fine-tuning/profiling/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu/logs.txt\n",
            " create mode 100644 fine-tuning/profiling/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu/profile.pkl\n",
            " create mode 100644 fine-tuning/profiling/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu/trace.html\n",
            "Enumerating objects: 11, done.\n",
            "Counting objects: 100% (11/11), done.\n",
            "Delta compression using up to 12 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (8/8), 4.71 MiB | 1.83 MiB/s, done.\n",
            "Total 8 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File fine-tuning/profiling/Llama-3.1-8B-Instruct-lr0.0001-b32-r64-a32-continuous-no_head-lora-cthulhu/trace.html is 56.74 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To github.com:ellolo/cthulhu_fine_tuning.git\n",
            "   b9effd4..4c00657  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporting and post-analysis"
      ],
      "metadata": {
        "id": "VOPI70l7xcLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print VRAM memory usage\n",
        "\n",
        "import torch\n",
        "\n",
        "mem_training = torch.cuda.max_memory_allocated(device)\n",
        "mem_used_training = round((mem_training - mem_lora_model_load) / 1024/1024/1024, 4)\n",
        "\n",
        "\n",
        "print(f\"training time: {(time_end_training - time_start_training):.1f} s\")\n",
        "print(f\"Memory used for original model (GB): {mem_used_model}\")\n",
        "print(f\"Memory used for LoRA model (GB): {mem_used_lora_model}\")\n",
        "print(f\"Memory used for training (GB): {mem_used_training}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3UdJ48svNS9",
        "outputId": "594914b5-1e3b-419c-a38c-5d20c39ec749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time: 9993.2 s\n",
            "Memory used for original model (GB): 7.2716\n",
            "Memory used for LoRA model (GB): 0.6265\n",
            "Memory used for training (GB): 17.1418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss and learning rate schedule\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_data = []\n",
        "with open(log_file, \"r\") as f:\n",
        "  for ln in f:\n",
        "    log_data.append(json.loads(ln.replace(\"'\",\"\\\"\")))\n",
        "\n",
        "df = pd.DataFrame.from_dict(log_data)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12,8))\n",
        "df[\"loss\"].plot(ax=axs[0])\n",
        "df[\"learning_rate\"].plot(ax=axs[1])\n",
        "df[\"lr_param_group_0\"].plot(ax=axs[1])\n",
        "df[\"lr_param_group_1\"].plot(ax=axs[1])\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "axs[0].set_title(\"Training loss\")\n",
        "axs[1].set_title(\"Learning rates\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "6GuP3o_RR3He",
        "outputId": "f9f9a3ac-13b0-4c48-c706-04b7396c33ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils to check GPU status"
      ],
      "metadata": {
        "id": "bo5SLwuZVcYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## free the memory again\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-KtBwGEf_Al",
        "outputId": "dc3322b4-ea74-4566-cf91-2d9b0624e551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! fuser -v /dev/nvidia*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB8rdMuCgKpj",
        "outputId": "ab22447d-b702-4729-aaeb-304ba48c3db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     USER        PID ACCESS COMMAND\n",
            "/dev/nvidia0:        root        293 F...m python3\n",
            "/dev/nvidiactl:      root        293 F...m python3\n",
            "/dev/nvidia-uvm:     root        293 F...m python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo kill -9 293"
      ],
      "metadata": {
        "id": "CPs5WW_6L_yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainer.get_optimizer_cls_and_kwargs(training_args))\n",
        "print(trainer.get)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oldGt1jFJ7I",
        "outputId": "64e0c17e-0d9d-44b6-a347-d4e4e0c219bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<class 'torch.optim.adamw.AdamW'>, {'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old code (deprecated)"
      ],
      "metadata": {
        "id": "pA4MXwIdxiaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's add the learning rate top the training logs\n",
        "class TrainerWithLoggerLr(Trainer):\n",
        "    def log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None:\n",
        "        # see:\n",
        "        #  - https://discuss.huggingface.co/t/how-to-show-the-learning-rate-during-training/13914/3\n",
        "        #  - method _get_learning_rate() of trainer_pt_utils.py\n",
        "        if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "          for k, group in enumerate(self.optimizer.param_groups()):\n",
        "            logs[f\"lr_param_group_{k}\"] = group[\"lr\"]\n",
        "        else:\n",
        "          for k, v in enumerate(self.lr_scheduler.get_last_lr()):\n",
        "            logs[f\"lr_param_group_{k}\"] = v\n",
        "        print(logs)\n",
        "        super().log(logs)"
      ],
      "metadata": {
        "id": "up9OTOtASB0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING TO HUGGING FACE HUB: BEST MODEL, LAST MODEL\n",
        "\n",
        "#trainer.push_to_hub(\"mpenna77/{run_name}\")\n",
        "\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "def upload_to_hub(api, hub_model_name, path):\n",
        "  if api.repo_exists()\n",
        "  api.create_repo(repo_id=f\"mpenna77/{run_name}\", private=True, exist_ok=True)\n",
        "  api.upload_folder(\n",
        "      folder_path=path,\n",
        "      repo_id=f\"mpenna77/{hub_model_name}\",\n",
        "      repo_type=\"model\",\n",
        "  )\n",
        "\n",
        "api = HfApi(token=userdata.get('HF_TOKEN_WRITE'))\n",
        "\n",
        "hub_model_name_last = f\"{run_name}-checkpoint-{trainer.state.max_steps}\"\n",
        "model_dir_last = f\"{out_dir}/checkpoint-{trainer.state.max_steps}\"\n",
        "upload_to_hub(api, hub_model_name_last, model_dir_last)\n",
        "\n",
        "hub_model_name_best = f\"{run_name}-best-model\"\n",
        "model_dir_best = f\"{out_dir}/best-model\"\n",
        "upload_to_hub(api, hub_model_name_best, model_dir_best)\n",
        "\n"
      ],
      "metadata": {
        "id": "-W2IX1tOVa6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING TO GITHUB: BEST MODEL, LAST MODEL, PROFILING FILES\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "print(f\"Best model at step: {trainer.state.best_global_step}\")\n",
        "print(f\"Best model checkpoint: {trainer.state.best_model_checkpoint}\")\n",
        "print(f\"Metric at best step (on eval set): {trainer.state.best_metric}\")\n",
        "\n",
        "best_model_dir = f\"{out_dir}/best-model\"\n",
        "best_model_state_file = f\"{out_dir}/best-model-trainer_state.json\"\n",
        "last_model_dir = f\"{out_dir}/checkpoint-{trainer.state.max_steps}\"\n",
        "\n",
        "trainer.save_model(best_model_dir)\n",
        "trainer.state.save_to_json(best_model_state_file)\n",
        "\n",
        "#! git lfs install\n",
        "#! git lfs track \"*safetensors\"\n",
        "#! git add .gitattributes\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add {best_model_dir}\n",
        "! git add {best_model_state_file}\n",
        "! git add {last_model_dir}/*json\n",
        "! git add {last_model_dir}/*md\n",
        "! git add {last_model_dir}/*safetensors\n",
        "! git add {last_model_dir}/*jinja\n",
        "! git add {profiling_dir}\n",
        "! git commit -m \"added new model\"\n",
        "! git push origin main"
      ],
      "metadata": {
        "id": "j2kAoMpdVV0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MAX_SEQ_LENGTH = 256\n",
        "context_length = 24\n",
        "stride = 23\n",
        "\n",
        "outputs = tokenizer(\n",
        "    #raw_datasets[\"train\"][:2][\"content\"],\n",
        "    sentence,\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    stride = stride,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8SmQXfviig7",
        "outputId": "47c26ff9-bcfb-4e88-8122-bf5054bcae1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs length: 108\n",
            "Input chunk lengths: [24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]\n",
            "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in outputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vlVJpJcWil1c",
        "outputId": "d922d0fb-4259-4c6c-d722-353e497697ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suitable  music  playing  low  in  the  background  can  begreat for establishing a mood\n",
            "itable  music  playing  low  in  the  background  can  begreat for establishing a mood.\n",
            "  music  playing  low  in  the  background  can  begreat for establishing a mood. Avoid\n",
            " music  playing  low  in  the  background  can  begreat for establishing a mood. Avoid pop\n",
            "  playing  low  in  the  background  can  begreat for establishing a mood. Avoid pop and\n",
            " playing  low  in  the  background  can  begreat for establishing a mood. Avoid pop and rock\n",
            "  low  in  the  background  can  begreat for establishing a mood. Avoid pop and rock songs\n",
            " low  in  the  background  can  begreat for establishing a mood. Avoid pop and rock songs as\n",
            "  in  the  background  can  begreat for establishing a mood. Avoid pop and rock songs as fam\n",
            " in  the  background  can  begreat for establishing a mood. Avoid pop and rock songs as familiarity\n",
            "  the  background  can  begreat for establishing a mood. Avoid pop and rock songs as familiarity \n",
            " the  background  can  begreat for establishing a mood. Avoid pop and rock songs as familiarity  and\n",
            "  background  can  begreat for establishing a mood. Avoid pop and rock songs as familiarity  and \n",
            " background  can  begreat for establishing a mood. Avoid pop and rock songs as familiarity  and  the\n",
            "  can  begreat for establishing a mood. Avoid pop and rock songs as familiarity  and  the \n",
            " can  begreat for establishing a mood. Avoid pop and rock songs as familiarity  and  the  su\n",
            "  begreat for establishing a mood. Avoid pop and rock songs as familiarity  and  the  sudd\n",
            " begreat for establishing a mood. Avoid pop and rock songs as familiarity  and  the  sudden\n",
            "great for establishing a mood. Avoid pop and rock songs as familiarity  and  the  sudden \n",
            " for establishing a mood. Avoid pop and rock songs as familiarity  and  the  sudden  changes\n",
            " establishing a mood. Avoid pop and rock songs as familiarity  and  the  sudden  changes \n",
            "ing a mood. Avoid pop and rock songs as familiarity  and  the  sudden  changes  in\n",
            " a mood. Avoid pop and rock songs as familiarity  and  the  sudden  changes  in \n",
            " mood. Avoid pop and rock songs as familiarity  and  the  sudden  changes  in  tempo\n",
            ". Avoid pop and rock songs as familiarity  and  the  sudden  changes  in  tempo \n",
            " Avoid pop and rock songs as familiarity  and  the  sudden  changes  in  tempo  and\n",
            " pop and rock songs as familiarity  and  the  sudden  changes  in  tempo  and \n",
            " and rock songs as familiarity  and  the  sudden  changes  in  tempo  and  volume\n",
            " rock songs as familiarity  and  the  sudden  changes  in  tempo  and  volume can\n",
            " songs as familiarity  and  the  sudden  changes  in  tempo  and  volume can \n",
            " as familiarity  and  the  sudden  changes  in  tempo  and  volume can  work\n",
            " familiarity  and  the  sudden  changes  in  tempo  and  volume can  work \n",
            "iliarity  and  the  sudden  changes  in  tempo  and  volume can  work  against\n",
            "  and  the  sudden  changes  in  tempo  and  volume can  work  against \n",
            " and  the  sudden  changes  in  tempo  and  volume can  work  against  you\n",
            "  the  sudden  changes  in  tempo  and  volume can  work  against  you.\n",
            " the  sudden  changes  in  tempo  and  volume can  work  against  you. \n",
            "  sudden  changes  in  tempo  and  volume can  work  against  you.  Look\n",
            " sudden  changes  in  tempo  and  volume can  work  against  you.  Look \n",
            "dden  changes  in  tempo  and  volume can  work  against  you.  Look  for\n",
            "en  changes  in  tempo  and  volume can  work  against  you.  Look  for \n",
            "  changes  in  tempo  and  volume can  work  against  you.  Look  for  atmospheric\n",
            " changes  in  tempo  and  volume can  work  against  you.  Look  for  atmospheric \n",
            "  in  tempo  and  volume can  work  against  you.  Look  for  atmospheric  classical\n",
            " in  tempo  and  volume can  work  against  you.  Look  for  atmospheric  classical \n",
            "  tempo  and  volume can  work  against  you.  Look  for  atmospheric  classical  mu\n",
            " tempo  and  volume can  work  against  you.  Look  for  atmospheric  classical  mu-\n",
            "  and  volume can  work  against  you.  Look  for  atmospheric  classical  mu- s\n",
            " and  volume can  work  against  you.  Look  for  atmospheric  classical  mu- sic\n",
            "  volume can  work  against  you.  Look  for  atmospheric  classical  mu- sic \n",
            " volume can  work  against  you.  Look  for  atmospheric  classical  mu- sic  or\n",
            " can  work  against  you.  Look  for  atmospheric  classical  mu- sic  or \n",
            "  work  against  you.  Look  for  atmospheric  classical  mu- sic  or  sound\n",
            " work  against  you.  Look  for  atmospheric  classical  mu- sic  or  soundscape\n",
            "  against  you.  Look  for  atmospheric  classical  mu- sic  or  soundscapes\n",
            " against  you.  Look  for  atmospheric  classical  mu- sic  or  soundscapes \n",
            "  you.  Look  for  atmospheric  classical  mu- sic  or  soundscapes  of\n",
            " you.  Look  for  atmospheric  classical  mu- sic  or  soundscapes  of \n",
            ".  Look  for  atmospheric  classical  mu- sic  or  soundscapes  of  electronic\n",
            "  Look  for  atmospheric  classical  mu- sic  or  soundscapes  of  electronic \n",
            " Look  for  atmospheric  classical  mu- sic  or  soundscapes  of  electronic  music\n",
            "  for  atmospheric  classical  mu- sic  or  soundscapes  of  electronic  music.\n",
            " for  atmospheric  classical  mu- sic  or  soundscapes  of  electronic  music. \n",
            "  atmospheric  classical  mu- sic  or  soundscapes  of  electronic  music.  Pub\n",
            " atmospheric  classical  mu- sic  or  soundscapes  of  electronic  music.  Published\n",
            "  classical  mu- sic  or  soundscapes  of  electronic  music.  Published \n",
            " classical  mu- sic  or  soundscapes  of  electronic  music.  Published  music\n",
            "  mu- sic  or  soundscapes  of  electronic  music.  Published  music especially\n",
            " mu- sic  or  soundscapes  of  electronic  music.  Published  music especially written\n",
            "- sic  or  soundscapes  of  electronic  music.  Published  music especially written for\n",
            " sic  or  soundscapes  of  electronic  music.  Published  music especially written for hor\n",
            "ic  or  soundscapes  of  electronic  music.  Published  music especially written for horror\n",
            "  or  soundscapes  of  electronic  music.  Published  music especially written for horror role\n",
            " or  soundscapes  of  electronic  music.  Published  music especially written for horror roleplaying\n",
            "  soundscapes  of  electronic  music.  Published  music especially written for horror roleplaying games\n",
            " soundscapes  of  electronic  music.  Published  music especially written for horror roleplaying games can\n",
            "scapes  of  electronic  music.  Published  music especially written for horror roleplaying games can also\n",
            "s  of  electronic  music.  Published  music especially written for horror roleplaying games can also be\n",
            "  of  electronic  music.  Published  music especially written for horror roleplaying games can also be pur\n",
            " of  electronic  music.  Published  music especially written for horror roleplaying games can also be purch\n",
            "  electronic  music.  Published  music especially written for horror roleplaying games can also be purchased\n",
            " electronic  music.  Published  music especially written for horror roleplaying games can also be purchased.\n",
            "  music.  Published  music especially written for horror roleplaying games can also be purchased. Period\n",
            " music.  Published  music especially written for horror roleplaying games can also be purchased. Period music\n",
            ".  Published  music especially written for horror roleplaying games can also be purchased. Period music (\n",
            "  Published  music especially written for horror roleplaying games can also be purchased. Period music (appropriate\n",
            " Published  music especially written for horror roleplaying games can also be purchased. Period music (appropriate to\n",
            "lished  music especially written for horror roleplaying games can also be purchased. Period music (appropriate to the\n",
            "  music especially written for horror roleplaying games can also be purchased. Period music (appropriate to the scenario\n",
            " music especially written for horror roleplaying games can also be purchased. Period music (appropriate to the scenario)\n",
            " especially written for horror roleplaying games can also be purchased. Period music (appropriate to the scenario) can\n",
            " written for horror roleplaying games can also be purchased. Period music (appropriate to the scenario) can also\n",
            " for horror roleplaying games can also be purchased. Period music (appropriate to the scenario) can also help\n",
            " horror roleplaying games can also be purchased. Period music (appropriate to the scenario) can also help,\n",
            "ror roleplaying games can also be purchased. Period music (appropriate to the scenario) can also help, such\n",
            " roleplaying games can also be purchased. Period music (appropriate to the scenario) can also help, such as\n",
            "playing games can also be purchased. Period music (appropriate to the scenario) can also help, such as tw\n",
            " games can also be purchased. Period music (appropriate to the scenario) can also help, such as twent\n",
            " can also be purchased. Period music (appropriate to the scenario) can also help, such as twenties\n",
            " also be purchased. Period music (appropriate to the scenario) can also help, such as twenties j\n",
            " be purchased. Period music (appropriate to the scenario) can also help, such as twenties jaz\n",
            " purchased. Period music (appropriate to the scenario) can also help, such as twenties jazz\n",
            "chased. Period music (appropriate to the scenario) can also help, such as twenties jazz played\n",
            "ased. Period music (appropriate to the scenario) can also help, such as twenties jazz played at\n",
            ". Period music (appropriate to the scenario) can also help, such as twenties jazz played at a\n",
            " Period music (appropriate to the scenario) can also help, such as twenties jazz played at a low\n",
            " music (appropriate to the scenario) can also help, such as twenties jazz played at a low volume\n",
            " (appropriate to the scenario) can also help, such as twenties jazz played at a low volume.\n"
          ]
        }
      ]
    }
  ]
}