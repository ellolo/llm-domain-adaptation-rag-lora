{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzeEy40HokSG"
      },
      "source": [
        "# Dataset generation for domain-specific LLM fine-tuning\n",
        "\n",
        "This notebook generates a QA dataset from domain-specific document(s).\n",
        "The generated dataset can be used to fine-tune a LLM to answer questions pertaining to the domain.\n",
        "\n",
        "The notebook takes in input a domain document as a PDF file, coverts it into text format and splits it into chunks. Then, for each chunk, it prompts a LLM to generate QA pairs referring to the content of the chunk.\n",
        "\n",
        "In this notebook, I use the [Chtulhu Rulebook](https://archive.org/details/call-of-cthulhu-core-rulebook-by-chaosium-inc.-z-lib.org) as an example of domain document. The PDF file is located in `data/pdf`.\n",
        "\n",
        "## Stack\n",
        "I use the following stack:\n",
        "- [Meta Synthetic Data Kit (MSDK](https://github.com/meta-llama/synthetic-data-kit/tree/main/synthetic_data_kit) to parse the input PDF, prompt the LLM to generate QA pairs, and curate these pairs.\n",
        "- [Llama-3.2-3B-Instruct](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct) to generate QA pairs.\n",
        "- [VLLM](https://docs.vllm.ai/en/latest/) to serve the above LLM to the MSDK.\n",
        "\n",
        "I recommend to use a machine with at least a **A100** Nvidia GPU, since the LLM is served locally.\n",
        "\n",
        "## Details\n",
        "In details, the following steps are performed:\n",
        "\n",
        "- Startup `Llama-3.2-3B-Instruct` LLM on the local machine using VLLM.\n",
        "\n",
        "- Covert the PDF in `data/pdf` into text format and store it into `data/output`, using the MSDK's `ingest` function.\n",
        "\n",
        "- Chunk the text file into smaller files of 2048 tokens (with 64 tokens overlap) and store them into `data/output`. The overlap between chunks ensures that the QA pairs cover all the content of the original document, since our chunking algorithm is rather simple and may harshly split sentences into different chunks.\n",
        "\n",
        "- Feed each chunk file to the MSDK's `create` function. The function prompts the LLM to generate 25 QA pairs for each of the chunk files. For each chunk file a corresponding file containing QA pairs is created in  `data/generated`.\n",
        "\n",
        "- Feed each QA pair file to the MSDK's `curate` function. The function prompts the VLLM to retain only QA pairs above a given quality threshold. For each QA pair file a corresponding file containing high-quality QA pairs is created in `data/curated`.\n",
        "\n",
        "- Format generated and curated files into ChatML format, for standardized use in LLM fine-tuning.\n",
        "\n",
        "- Push all files into this Github repo, under the `data` folder.\n",
        "\n",
        "Remember: especially if running on Google Colab, ensure that your local machines has at least a A100 GPU.\n",
        "\n",
        "## References\n",
        "- [Meta kit page](https://github.com/meta-llama/synthetic-data-kit/blob/main/README.md)\n",
        "- [Unsloth collab notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb#scrollTo=2ejIt2xSNKKp). I heavily used this Notebook as a reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKP6o8zSEVWP"
      },
      "source": [
        "## Install dependencies and cloning github repo\n",
        "\n",
        "The github repo is used to store the generated dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uJwGuUL33R4O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install synthetic-data-kit==0.0.3\n",
        "\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install vllm\n",
        "else:\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    import re, requests\n",
        "    !pip install --no-deps vllm\n",
        "    !pip install --no-deps bitsandbytes\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUxYhe_By7Yf",
        "outputId": "945b0548-abd6-4c7b-a12f-21e1727a221a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-4c545346\n",
            "Cloning into 'cthulhu_fine_tuning'...\n",
            "remote: Enumerating objects: 3207, done.\u001b[K\n",
            "remote: Counting objects: 100% (3199/3199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1233/1233), done.\u001b[K\n",
            "remote: Total 3207 (delta 1986), reused 3168 (delta 1963), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3207/3207), 71.90 MiB | 16.56 MiB/s, done.\n",
            "Resolving deltas: 100% (1986/1986), done.\n",
            "Updating files: 100% (2804/2804), done.\n"
          ]
        }
      ],
      "source": [
        "# CLONING REPO CONTAINING INPUT DATA\n",
        "#\n",
        "# this is also the repo where we will store intermediate data\n",
        "\n",
        "! mkdir -p /root/.ssh\n",
        "with open(\"/root/.ssh/id_rsa\", mode=\"w\") as fp:\n",
        "    fp.write(\"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\n",
        "NhAAAAAwEAAQAAAgEApUPD7gz0vInJz1dYlIzxWNa2fuvnUiT/TrVJNk+WUK5+KgLFgK6g\n",
        "jYWDITJm9gLMVDQwhTtKosHzgnzVvP4PgYlGf5jRcqUrarWiXejYIG9zoKHyi33X1gi2zo\n",
        "FKAATnycO+XHdGTz3JOrrmlAsvkIKokji3rqWFblDFJ3u0aSeyzHxNs3xJNFGpihBWaAYy\n",
        "GVQaV14JVdU3PfmyaxmzD77NE53Z/fVk0k72SjNJ/7Ql50OaXpsGXTkySMGCXrihnzajF2\n",
        "tVJwdxnWKT9Z2/8akLsrg8LhiVcRvWrrElgo63azLrTIaKyv+D827qh4k1pEvmZ+aKjPc0\n",
        "2Ota9l6JEIhYhnOhtsQwU9Mjcq501Ce4L/qBsDfRFx+qx+vCxZAMc80+Rapbat5jfsNpbp\n",
        "YhgtXzmTnlIGO0Fc0mX/IKyGTUL/RyV4B0OP6vIAanLS01WIMbkAXfkEdgu1MKvKF8hQGl\n",
        "GryB1NfEPDBE3jyCljxhBuyFqN5Kp/EVeFiw38AwQAe4+u5VDgy3ZHqcRT/H0UryYjC6S0\n",
        "nymFB/ObOijWw03W6YKEaOeqE/HNww7CO3MtuAbgwUqNYZF+zOe1v58Mm7xZVzk8+hirok\n",
        "wQlTl4CYWX/ql0+Jwbz2IpDiX4iCWcOMo40cJmZlYVR4jL54sETNCL91FbkmjP3/YP5Qn6\n",
        "kAAAdIVX2ag1V9moMAAAAHc3NoLXJzYQAAAgEApUPD7gz0vInJz1dYlIzxWNa2fuvnUiT/\n",
        "TrVJNk+WUK5+KgLFgK6gjYWDITJm9gLMVDQwhTtKosHzgnzVvP4PgYlGf5jRcqUrarWiXe\n",
        "jYIG9zoKHyi33X1gi2zoFKAATnycO+XHdGTz3JOrrmlAsvkIKokji3rqWFblDFJ3u0aSey\n",
        "zHxNs3xJNFGpihBWaAYyGVQaV14JVdU3PfmyaxmzD77NE53Z/fVk0k72SjNJ/7Ql50OaXp\n",
        "sGXTkySMGCXrihnzajF2tVJwdxnWKT9Z2/8akLsrg8LhiVcRvWrrElgo63azLrTIaKyv+D\n",
        "827qh4k1pEvmZ+aKjPc02Ota9l6JEIhYhnOhtsQwU9Mjcq501Ce4L/qBsDfRFx+qx+vCxZ\n",
        "AMc80+Rapbat5jfsNpbpYhgtXzmTnlIGO0Fc0mX/IKyGTUL/RyV4B0OP6vIAanLS01WIMb\n",
        "kAXfkEdgu1MKvKF8hQGlGryB1NfEPDBE3jyCljxhBuyFqN5Kp/EVeFiw38AwQAe4+u5VDg\n",
        "y3ZHqcRT/H0UryYjC6S0nymFB/ObOijWw03W6YKEaOeqE/HNww7CO3MtuAbgwUqNYZF+zO\n",
        "e1v58Mm7xZVzk8+hirokwQlTl4CYWX/ql0+Jwbz2IpDiX4iCWcOMo40cJmZlYVR4jL54sE\n",
        "TNCL91FbkmjP3/YP5Qn6kAAAADAQABAAACAEaWEH/C4d8LPPqFlIxyPH0UzAKe0IC505/y\n",
        "9y+uw4V3WeSopWGmdGWt0kmiBO7rWAlY9yZYojKtA0xG9GWR396UWtuR0leUq1wa8xwIIR\n",
        "ONdsXzlaw1ljPRKf8+onQqpDN9mvdUbF/ZBHNEs8okku62l7hIaE+8W6a38dVA1VgagBgt\n",
        "uWRBX+TsQiz5eGZayxgdX1jUjcku1bbvSODMq7m8ZUwNHjgFkUfwOOqNSHxiHdROgAcLUK\n",
        "cNkGgZ2oyJcGKXzAXrLoYKfGDb41VDSOG3MYtmfDG2B1I1sTaQ6/P87+Nl7rETQAGfK+UU\n",
        "CTDVjmc7kc/r3F6EEXra31GeJA0OrsWt8o5lLO40DQLQVnta3xxQHNVIMlSOjgEVTd7MrC\n",
        "a81v07L5dLTEAj73guKFHYnRT/Ixd1AKYrX7OhMeVGN9sJn7y7FYD4m+gF3T+et2VOWw0R\n",
        "RuQyjoCopnlH+Lo24yJk/XSsQL3IVHasTCaThMz/km7AH7KktxK7/SNx7PwOuOU6076IMd\n",
        "37eawrIg5ecoEAp1MKfMJUG2jcN1Xl7RFpJaDuCu7qsFfjiSEPnVh6yQUkyMBuzni8mLna\n",
        "588TxVp9V3P6y9XZyDMeBN671u3Ro8HCxEJEQxTZoFgxdpaUQ0kO4oy7FrJxd22GkldMyi\n",
        "r856D7O+NORiFPWy//AAABABJLjG39AtIOBh+4mkIpT0FdWWqyTBwJdb+t3DyP+vIpLxmg\n",
        "U0dsTKVcAudwjr9Dv9XfA/BejcNhMvlzB9ob0NUi7tKTg8Xo2JhZQvpnQkBEavlTLm6rgN\n",
        "l3egQf8tIHCLfZ5+aF+mwODibIRK7kBTWlc7ln586JIonhgWzm0g3sD23LG0YW/2cCgi0r\n",
        "qtZQ6lUsJGFJGOUrZnomHnS9woCxS+BeEH4uE8lzXr5/0NvxmAGhHX9kt2fmwZWicrfqnu\n",
        "Crd5RDp9Dpe/iQzyvJssiSPvUFEwTOQEmO5Ce/kNPA0hKpBWYRoQpkVdnPsgEOFiZIqmbR\n",
        "vUu6I6zqxsNA09YAAAEBAOHLauNS72FUrybBrDoq5cvdTsg9yayMbuNsVGLHX/GOZWCkLF\n",
        "b9E7200i28rrz4Ue6QwsOTuabGT18Y2cTVQmiTaQzOLCu9IA7p90r14zb18V0KXyrV4Tlm\n",
        "YpfY+gqyWbiSgaTIHgZFW6qT+/kHKi8Wj/yx3NVtJFYefMcHl71wF90au+l9u+CGQkyZ1w\n",
        "yYXFIe9RmJt6uuLjoT55wdit63OdfGxTDIzdaLW3FHlxcgk7NmIYslHhLBBnSbM+olKGjh\n",
        "R7dkOzTuB2S76hOOLMSkFGUozYcscgLKlFGriZJjRzMzlnTyplH8WOp8NHUB3syy9/sSrJ\n",
        "rGDMftPsTff9MAAAEBALtfb4XvNoa/PhryNEycwbq9w0rrp8JL45ia6r7fgs6YJSReHwfr\n",
        "Lb/PtXDFay+dVNZwnCzQT1Ha9y40KhXpDEGFWBGwvcNnFhm1ZTl4TqPlSnFdTS+P6PE+vY\n",
        "Fsva2mLxNEBLjQPQPPpE1aAmrRsH3hG1OAbCa7F5BufTiz7WsHnvwSZpXgmudhkAZZGQYb\n",
        "6/0X7pUJBkm0N4XIjl6dreTMxL6iERx1eQM/S1k97tqH1RUqrLxdcEWvEygB1WpqyiEFrM\n",
        "q41tWqMTkadBD5lyg91y2OHcV5a5R+/9fLKUxVrkKvMziRlMU8uNPD6pggf5P0g/uv3t/g\n",
        "fAVid9m6cRMAAAARcm9vdEBjMjhlMjE0Y2Q1OGYBAg==\n",
        "-----END OPENSSH PRIVATE KEY-----\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# <COPY FROM LOCAL DISK AT: ~/dev/llm_cthulhu_fine_tuning/keys>\n",
        "! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "! chmod go-rwx /root/.ssh/id_rsa\n",
        "! git clone git@github.com:ellolo/cthulhu_fine_tuning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qwrudfreSjSG",
        "outputId": "ed8b97e3-81e2-4fa4-efc0-d0b41c257ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/cthulhu_fine_tuning\n"
          ]
        }
      ],
      "source": [
        "%cd /content/cthulhu_fine_tuning\n",
        "! mkdir -p data/{pdf,html,youtube,docx,ppt,txt,output,generated,cleaned,final}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x6sSPHdFKKD"
      },
      "source": [
        "# Start serving LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS9euWyfs_p8",
        "outputId": "0c29026a-0b99-42ab-a6eb-88f3c24d126a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/cthulhu_fine_tuning\n"
          ]
        }
      ],
      "source": [
        "# START VLLM\n",
        "# If in Google Collab, use A100 GPU\n",
        "# https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#cli-reference\n",
        "#\n",
        "# It takes some time to get the VLLM server up.\n",
        "# To check if it is ready, run the next cell or check the log file vll_logs.out\n",
        "# below\n",
        "\n",
        "#! NCCL_P2P_DISABLE=1 VLLM_LOGGING_LEVEL=DEBUG CUDA_LAUNCH_BLOCKING=1 NCCL_DEBUG=TRACE VLLM_TRACE_FUNCTION=1 vllm serve unsloth/Llama-3.2-3B-Instruct --port 8000 --gpu-memory-utilization 0.8 --max_model_len 2048\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "! VLLM_LOGGING_LEVEL=DEBUG vllm serve \\\n",
        "    unsloth/Llama-3.2-3B-Instruct \\\n",
        "    --port 8000 \\\n",
        "    --gpu-memory-utilization 0.8 \\\n",
        "    --max_model_len 2048 \\\n",
        "    --quantization bitsandbytes > vllm_logs.out 2> vllm_logs.err &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEtrDYmFvGjx",
        "outputId": "29fd6516-6110-47c9-d06e-12a870ce97ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1747653574\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-83eb9084ff3846f88471e28219f211a6'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1747653574\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "# check if/when VLLM server is up\n",
        "\n",
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkP0IH1NFQy1"
      },
      "source": [
        "# Covert PDF into text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fmDsVsGg8yX",
        "outputId": "e0ce5c75-d25b-466f-e589-6a8c0cf4c9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/cthulhu_fine_tuning\n",
            "Extracting pdf...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Processing data/pdf/cthulhu.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/cthulhu.txt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# EXTRACT PDF INTO TXT\n",
        "#\n",
        "# parse pdf and store into text file into data/output directory.\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "print(\"Extracting pdf...\")\n",
        "!synthetic-data-kit ingest data/pdf/cthulhu.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk text file\n",
        "Tokenize the text document using the tokenizer of the served LLM model. Split the token sequence into chunks of `chunk_size` tokens. Store the the corresponding textual chunks into `data/output`."
      ],
      "metadata": {
        "id": "2zkaCjSiRkGV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRREMbwnhRY7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "def chunk_data(\n",
        "    filename: str,\n",
        "    tokenizer,\n",
        "    max_seq_length: int = 2048,\n",
        "    max_generation_tokens: int = 512,\n",
        "    overlap: int = 64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Chunks text data from a given file into smaller files based on token limits.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The path to the input text file.\n",
        "            tokenizer: The tokenizer to use for tokenizing the text.\n",
        "            max_seq_length (int, optional): The maximum sequence length for each chunk. Defaults to 2048.\n",
        "            max_generation_tokens (int, optional): The maximum number of tokens to reserve for generation.\n",
        "                                                   Defaults to 512.\n",
        "            overlap (int, optional): The number of overlapping tokens between consecutive chunks.\n",
        "                                     Defaults to 64.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of filenames for the generated chunk files.\n",
        "\n",
        "        Raises:\n",
        "            RuntimeError: If the calculated maximum tokens for input is too small.\n",
        "            AssertionError: If the input filename is None or does not exist.\n",
        "        \"\"\"\n",
        "        # Adapted from:\n",
        "        # https://github.com/unslothai/unsloth/blob/main/unsloth/dataprep/synthetic.py\n",
        "\n",
        "        # Chunks data by max tokens and generation length\n",
        "        assert(filename is not None)\n",
        "        assert(os.path.exists(filename))\n",
        "\n",
        "        with open(filename, \"r\") as f:\n",
        "          text = f.read()\n",
        "\n",
        "        max_tokens = max_seq_length - max_generation_tokens*2 - 128 # -128 to reduce errors\n",
        "        if max_tokens <= 5:\n",
        "            raise RuntimeError(\"Generation length is way too long!\")\n",
        "        input_ids = tokenizer(text, add_special_tokens = False).input_ids\n",
        "\n",
        "        # Get left and right boundaries\n",
        "        length = len(input_ids)\n",
        "        n_chunks = int(np.ceil(length / (max_tokens - overlap)))\n",
        "        boundaries = np.ceil(np.linspace(0, length - overlap, n_chunks)).astype(int)\n",
        "        boundaries = np.stack((boundaries[:-1], (boundaries + overlap)[1:])).T\n",
        "        boundaries = np.minimum(boundaries, length).tolist()\n",
        "\n",
        "        # Get extension of filename like .txt\n",
        "        filename, extension = os.path.splitext(filename)\n",
        "        if filename.endswith(\"/\"):\n",
        "          filename = filename[:-1]\n",
        "\n",
        "        all_filenames = []\n",
        "        for i, (left, right) in enumerate(boundaries):\n",
        "            chunked_text = tokenizer.decode(input_ids[left : right])\n",
        "            new_filename = f\"{filename}_{i}{extension}\"\n",
        "            all_filenames.append(new_filename)\n",
        "            with open(new_filename, \"w\") as f:\n",
        "              f.write(chunked_text)\n",
        "        pass\n",
        "        return all_filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oGHqbljBog",
        "outputId": "5d2da72b-aff1-40e1-de20-6763a34e4a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing and chunking...\n",
            "560 ['data/output/cthulhu_0.txt', 'data/output/cthulhu_1.txt', 'data/output/cthulhu_2.txt']\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "max_generation_tokens = 512\n",
        "overlap = 64\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Tokenizing and chunking...\")\n",
        "filenames = chunk_data(\n",
        "    \"data/output/cthulhu.txt\",\n",
        "    tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    max_generation_tokens=max_generation_tokens,\n",
        "    overlap=overlap,\n",
        "    )\n",
        "print(len(filenames), filenames[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i_EAR_8FVh-"
      },
      "source": [
        "# Generate QA pairs\n",
        "\n",
        "For each chunk file, generate QA pairs using the [MSDK generator](https://github.com/meta-llama/synthetic-data-kit/blob/main/synthetic_data_kit/generators/qa_generator.py), as follows:\n",
        "- Prompt the LLM to generate a 3-5 sentence summary of the chunk, using the prompt\n",
        "stored in the MSDK [config file](https://github.com/ellolo/cthulhu_fine_tuning/blob/main/config/synthetic_data_kit_config.yaml). Temperature for this task is set to 0.1.\n",
        "- Sub-chunk each chunk into smaller chunks.\n",
        "- Prompt the LLM to generate 25 QA pairs for each chunk. Specifically, for each sub-chunk: ask LLM to generate 25/num_subchunks QA pairs using the QA prompt stored in the [config file](https://github.com/ellolo/cthulhu_fine_tuning/blob/main/config/synthetic_data_kit_config.yaml).\n",
        "The output QA pairs are stored in json format in the `data/generated` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxAssoVClwNX"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "out_dir = \"data/output\"\n",
        "gen_dir = \"data/generated\"\n",
        "num_qa_pairs = 25\n",
        "\n",
        "# Get list of chunk files for which QA pairs have been already generated in\n",
        "# previous sessions of the notebook.\n",
        "generated_files = glob.glob(f\"{gen_dir}/*pairs.json\")\n",
        "if generated_files:\n",
        "  base_name = generated_files[0].split(\"/\")[-1]\n",
        "  base_name = \"_\".join(base_name.split(\"_\")[0:-3])\n",
        "  completed_files = [\n",
        "      f\"{out_dir}/{base_name}_{fname.split('_')[-3]}.txt\" for fname in generated_files\n",
        "      ]\n",
        "  filenames = sorted(glob.glob(f\"{out_dir}/*_[0-9]*.txt\"), key=os.path.getmtime)\n",
        "  filenames_to_do = list(set(filenames) - set(completed_files))\n",
        "  print(f\"QA pairs already generated for {len(completed_files)} files\")\n",
        "  print(f\"Need to generate for {len(filenames_to_do)} files\")\n",
        "else:\n",
        "  filenames_to_do = filenames\n",
        "  print(f\"Need to generate for {len(filenames_to_do)} files\")\n",
        "\n",
        "\n",
        "# Generate QA pairs for chunk files that still need to be processed.\n",
        "i = 0\n",
        "for fname in filenames_to_do:\n",
        "    print(f\"Doing {i+1} of remaining {len(filenames_to_do)} files\")\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {fname} \\\n",
        "        --num-pairs {num_qa_pairs} \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(5) # Sleep a bit to leave some room for processing\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9LW2c5gxtSA"
      },
      "outputs": [],
      "source": [
        "# Store generated QA pair files in github so that we don't loose them when we\n",
        "# close the notebook\n",
        "\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add data/generated\n",
        "! git commit -m \"added new qa pairs\"\n",
        "! git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BL0aroKocub"
      },
      "outputs": [],
      "source": [
        "# double check that the generated QA pairs json files are correct\n",
        "\n",
        "import glob\n",
        "import json\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "gen_dir = \"data/generated/*qa_pairs.json\"\n",
        "\n",
        "bad_count = 0\n",
        "fnames = glob.glob(gen_dir)\n",
        "for fname in fnames:\n",
        "  is_bad = False\n",
        "  with open(fname, 'r', encoding='utf-8') as f:\n",
        "    try:\n",
        "      data = json.load(f)\n",
        "      if not \"summary\" in data:\n",
        "        print(f\"{fname}: missing field summary.\")\n",
        "        is_bad = True\n",
        "      if \"summary\" in data and len(data[\"summary\"]) < 10:\n",
        "        print(f\"{fname}: missing summary text.\")\n",
        "        is_bad = True\n",
        "      if not \"qa_pairs\" in data:\n",
        "        print(f\"{fname}: missing field qa_pairs.\")\n",
        "        is_bad = True\n",
        "      if \"qa_pairs\" in data and len(data[\"qa_pairs\"]) == 0:\n",
        "        print(f\"{fname}: missing qa pairs.\")\n",
        "        is_bad = True\n",
        "      bad_count += is_bad\n",
        "    except json.JSONDecodeError:\n",
        "      print(f\"{fname}: not a valid json.\")\n",
        "      bad_count += 1\n",
        "\n",
        "print(f\"Number of badly generated files: {bad_count} of total {len(fnames)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqH6hJOsFdtb"
      },
      "source": [
        "# Curate QA pairs\n",
        "\n",
        "Filter out low quality QA pairs using MSDK.\n",
        "\n",
        "We use the default code and strategy of the MSDK, as follows:\n",
        "- Prompt the same LLM that generated the pairs, to score them according to accuracy (0-3), relevance (0-2), clarity (0-2) and usefulness (0-3). Prompts for these tasks are in the [config file](https://github.com/ellolo/cthulhu_fine_tuning/blob/main/config/synthetic_data_kit_config.yaml).\n",
        "- Retain QA pairs which have a summed score above a threshold (0 to 10, where 10 is highest quality)\n",
        "\n",
        "\n",
        "Remember that VLLM needs to be up and running to run this code.\n",
        "\n",
        "Curation took me about about 1.5 hours in Google Colab using a A100 GPU, costing about 10 compute units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swoE20zpwEQ9"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "gen_dir = \"data/generated/*qa_pairs.json\"\n",
        "clean_dir = \"data/cleaned\"\n",
        "\n",
        "Path(clean_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "fnames = glob.glob(gen_dir)\n",
        "for fname in fnames:\n",
        "  out_fname_base = f\"{Path(fname).stem}_clean.json\"\n",
        "  out_fname = Path(clean_dir,out_fname_base)\n",
        "  print(out_fname)\n",
        "  print(f\"Doing {i+1} of {len(fnames)} files\")\n",
        "  if not out_fname.exists():\n",
        "    ! synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        curate \\\n",
        "        --threshold 6 \\\n",
        "        --output {out_fname} \\\n",
        "        {fname}\n",
        "  else:\n",
        "    print(f\"{out_fname} already done, skipping!\")\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5HtsRuH9Ohe"
      },
      "outputs": [],
      "source": [
        "# store cleaned QA pair files in github so that we don't loose them when we\n",
        "# close the notebook\n",
        "\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add data/cleaned\n",
        "! git commit -m \"added new cleaned qa pairs\"\n",
        "! git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C9rcrNs9-Mm"
      },
      "outputs": [],
      "source": [
        "# check how many of the generated QA pairs have been retained after curation\n",
        "\n",
        "import glob\n",
        "import json\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "clean_dir = \"data/cleaned/*qa_pairs_clean.json\"\n",
        "\n",
        "fnames = glob.glob(clean_dir)\n",
        "total_pairs = 0\n",
        "retained_pairs = 0\n",
        "bad_count = 0\n",
        "for fname in fnames:\n",
        "  with open(fname, 'r', encoding='utf-8') as f:\n",
        "    try:\n",
        "      data = json.load(f)\n",
        "      metrics = data[\"metrics\"]\n",
        "      total_pairs += metrics[\"total\"]\n",
        "      retained_pairs += metrics[\"filtered\"]\n",
        "    except:\n",
        "      print(f\"Skipping file: {fname} (bad json).\")\n",
        "      bad_count += 1\n",
        "\n",
        "print(f\"Retained {retained_pairs} QA pairs of total {total_pairs} ({retained_pairs / total_pairs})\")\n",
        "print(f\"{bad_count} file of total {len(fnames)} where skipped due to bad json.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObOXC0thIxW"
      },
      "source": [
        "# Format QA pairs to ChatML\n",
        "\n",
        "Format the generated and cleaned QA pairs files into a chat template that can be later easily converted json to the [ChatML format](https://gist.github.com/edwardzjl/8df07c1f7140c9a3e2f48d33a8032090).\n",
        "\n",
        "See [Hugging Face LLM course](https://huggingface.co/learn/llm-course/en/chapter11/2) for a short intro on ChatML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4nuGCCWAdBRK"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "#input_dir = \"data/cleaned/*.json\"\n",
        "#format_dir = \"data/final_cleaned\"\n",
        "\n",
        "input_dir = \"data/generated/*.json\"\n",
        "format_dir = \"data/final_generated\"\n",
        "\n",
        "Path(input_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "fnames = glob.glob(input_dir)\n",
        "for fname in fnames:\n",
        "  out_fname_base = f\"{Path(fname).stem}_final.json\"\n",
        "  out_fname = Path(format_dir,out_fname_base)\n",
        "  print(out_fname)\n",
        "  print(f\"Doing {i+1} of {len(fnames)} files\")\n",
        "  if not out_fname.exists():\n",
        "    ! synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as \\\n",
        "        --format chatml \\\n",
        "        --storage json \\\n",
        "        --output {out_fname} \\\n",
        "        {fname}\n",
        "  else:\n",
        "    print(f\"{out_fname} already done, skipping!\")\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxKGtm2Yj-uw"
      },
      "outputs": [],
      "source": [
        "# store formatted QA pair files in github so that we don't loose them when we\n",
        "# close the notebook\n",
        "\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add data/final_cleaned\n",
        "! git add data/final_generated\n",
        "! git commit -m \"added new formatted qa pairs\"\n",
        "! git push origin main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqCFCT0LqM_0"
      },
      "source": [
        "# Utils to check GPU status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7UxxCbVqSRH",
        "outputId": "233544df-9c5e-4f98-a04a-21200bb266ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri May 16 10:42:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0             58W /  400W |     423MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!  nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U58-9XSqUIr",
        "outputId": "32a7e383-a6b6-48a3-9a04-e9a2bfe1498b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     USER        PID ACCESS COMMAND\n",
            "/dev/nvidia0:        root        717 F...m python3\n",
            "/dev/nvidiactl:      root        717 F...m python3\n",
            "/dev/nvidia-uvm:     root        717 F...m python3\n"
          ]
        }
      ],
      "source": [
        "# check pids running on gpu\n",
        "! sudo fuser -v /dev/nvidia*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlNkvq97qXTC"
      },
      "outputs": [],
      "source": [
        "# kill some pids\n",
        "! kill -9 9597"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbsCjaw3pESz"
      },
      "source": [
        "# Old code (deprecated)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np42Y8y0Ee8-"
      },
      "source": [
        "This is old code to install dependencies if one wanto to use the unsloth wrapper (see: )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MiQXu9nEl-X"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "    !pip install synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    !pip install synthetic-data-kit==0.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGI7xxazEvJl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwR90eJJFyqO"
      },
      "source": [
        "Launching the vllm server using unsloth, as in the cell below,  did not work. The server of the llm started hanging after 3 documents.\n",
        "This is why we instead launch the server manually using `vllm serve unsloth/Llama-3.2-3B-Instruct`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KLSqo77I6ZbM"
      },
      "outputs": [],
      "source": [
        "# START AND SERVE LLM MODEL\n",
        "#\n",
        "# initialize model that will generate the dataset and serve it on port 8000\n",
        "# Specifically:\n",
        "#   - initialize HF tokenizer for the specific model\n",
        "#   - Sets max_seq_length: user-specified length of input sequence (context +\n",
        "#     generated tokens) if mem allows\n",
        "#   - Sets max_num_seqs (i.e. prompts that can be passed in a single inference\n",
        "#     call) based on avail mem.\n",
        "#   - Load vllm model bitsandbytes weights quantization\n",
        "#     (https://docs.vllm.ai/en/latest/,\n",
        "#     https://docs.vllm.ai/en/latest/api/vllm/vllm.engine.llm_engine.html)\n",
        "#   - serve vllm model on localhost:8000 as a subprocess\n",
        "\n",
        "from unsloth.dataprep import SyntheticDataKit\n",
        "\n",
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "   #model_name = \"unsloth/Llama-3.3-70B-Instruct\",\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        "    gpu_memory_utilization = 0.85,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiREHaCkWnGk"
      },
      "outputs": [],
      "source": [
        "# CONFIGURE QUESTION GENERATION\n",
        "#\n",
        "# Sets the parameters that will be used for generating QA pairs using the\n",
        "# Meta Toolkit.\n",
        "# Parameters are:\n",
        "#  - temperature\n",
        "#  - top_p\n",
        "#  - chunk_size:            size of text chunks for processing (i.e. how big are\n",
        "#                           the chunks in which each document will be split)\n",
        "#  - overlap:               overlap (num tokens) between chunks to maintain\n",
        "#                           context\n",
        "#  - max_generation_tokens: max number of tokens that will be generated by the\n",
        "#                           model when generating a single question\n",
        "#  - num_pairs:             default number of QA pairs to generate for each\n",
        "#                           chunk\n",
        "#\n",
        "# Note that the chunk_size parameter of the Meta toolkit is set automatically to\n",
        "# max_seq_length - max_generation_tokens*2\n",
        "# chunk_size is basically the size of the input layer.\n",
        "#\n",
        "# All the parameters above are then written to the config file of the  Meta\n",
        "# toolkit (synthetic_data_kit_config.yaml).\n",
        "#\n",
        "# See here for full config documentation of the Meta Toolkit:\n",
        "# https://github.com/meta-llama/synthetic-data-kit/blob/main/synthetic_data_kit/config.yaml\n",
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbhquJOx4wOB"
      },
      "outputs": [],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UfdiQ-_zlAcD"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    line = generator.vllm_process.stdout.readline()\n",
        "    if not line:\n",
        "        break\n",
        "    print(line.rstrip(), flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU9P09kB5XMK",
        "outputId": "e77f13ec-de49-4488-9277-51ce346fb083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting pdf...\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Processing data/pdf/cthulhu.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/cthulhu.txt\u001b[0m\n",
            "Tokenizing and chunking...\n",
            "560 ['data/output/cthulhu_0.txt', 'data/output/cthulhu_1.txt', 'data/output/cthulhu_2.txt']\n"
          ]
        }
      ],
      "source": [
        "# PREPARE TEXT INTO CHUNKS\n",
        "#\n",
        "# parse pdf and store into text file into data/output directory.\n",
        "print(\"Extracting pdf...\")\n",
        "!synthetic-data-kit \\\n",
        "    -c synthetic_data_kit_config.yaml \\\n",
        "    ingest data/pdf/cthulhu.pdf\n",
        "\n",
        "# Tokenize the document using appropriate tokenizer, splits the full token\n",
        "# sequence into chunks of length chunk_size tokens, and stores the corresponding\n",
        "# textual chunks into output directory\n",
        "print(\"Tokenizing and chunking...\")\n",
        "filenames = generator.chunk_data(\"data/output/cthulhu.txt\")\n",
        "print(len(filenames), filenames[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEKBloKR-5_z",
        "outputId": "e7b753cf-7b78-43a9-ebd3-30ebcbe14b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['data/generated/cthulhu_1_qa_pairs.json']\n",
            "QA pairs already generated for 1 files\n",
            "Need to generate for 559 files\n"
          ]
        }
      ],
      "source": [
        "# GENERATE QA PAIRS\n",
        "#\n",
        "# The output is sotired into data/generated\n",
        "#\n",
        "# Parameters:\n",
        "#  --num-pairs: number of generations per chunk (e.g. num of QA pairs)\n",
        "#  --type:      type of generation. Can be:\n",
        "#               qa: QA pairs\n",
        "#               cot: chain of thoughts\n",
        "#\n",
        "# QA are generated as follows, for each chunk:\n",
        "#   - ask llm to generate a 3-5 sentence summary of the chunk, using the prompt\n",
        "#     stored in the config file: https://github.com/meta-llama/synthetic-data-kit/blob/main/configs/config.yaml)\n",
        "#     temperature for this task is set to 0.1\n",
        "#   - sub-chunks each chunk into smaller chunks\n",
        "#   - ask to generate 25 QA pairs for each chunk. Specifically, for each\n",
        "#     sub-chunk: ask llm to generate 25/num_subchunks QA pairs using QA prompt\n",
        "#     stored in the config file: https://github.com/meta-llama/synthetic-data-kit/blob/main/configs/config.yaml)\n",
        "#\n",
        "# Look here for more details:\n",
        "# https://github.com/meta-llama/synthetic-data-kit/blob/main/synthetic_data_kit/generators/qa_generator.py\n",
        "import glob\n",
        "import os\n",
        "\n",
        "out_dir = \"data/output\"\n",
        "gen_dir = \"data/generated\"\n",
        "\n",
        "# get list of files for which QA pairs have been already generated\n",
        "generated_files = glob.glob(f\"{gen_dir}/*pairs.json\")\n",
        "if generated_files:\n",
        "  print(generated_files)\n",
        "  base_name = generated_files[0].split(\"/\")[-1]\n",
        "  base_name = \"_\".join(base_name.split(\"_\")[0:-3])\n",
        "  completed_files = [\n",
        "      f\"{out_dir}/{base_name}_{fname.split('_')[-3]}.txt\" for fname in generated_files\n",
        "      ]\n",
        "  print(f\"QA pairs already generated for {len(completed_files)} files\")\n",
        "\n",
        "  # get list of output files\n",
        "  filenames = sorted(glob.glob(f\"{out_dir}/*_[0-9]*.txt\"), key=os.path.getmtime)\n",
        "  filenames_to_do = list(set(filenames) - set(completed_files))\n",
        "  print(f\"Need to generate for {len(filenames_to_do)} files\")\n",
        "else:\n",
        "  filenames_to_do = filenames\n",
        "  print(f\"Need to generate for {len(filenames_to_do)} files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "kCYsIDeeiipe",
        "outputId": "543c8bb0-14fb-4662-99a8-670bb4417e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2KProcessing 5 chunks to generate QA pairs...\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 25 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/cthulhu_160_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/cthulhu_160_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/cthulhu_160.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/cthulhu_160_qa_pairs.json\u001b[0m\n",
            "\u001b[2KProcessing 6 chunks to generate QA pairs...\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 24 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/cthulhu_44_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/cthulhu_44_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/cthulhu_44.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/cthulhu_44_qa_pairs.json\u001b[0m\n",
            "\u001b[2KProcessing 7 chunks to generate QA pairs...\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 0 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/cthulhu_251_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/cthulhu_251_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/cthulhu_251.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/cthulhu_251_qa_pairs.json\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d19ece1c0fa8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames_to_do\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'synthetic-data-kit          -c synthetic_data_kit_config.yaml          create {fname}          --num-pairs 25          --type \"qa\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sleep some time to leave some room for processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "for fname in filenames_to_do[:30]:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {fname} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(5) # Sleep some time to leave some room for processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moO4ik-pq3uo"
      },
      "source": [
        "This is old code used to tunnel the vllm server endpoints (which are running on localhost:8000) to a web page in the internet. This is done using localx-colab.\n",
        "\n",
        "This was used to check the status, metrics, etc of the vllm server, in an attempt to debug it. However thee is not endpoint for debug messages, therefore ended up not using this,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1J1zk91_NBt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6JYCstFWYXT"
      },
      "outputs": [],
      "source": [
        "!pip install loclx-colab\n",
        "\n",
        "# SETUP TUNNELING\n",
        "import loclx_colab.loclx as lx\n",
        "port = 8000 # The service port that you want to expose\n",
        "access_token = 'CQFAU5poxrD8CJxVcNBf9Xy1FIPoT2wGkUip4H3Z' # Your LocalXpose token here\n",
        "url = lx.http_tunnel_start(port, access_token)\n",
        "print(f\"Your service is exposed to this URL: https://{url}\")\n",
        "print(f\"List models: https://{url}/v1/models\")\n",
        "print(f\"Metrics: https://{url}/metrics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iY3pesYrUIW"
      },
      "outputs": [],
      "source": [
        "lx.login(access_token)\n",
        "lx.http_tunnel_status()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PzeEy40HokSG",
        "0x6sSPHdFKKD",
        "dkP0IH1NFQy1",
        "2zkaCjSiRkGV",
        "-i_EAR_8FVh-",
        "qqH6hJOsFdtb",
        "XObOXC0thIxW",
        "MqCFCT0LqM_0",
        "UbsCjaw3pESz"
      ],
      "authorship_tag": "ABX9TyOiWuRRM8KtylTkDlcJT5xv"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}