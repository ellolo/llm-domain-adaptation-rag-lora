{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellolo/cthulhu_fine_tuning/blob/main/notebooks/2_lora_qa_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZSki1EXdO1"
      },
      "source": [
        "# LoRA fine-tuning on QA dataset\n",
        "\n",
        "This notebook fine-tunes `Llama-3.1-8B-Instruct` and `Llama-3.2-1B-Instruct` on a domain-specific Question Answering (QA) dataset using [Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685). The goal is to obtain a fine-tuned model that answers user questions on that domain.\n",
        "\n",
        "As an example, I use the Chtulhu game as the target domain, and a dataset of about 20K QA pairs derived from the Chtulhu Rulebook as fine-tuning dataset. The process to derive this dataset is described in `1_synt_dataset_generation.ipynb`. The idea is to use the fine-tuned model while playing the Game of Chtulhu, to answer players' questions regarding the rules of the game.\n",
        "\n",
        "## Stack\n",
        "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) to load and quantize the base models, and to perform fine-tuning\n",
        "- [BitsAndBytes](https://huggingface.co/docs/bitsandbytes/main/en/index) for quantization.\n",
        "- [Hugging Face Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/en/index) to configure the Lora model.\n",
        "- [Hugging Face Hub](https://huggingface.co/docs/huggingface_hub/en/index) to store the Lora models checkpoints.\n",
        "\n",
        "I recommend to use a machine with at least a **A100** Nvidia GPU. Although Lora is designed to save VRAM, a decent GPU is stil needed.\n",
        "\n",
        "## Details\n",
        "At a high-level, I do the following (more details are provided in the rest of the notebook):\n",
        "\n",
        "1. Install dependencies and clone this github repo. I use the repo to read the fine-tuning dataset and also to store performance data of the fine-tuning process.\n",
        "2. Load the base model (either `Llama-3.1-8B-Instruct` or `Llama-3.2-1B-Instruct`) and quantize it to 4-bits to save VRAM. When quantized, these models are small enough to perform Lora fine-tuning on a A100 GPU.\n",
        "3. Setup and configure the Lora model. This means that we create low-dimensional Lora adapters on top of specific layers of the base model, and fine-tune the weights of these adapters, leaving the weights of the original base model untouched, thus saving time and VRAM. I add Lora adapeters to all linear layers of the Llama models (both attention and FFN layers) and experiment with a Lora rank of 16 and 64, fix Lora α to 32, and always activate rLora.\n",
        "4. Load the Chtulhu Rulebook QA pairs dataset. Each example in the dataset is a Question-Answering pair extracted from the Chtulhu Rulebook. More details are provided in the following sections.\n",
        "5. Fine-tune the Lora model. I track the loss as the main metric across a training and evaluation set split of the dataset. I also track and store VRAM snapshots to monitor the memory usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mhhYJyRXqKO"
      },
      "source": [
        "# 1. Install dependencies and clone github repo\n",
        "\n",
        "The github repo contains the dataset for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0nOi29Htu5-S"
      },
      "outputs": [],
      "source": [
        "# install dependecies\n",
        "\n",
        "%%capture\n",
        "! pip install -U 'transformers[torch]' peft datasets bitsandbytes #flash-attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr5QFe5m0OH1"
      },
      "outputs": [],
      "source": [
        "# get HF token from secrets and login in HF hub, so that we can download models from HF Hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "  login(HF_TOKEN)\n",
        "else:\n",
        "  login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhQOorBxhzh-"
      },
      "outputs": [],
      "source": [
        "# Clone github repository.\n",
        "\n",
        "# Past here the SSH key stored on my personal laptopat: ~/dev/llm_cthulhu_fine_tuning/keys\n",
        "# This is not safe, but unfortunaly using Colab secrets did not work.\n",
        "! mkdir -p /root/.ssh\n",
        "with open(\"/root/.ssh/id_rsa\", mode=\"w\") as fp:\n",
        "    fp.write(\"\"\"<YOUR SSH KEY TO THE REPO>\"\"\")\n",
        "\n",
        "# <COPY FROM LOCAL DISK AT: ~/dev/llm_cthulhu_fine_tuning/keys>\n",
        "! ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "! chmod go-rwx /root/.ssh/id_rsa\n",
        "! git clone git@github.com:ellolo/cthulhu_fine_tuning.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1xVR0kyX2b8"
      },
      "source": [
        "# 2. Load quantized pretrained model\n",
        "\n",
        "I experimented with the following base models: `Llama-3.1-8B-Instruct` and  `Llama-3.2-1B-Instruct`. These models are small enough to be finetuned with LoRA on a A100 GPU.\n",
        "It could be interesting to experiment also with base models that are not tuned to chat (e.g. `Llama-3.2-1B`) as proposed in [2], with bigger models, and with models other open-sourced models outside Llama family.\n",
        "\n",
        "The base model is loaded into the GPU with 4-bit BitsAndBytes quantization.  Using quantization saves GPU memory, which allowed me to run LoRA fine-tuning with a 8B model on a single A100 GPU.\n",
        "\n",
        "- **4-bit vs 8-bit quantization** \\\\\n",
        "Two types of quantization are typically used, 8-bit and 4-bit. The latter has slighly lower precision on existing benchmarks, but allows to significantly shrink memory. Using 4-bit instead of 8-bit allowed me to use a bigger model (Llama 8B instead of 3B). This is good, as some empirical evidences [7] show that a 4-bit 8B is more accurate than a a 8-bit 3B model.\n",
        "\n",
        "- **BitsAndBytes** \\\\\n",
        "There are many available approaches to quantize a model, as described in [4, 5]. For PEFT fine-tuning and for working on CUDA/Nvidia, Hugging Face recommends to use BitsAndBytes [9], since it is the most tested for PEFT.\n",
        "\n",
        "- **Flesh attention** \\\\\n",
        "I load the model with flash-attention when the architecture of the GPU allows. Flesh attention should increase memory performance and stability [13, 14, 15].\n",
        "\n",
        "GPU memory used for base models during my experiments, measured with nvidia-smi:\n",
        "\n",
        "| Model | Quantization | VRAM (GB) |\n",
        "|-------|--------------|----------|\n",
        "| Llama-3.2-1B | none | 5.99 |\n",
        "| Llama-3.2-1B | 4-bit | 1.60 |\n",
        "| Llama-3.1-8B | 4-bit | 7.27 |\n",
        "\n",
        "## References\n",
        "\n",
        "Fine-tuning in Hugging Face using PEFT:\n",
        "\n",
        "- [1] https://huggingface.co/docs/transformers/main/en/peft\n",
        "\n",
        "Best base models for LoRA:\n",
        "- [2] https://docs.unsloth.ai/get-started/fine-tuning-guide/what-model-should-i-use\n",
        "\n",
        "Llama 3.2 architecture:\n",
        "- [3] https://www.analyticsvidhya.com/blog/2024/09/llama-3-2-models/\n",
        "\n",
        "Quantization quick intro:\n",
        "- [4] https://huggingface.co/docs/transformers/main/en/quantization/concept_guide\n",
        "- [5] https://huggingface.co/docs/transformers/en/quantization/overview\n",
        "- [6] https://huggingface.co/blog/merve/quantization\n",
        "\n",
        "4-bit vs 8-bit quantization:\n",
        "- [7] https://www.reddit.com/r/LocalLLaMA/comments/13mxq66/13b_4bit_or_7b_8bits/\n",
        "- [8] https://medium.com/@shikharstruck/shrinking-elephants-a-funny-guide-to-4-bit-and-8-bit-quantization-for-llms-with-lora-ddf9f1a62070\n",
        "\n",
        "BitsAndBytes:\n",
        "- [9] https://huggingface.co/docs/transformers/main/en/quantization/selecting\n",
        "- [10] https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "- [11] https://huggingface.co/docs/transformers/en/quantization/bitsandbytes\n",
        "- [12] https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig\n",
        "\n",
        "Flesh attention:\n",
        "- [13] https://huggingface.co/docs/trl/en/sft_trainer#using-flash-attention-and-flash-attention-2\n",
        "- [14] https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html\n",
        "- [15] https://medium.com/data-science-in-your-pocket/what-is-flash-attention-f5dc22522a77\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41AjZ7sYgDV5"
      },
      "outputs": [],
      "source": [
        "# Set base model\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # \"meta-llama/Llama-3.2-1B-Instruct\" # \"meta-llama/Llama-3.2-1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q6QfpXmMPfh"
      },
      "outputs": [],
      "source": [
        "# Configure BitsAndBytes\n",
        "\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                     # weights stored in 4 bits (saves memory)\n",
        "    bnb_4bit_quant_type=\"nf4\",             # format used for storing in 4 bits\n",
        "    bnb_4bit_use_double_quant=True,        # double quantize (saves memory)\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # perform operations in 16bit instead of 32bit (speed up fine-tuning)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uILpfoHzMj3Y"
      },
      "outputs": [],
      "source": [
        "# Load quantized model on GPU\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "SKIP_FLASH_ATTENTION = True # force skipping flash attention\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "mem_start = torch.cuda.memory_allocated(device)\n",
        "\n",
        "# check if we can use flash-attention\n",
        "if torch.cuda.get_device_capability()[0] >= 8 and not SKIP_FLASH_ATTENTION:\n",
        "    print(\"Using flash-attention\")\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "    print(\"Using native attention\")\n",
        "    attn_implementation = \"eager\"\n",
        "\n",
        "# load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    attn_implementation=attn_implementation,\n",
        ").to(device)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "mem_model_load = torch.cuda.memory_allocated(device)\n",
        "mem_used_model = round((mem_model_load - mem_start) / 1024/1024/1024, 4)\n",
        "print(f\"Using device:{device}\")\n",
        "print(f\"Memory used for original model (GB): {mem_used_model}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ms1NKZ1YznC"
      },
      "source": [
        "# 3. Setup LoRA model\n",
        "\n",
        "I experimented with different Llora hyperparameter setups, largely following existing recommendations from the literature [1, 2, 3]:\n",
        "- **r**: 16, 64 \\\\\n",
        "Literature suggests to keep the Llora rank _r_ between 8 and 128. I tried 16 and 64 to verify the hypothesis that a larger _r_ allows for more capacity and hence increases performance. I did not experiment with _r>64_ as this leads to too much memory usage.\n",
        "- **α**: 32 \\\\\n",
        "Literature suggests to set the scaling factor _α_ to _α=r_ or _α=2*r_. A higher scaling factor amplifies the effect of the fine-tuning, i.e. the Llora. A lower value pushed the model to rely mor eon the original parameters.\n",
        "- **rLlora**: active \\\\\n",
        "rLlora takes the square root of _r_ when merging the Llora adapter, instead of _r_, which brings more stability to the fine-tuning process.\n",
        "\n",
        "- **layers with Llora**: all linear layers \\\\\n",
        "Empirical experiment [1, 3, 4] show that best performance are achieved when applying Llora adapters to all linear layers, including both attention and FFN layers. In the transformer architecture there are: `q_proj, v_proj, k_proj, o_proj` (attention) and `gate_proj, , down_proj, up_proj` (FFN).\n",
        "\n",
        "- **batch size**: 16, 64\n",
        "A small batch size allows to save VRAM, but I was able to push to 64 examples on the A100, so to get more stability and faster training time.\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Llora intro:\n",
        "- [1] https://arxiv.org/abs/2106.09685\n",
        "- [2] https://medium.com/@kednaik/llm-fine-tuning-with-lora-8e06f2227183\n",
        "\n",
        "Hyperparameters setting for Llora:\n",
        "- [3] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide\n",
        "- [4] https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/\n",
        "- [5] https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiW7eer4MoDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2ff057-9662-4300-9017-6ced8b26391a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layers: ['q_proj', 'v_proj', 'k_proj', 'gate_proj', 'o_proj', 'down_proj', 'up_proj']\n"
          ]
        }
      ],
      "source": [
        "# Set Llora hyperparameters\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "def get_linear_layers(model, exclude_lm_head = True):\n",
        "    \"\"\"\n",
        "    Returns the short sematic name of all layers that are linear,\n",
        "    (excluding the final head layer if needed).\n",
        "\n",
        "    Args:\n",
        "        model: HF model to get linear layers from.\n",
        "        exclude_lm_head (boolean, optional): True if lm_head layer must be included\n",
        "    Returns:\n",
        "        list: List of linear layers of the model.\n",
        "    \"\"\"\n",
        "    linear_layers = set()\n",
        "    for name, module in model.named_modules():\n",
        "      module_type = str(type(module))\n",
        "      if \"Linear\" in module_type:\n",
        "        linear_layers.add(name.split(\".\")[-1])\n",
        "    if exclude_lm_head and 'lm_head' in linear_layers:\n",
        "      linear_layers.remove(\"lm_head\")\n",
        "    return list(linear_layers)\n",
        "\n",
        "print(f\"Linear layers: {get_linear_layers(model)}\")\n",
        "\n",
        "LORA_R = 64 #16\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,                                 # rank (dim) of the adapters A and B. Higher value means more parameters to train (more expressivity but also more mem). Increasing does not bring substantial increase in performance.\n",
        "    lora_alpha=LORA_ALPHA,                    # magnitude of adapter in changing the orginal params (W' = W + a*(A*B)). Usually a=2*r. Typicall set between 4 and 64. High a leans more to the adapter, low a leans more on original weights. Should be bigger for smaller models.\n",
        "    target_modules= get_linear_layers(model), # to which layers to apply lora adapters. This is the param that influences performance the most. According to QLoRA paper should use adapters to all linear layers to match full finetuning accuracy\n",
        "    lora_dropout=0.0,                         # setting to 0 allows faster training, but risks some overfitting\n",
        "    bias=\"none\",\n",
        "    use_rslora=True,                          # stabilizes adapters\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBjtZQZFAq-_"
      },
      "outputs": [],
      "source": [
        "# Add LoRA adapters to the selected layers of the base model's architecture.\n",
        "\n",
        "from peft import PeftModelForCausalLM\n",
        "\n",
        "model: PeftModelForCausalLM = get_peft_model(model, lora_config)\n",
        "\n",
        "mem_lora_model_load = torch.cuda.memory_allocated(device)\n",
        "mem_used_lora_model = round((mem_lora_model_load - mem_model_load) / 1024/1024/1024, 4)\n",
        "print(f\"Memory used for original model (GB): {mem_used_model}\")\n",
        "print(f\"Memory used for LoRA model (GB): {mem_used_lora_model}\")\n",
        "print(model)\n",
        "print(\"*\"*20)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep00j1zEZ2IE"
      },
      "source": [
        "# 4. Prepare QA dataset\n",
        "\n",
        "I fine-tune the LLM with a dataset of  11,930 Question-Ansering (QA) pairs extracted from the Chtuhlu Rulebook using the [Meta Synthetic Data Kit (MSDK)](https://github.com/meta-llama/synthetic-data-kit/tree/main/synthetic_data_kit). The process to generate the dataset is described in `1_synt_dataset_generation.ipynb`.\n",
        "\n",
        "In practice, each QA pair is modeled as a chat session, where the user asks the question and the LLM answers it. This should teach the LLM to answer questions about the Chtuhlu game.  Here is an example of a QA pair from the dataset, which follows the typical [1, 2] json format:\n",
        "\n",
        "```\n",
        "{\"messages\": [\n",
        "  { \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful AI assistant.\"\n",
        "  },\n",
        "  { \"role\": \"user\",\n",
        "    \"content\": \"What happens when establishing a contact with a non-player character?\"\n",
        "  },\n",
        "  { \"role\": \"assistant\",\n",
        "    \"content\": \"A successful roll means the non-player character has heard of the investigator by reputation or has previously met the contact.\"\n",
        "  }\n",
        "  ]}\n",
        "```\n",
        "\n",
        "Specifically, I do the following:\n",
        "\n",
        "1. Load the Tokenizer of the base model, load the dataset, and format each QA pair into the  model's internal chat template [1]. We ensure that:\n",
        "  - The base model is an **Instruct model**. \\\\\n",
        "This means that the base model is already pretrained to interactively chat with the user. This makes our Llora fine-tuning easier, as we only need to teach the model to respond to domain-specific questions, and not also to learn to chat.\n",
        "  - The Tokenizer class of the model implements the `apply_chat_template()` method, which converts the ChatML json format into the LLM's chat template that was used for pretraining, so that our dataset is consistent to the original pretrained data. If you are not using a model that has a chat template, you need to add new special tokens to the tokenizer's vocabulary [3], which model the chat's roles and contenxt (e.g. Llama 3 uses the following tokens: `<|begin_of_text|>`, `<|enf_of_text|>`, `<|eot_id|>`).\n",
        "\n",
        "2. Tokenize the dataset [4, 5, 6] and remove pairs that exceed `MAX_SEQ_LENGTH` in order to save memory at fine-tuning time.\n",
        "\n",
        "3. Initialize the Data Collator that will generate batches during fine-tuning. \\\\\n",
        "I use the `DataCollatorForLanguageModeling` which performs the following for each batch (see [7] for a complete guide):\n",
        " - **Padding**: finds the example in the batch with the longest 'input_ids'  token sequence _n_. Pads the 'input_ids' token sequence of the other examples in the batch up to _n_, using the tokenizer's pad token. This results in all token sequences in the batch to have the same length.\n",
        " - **Labels**: create a new 'label sequence' for each example in the batch. This is the field used by the Trainer to compute the loss. The label sequence is created as follows. First, it copies the token ids from the 'input_ids' sequence into the label sequence. Then, it replaces pad token ids with -100 so that pad tokens are not used for loss computation. \\\\\n",
        "Note that the label for token $u_i$ should be token $u_i+1$ since we want to predict the _next_ token $u_i+1$ from the context $u_0...u_i$. Therefore we we would expect the Data Collator to not simply 'copy' the input_ids into the label sequence, but to shift the label of one position. In Hugging Face this shifting is actualy done by the trainer itself [8], therefore just copying is ok.\n",
        "\n",
        "\n",
        "\n",
        "### Note about Pad token in Llama.\n",
        "\n",
        "`DataCollatorForLanguageModeling` requires the Tokenizer to have a pad token, or it fails. Since Llama does not have a pad token, I set it manually to one of the unused 'reserved tokens' (see file tokenizer_config.json in Hugging Face). Specificially I used the aptly named `<|finetune_right_pad_id|>` [10].\n",
        "I could have alternaitely used the `eos_token` or a brand new token, but these have the following disadvantages:\n",
        "-  eos_token: this would result in the collator to 'mask' the eos_token (i.e.\n",
        "set all the occurrences of eos_token, also those that actually mean 'end of sentence') to -100. This may result in the model never to emit eos_token, end therefore endlessly generate [11, 12].\n",
        "- new token: this would change the token vocabulary size. We would therefore need to resize the embedding layers, which is inconvenient and may not work with quantized models [13, 14].\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Chat templates and ChatML\n",
        "- [1] https://huggingface.co/docs/transformers/en/chat_templating\n",
        "- [2] https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md\n",
        "- [3] https://huggingface.co/docs/trl/en_sft_trainer#add-special-tokens-for-chat-format\n",
        "\n",
        "Tokenization in Hugging Face:\n",
        "- [4] https://huggingface.co/learn/llm-course/chapter2/4?fw=pt\n",
        "- [5] https://huggingface.co/learn/llm-course/chapter3/2?fw=pt\n",
        "- [6] https://huggingface.co/learn/llm-course/chapter2/5?fw=pt#attention-masks\n",
        "\n",
        "Data Collators:\n",
        "- [7] https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2/\n",
        "- [8] https://huggingface.co/learn/llm-course/en/chapter7/6#initializing-a-new-model\n",
        "- [9] https://discuss.huggingface.co/t/how-labelled-data-is-processed-transformers-trainer/80644/6\n",
        "\n",
        "Pad token in Llama:\n",
        "- [10] http://github.com/unslothai/unsloth/issues/416\n",
        "- [11] https://github.com/huggingface/transformers/issues/25773\n",
        "- [12] https://github.com/huggingface/transformers/issues/22794\n",
        "- [13] https://medium.com/@coldstart_coder/adding-custom-tokens-to-huggingface-models-1981f114efc1\n",
        "- [14] https://www.youtube.com/watch?v=MB0ZJ5Y-07s           \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmFBAqG67Nmr"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and ensure we use an Instruct model\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "try:\n",
        "  tokenizer.apply_chat_template([{\"role\":\"system\", \"content\":\"try\"}])\n",
        "except ValueError:\n",
        "  raise ValueError(\"You are using a model that is not an 'instruct' (i.e. chat) model. Please:\\n 1. replace function apply_chat_template() with your own function to format examples into chatML \\n 2. specify chatml special tokens by hand as described in: https://huggingface.co/docs/trl/en/sft_trainer#add-special-tokens-for-chat-format\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yzEPaQ6iKV-"
      },
      "outputs": [],
      "source": [
        "# Loads the dataset from the Cthulhu guidebook's QA pairs.\n",
        "\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning\n",
        "\n",
        "def generate_examples(dir: str, tokenizer, last_index) -> str:\n",
        "  \"\"\"\n",
        "  Generator function to load QA pairs from JSON files and format them into ChatML.\n",
        "\n",
        "  Args:\n",
        "      dir (str): The directory containing the JSON files.\n",
        "      tokenizer: The tokenizer with an `apply_chat_template` method.\n",
        "      last_index: The index of the last file to include in the dataset.\n",
        "\n",
        "  Yields:\n",
        "      dict: A dictionary containing 'file_index' and 'chatml' for each example,\n",
        "      where 'file_index' is the index of the file the example is extracted from,\n",
        "      and 'chatml' is the actual conversation in the chatML format of the specific\n",
        "      LLM model.\n",
        "  \"\"\"\n",
        "  fnames = glob.glob(f\"{dir}/*.json\")\n",
        "  for fname in fnames:\n",
        "    file_index = int(Path(fname).name.split(\"_\")[1])\n",
        "    with open(fname) as f:\n",
        "      if last_index and file_index > last_index:\n",
        "        print(f\"Skipping file: {fname}\")\n",
        "        continue\n",
        "      for line in f:\n",
        "        messages = json.loads(line)[\"messages\"]\n",
        "        chatml = tokenizer.apply_chat_template( # will fail if the model is not an instruct model\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        yield {\"file_index\":file_index, \"chatml\": chatml}\n",
        "\n",
        "\n",
        "#DATASET_LAST_FILE_INDEX=159\n",
        "DATASET_LAST_FILE_INDEX=None\n",
        "dataset_path = \"data/final_cleaned\"\n",
        "\n",
        "dataset = Dataset.from_generator(\n",
        "    generate_examples,\n",
        "    gen_kwargs={\n",
        "        \"dir\": dataset_path,\n",
        "        \"tokenizer\": tokenizer,\n",
        "        \"last_index\": DATASET_LAST_FILE_INDEX\n",
        "        },\n",
        "    split=\"train\"\n",
        "    )\n",
        "\n",
        "# print first QA pair of the training dataset in internal ChatML format\n",
        "print(dataset[0])\n",
        "\n",
        "## sort according to original document\n",
        "#dataset = dataset.sort(\"file_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP8zv1Ll_xFj"
      },
      "outputs": [],
      "source": [
        "# Tokenize dataset, remove long examples, split into training/testing.\n",
        "\n",
        "from itertools import compress\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "def get_model_max_seq_length(model: AutoModelForCausalLM):\n",
        "  \"\"\"\n",
        "  Determines the maximum sequence length supported by the model.\n",
        "\n",
        "  Args:\n",
        "      model (AutoModelForCausalLM): The Hugging Face causal language model.\n",
        "\n",
        "  Returns:\n",
        "      int: The maximum sequence length. Defaults to 1024 if not found in model config.\n",
        "  \"\"\"\n",
        "  max_context = None\n",
        "  for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "      max_context = getattr(model.config, length_setting, None)\n",
        "      if max_context:\n",
        "          print(f\"Found model max context: {max_context}\")\n",
        "          break\n",
        "  if not max_context:\n",
        "      max_context = 1024\n",
        "      print(f\"Using default max context: {max_context}\")\n",
        "  return max_context\n",
        "\n",
        "\n",
        "def tokenize(dataset, tokenizer: AutoTokenizer):\n",
        "  \"\"\"\n",
        "  Tokenizes the dataset using the provided tokenizer.\n",
        "\n",
        "  Args:\n",
        "      dataset: The dataset to tokenize.\n",
        "      tokenizer (AutoTokenizer): The tokenizer to use.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing the tokenized data.\n",
        "  \"\"\"\n",
        "  return tokenizer(\n",
        "      dataset[\"chatml\"],\n",
        "      padding=False, # padding will be done by the DataCollector\n",
        "      truncation=True,\n",
        "      )\n",
        "\n",
        "\n",
        "# max length of the context that we allow (keeps mem in check)\n",
        "MAX_SEQ_LENGTH = 256\n",
        "\n",
        "# tokenize\n",
        "dataset_tokenized = dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=[\"file_index\", \"chatml\"],\n",
        "    batched=True,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "  )\n",
        "\n",
        "# remove examples that are too long\n",
        "length_bf = len(dataset_tokenized)\n",
        "max_seq_length = min(MAX_SEQ_LENGTH, get_model_max_seq_length(model))\n",
        "dataset_tokenized = dataset_tokenized.filter(lambda x: len(x[\"input_ids\"]) <= max_seq_length)\n",
        "\n",
        "# split into train and eval sets\n",
        "dataset_tokenized = dataset_tokenized.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
        "\n",
        "# stats\n",
        "print(\"*\"*40)\n",
        "print(f\"Dataset size before truncating at {max_seq_length}: {length_bf}\")\n",
        "print(f\"Dataset size after truncating at {max_seq_length}: {len(dataset_tokenized)}\")\n",
        "print(f\"Examples in training set: {len(dataset_tokenized['train'])}\")\n",
        "print(f\"Examples in test set: {len(dataset_tokenized['test'])}\")\n",
        "print(\"*\"*40)\n",
        "\n",
        "print(f\"\\n{dataset_tokenized}\")\n",
        "print(\"*\"*20)\n",
        "print(\"tokenized example:\")\n",
        "print(dataset_tokenized[\"train\"][0])\n",
        "print(\"*\"*20)\n",
        "print(\"decoded tokenized example:\")\n",
        "print(tokenizer.decode(dataset_tokenized[\"train\"][0][\"input_ids\"]))\n",
        "print(\"*\"*20)\n",
        "print(\"Decoded tokenized example (only tokens that are not masked):\")\n",
        "print(tokenizer.decode(list(compress(dataset_tokenized[\"train\"][0][\"input_ids\"], dataset_tokenized[\"train\"][0][\"attention_mask\"]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP1iJ9FMC_8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6951bfc-2814-4943-e7d6-184014ea969d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max context length in dataset: 217\n",
            "min context length in dataset: 49\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<BarContainer object of 84 artists>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot distribution of context length across the dataset\n",
        "\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "context_lengths = defaultdict(int)\n",
        "for example in dataset_tokenized[\"train\"]:\n",
        "  context_lengths[len(example[\"input_ids\"])] += 1\n",
        "\n",
        "print(f\"max context length in dataset: {max(context_lengths.keys())}\")\n",
        "print(f\"min context length in dataset: {min(context_lengths.keys())}\")\n",
        "display(plt.bar(context_lengths.keys(), context_lengths.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riAXRS0Vju1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871af023-8a40-41df-818a-1428a0447f58",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Tokenizer vocab size before adding pad token: 128256\n",
            "Tokenizer vocab size after adding pad token: 128256\n",
            "Tokenizer pad token: <|finetune_right_pad_id|>\n",
            "Tokenizer pad token id: 128004\n",
            "Model pad token id: 128004\n",
            "****************************************\n"
          ]
        }
      ],
      "source": [
        "# set pad token for Llama tokenizer\n",
        "\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<|finetune_right_pad_id|>\"})\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "print(\"*\"*40)\n",
        "print(f\"Tokenizer vocab size before adding pad token: {len(tokenizer)}\")\n",
        "print(f\"Tokenizer vocab size after adding pad token: {len(tokenizer)}\")\n",
        "print('Tokenizer pad token:', tokenizer.pad_token)\n",
        "print('Tokenizer pad token id:', tokenizer.pad_token_id)\n",
        "print('Model pad token id:', model.config.pad_token_id)\n",
        "print(\"*\"*40)\n",
        "\n",
        "\n",
        "# inizialize data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCiul5PynkV3"
      },
      "outputs": [],
      "source": [
        "# let's check that the data collator produces output as we expect\n",
        "\n",
        "import torch\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_tokenized[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=2\n",
        "    )\n",
        "\n",
        "def occurrence_token_id(tensor, token_id):\n",
        "  mask = token_id == tensor\n",
        "  return mask.sum().item()\n",
        "\n",
        "i = 0\n",
        "for batch in dataloader:\n",
        "      print()\n",
        "      print(f\"Sequences length: {len(batch['input_ids'][0])}\")\n",
        "      print(f\"Padding tokens in sequence 1: {occurrence_token_id(batch['input_ids'][0], tokenizer.pad_token_id)}\")\n",
        "      print(f\"Padding tokens in sequence 2: {occurrence_token_id(batch['input_ids'][1], tokenizer.pad_token_id)}\")\n",
        "      print(f\"Sequence 1 decoded:\\n {tokenizer.decode(batch['input_ids'][0])}\")\n",
        "      print(f\"\\nSequence 2 decoded:\\n {tokenizer.decode(batch['input_ids'][1])}\")\n",
        "      print(f\"Actual input to Trainer:\")\n",
        "      print(batch)\n",
        "      print(\"*\"*50)\n",
        "      if i == 2:\n",
        "        break\n",
        "      i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytMI7s3xcvuy"
      },
      "source": [
        "# 5. Execute fine-tuning\n",
        "\n",
        "Now that the dataset is ready, that the base model is loaded and that the Llora hyper-parameters are set, we can execute the actual fine-tuning. I did the following:\n",
        "\n",
        "1. If the fine-tuning process was already started in a previous session, download the corresponding checkpoints from the Hugging Face Hub to the local file system.\n",
        "\n",
        "2. Set fine-tuning parameters:.\n",
        "  -  **Learning rate** [1, 2, 3]: I experimented with 1e-4, and reduced to 3e-5 if training was unstable. Typically, when increasing LoRA α, the learning rate should decrease accordingly. \\\\\n",
        "    I used a simple Linear Scheduler [4], which warms up the learning rate until `warmup_ratio` steps, then linearly decreases it.\n",
        "  - **Epochs** [5]: I set this to 3 and checked covergence during the learning process.\n",
        "  - **Batch size** [6]: I set the batch size as big as possible based on VRAM availability. I typically used 16, but pushed to 64 when memory allowed it.\n",
        "\n",
        "3. Execute fine-tuning.\n",
        "\n",
        "4. Save the following:\n",
        "  - to Hugging Face Hub => best model and last checkpoint model. I use my personal HF namespace `mpenna77`\n",
        "  - to Github => PyTorch memory profiling files [10]\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Learning rate for Llora:\n",
        "- [1] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#learning-rate\n",
        "- [2] https://www.determined.ai/blog/lora-parameters\n",
        "- [3] https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2?_gl=1*1xhtjl9*_gcl_au*ODk3ODg5MjE3LjE3NDc5MDcwODY.#sensitivity-of-lora-to-learning-rate\n",
        "- [4] https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup\n",
        "\n",
        "Epochs for Llora:\n",
        "- [5] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#epochs\n",
        "\n",
        "Batch size for Llora:\n",
        "- [6] https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#effective-batch-size\n",
        "\n",
        "Use PEFT and Trainer in Hugging Face:\n",
        "- [7] https://huggingface.co/docs/peft/quicktour\n",
        "- [8] https://huggingface.co/docs/transformers/v4.52.2/en/trainer\n",
        "- [9] https://huggingface.co/learn/llm-course/chapter3/2?fw=pt\n",
        "\n",
        "Memory profiling in PyTorch:\n",
        "- [10] https://zdevito.github.io/2022/12/09/memory-traces.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model checkpoints from Hugging Face Hub to local file system.\n",
        "# This is only needed to resume fine-tuning from a pre-existing checkpoint\n",
        "\n",
        "#from huggingface_hub import HfApi\n",
        "\n",
        "#%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "\n",
        "#namespace=\"mpenna77\"\n",
        "\n",
        "#api = HfApi(token=userdata.get('HF_TOKEN_WRITE')) # read HF token from Colab secrets\n",
        "#api.snapshot_download(\n",
        "#    \"mpenna77/Llama-3.1-8B-Instruct-lr0.0001-b16-r64-a32-lora-cthulhu\",\n",
        "#    local_dir=\"../Llama-3.1-8B-Instruct-lr0.0001-b16-r64-a32-lora-cthulhu\")\n"
      ],
      "metadata": {
        "id": "eLBh1H8HOB-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqqM5882Ca-n"
      },
      "outputs": [],
      "source": [
        "# Execute fine-tuning\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback, DataCollatorWithPadding\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning\n",
        "\n",
        "LR = 1e-4\n",
        "#BATCH_SIZE = 64\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "run_name = f\"{model_name.split('/')[-1]}-lr{LR}-b{BATCH_SIZE}-r{LORA_R}-a{LORA_ALPHA}-lora-cthulhu\"\n",
        "if DATASET_LAST_FILE_INDEX:\n",
        "  run_name += f\"-cut{DATASET_LAST_FILE_INDEX}\"\n",
        "out_dir = f\"fine-tuning/models/{run_name}\"\n",
        "print(run_name)\n",
        "\n",
        "Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    learning_rate=LR,                        # start with 1e-4 and reduce to 3e-5 if training is unstable. If increasing LoRA apha, learning rate should decrease accordingly.\n",
        "    gradient_accumulation_steps = 1,         # after how many steps (batches) to sum up the gradient and then update params\n",
        "    lr_scheduler_type = \"linear\",            # learning rate is warmed up linearly until warmup_ratio steps, then linearly decreases\n",
        "    warmup_ratio = 0.05,                     # first x% of training steps will be used to slowly ramp-up learning rate (default is 0.1). Makes training more stable.\n",
        "    weight_decay=0.3,                        # keep between 1.0 and 0.3\n",
        "    per_device_train_batch_size=BATCH_SIZE,  # depends on VRAM availability\n",
        "    per_device_eval_batch_size=BATCH_SIZE,   # depends on VRAM availability\n",
        "    num_train_epochs=3,                      # 3 should be sufficient. More could overfit and decrease creativity.\n",
        "    eval_strategy=\"steps\",                   # set to \"epoch\" if running evaluation every epoch, or \"step\" if at every batch\n",
        "    save_strategy=\"steps\",                   # set to \"epoch\" if storing checkpoints every epoch, or \"step\" if at every batch\n",
        "    save_steps=250,\n",
        "    eval_steps=250,\n",
        "    logging_steps=25,\n",
        "    load_best_model_at_end=True,     # returns the best checkpointed model at the end of training, determined as the checkpoint with smallest loss on eval dataset\n",
        "    report_to=\"none\",                # Disable 'Weights and Biases' reporting\n",
        "    label_names=[\"labels\"],          # see: https://github.com/unslothai/unsloth/issues/1788\n",
        "    #max_steps = 20,\n",
        ")\n",
        "\n",
        "time_start_training = time.perf_counter()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_tokenized[\"train\"],\n",
        "    eval_dataset=dataset_tokenized[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# start capturing VRAM snapshots\n",
        "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
        "\n",
        "# fine-tune from scratch\n",
        "results = trainer.train()\n",
        "## fine-tune resuming from a pre-existing checkpoint\n",
        "#results = trainer.train(resume_from_checkpoint=\"/content/Llama-3.1-8B-Instruct-lr0.0001-b16-r64-a32-lora-cthulhu/checkpoint-1000\")\n",
        "\n",
        "# end capturing VRAM snapshots\n",
        "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
        "\n",
        "time_end_training = time.perf_counter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8skVN5xstaz4"
      },
      "outputs": [],
      "source": [
        "# Saving model checkpoints to Hugging Face Hub\n",
        "#\n",
        "# TODO: This could be done easier by just setting the option in the Trainer.\n",
        "#       See Notebook 3_lora_cont_pretraining.ipynb for how to do this.\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "\n",
        "def upload_to_hub(api, hub_repo_id, model_local_path, peft_checkpoint):\n",
        "  \"\"\"\n",
        "  Uploads a model checkpoint to the Hugging Face Hub.\n",
        "\n",
        "  Args:\n",
        "      api (HfApi): The Hugging Face API object.\n",
        "      hub_repo_id (str): The ID of the repository on the Hugging Face Hub.\n",
        "      model_local_path (str): The local path to the model.\n",
        "      peft_checkpoint (str): The name of the PEFT checkpoint to upload.\n",
        "  \"\"\"\n",
        "  api.create_repo(repo_id=f\"{namespace}/{hub_repo_id}\", private=True, exist_ok=True)\n",
        "  api.upload_folder(\n",
        "      folder_path=f\"{model_local_path}/{peft_checkpoint}\",\n",
        "      repo_id=f\"{namespace}/{hub_repo_id}\",\n",
        "      path_in_repo=peft_checkpoint,\n",
        "      repo_type=\"model\",\n",
        "  )\n",
        "\n",
        "namespace=\"mpenna77\"\n",
        "out_dir = f\"fine-tuning/models/{run_name}\"\n",
        "\n",
        "api = HfApi(token=userdata.get('HF_TOKEN_WRITE')) # read HF token from Colab secrets\n",
        "\n",
        "#upload best model\n",
        "upload_to_hub(api, run_name, out_dir, \"best-model\")\n",
        "\n",
        "#upload last model\n",
        "upload_to_hub(api, run_name, out_dir, f\"checkpoint-{trainer.state.max_steps}\")\n",
        "\n",
        "## upload other checkpoints\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-250\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-500\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-750\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-1000\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-1250\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-1500\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-1750\")\n",
        "#upload_to_hub(api, run_name, out_dir, f\"checkpoint-2000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG1kG9zo_X2n",
        "outputId": "d6c50835-6065-4dac-a455-891e101a63d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama-3.1-8B-Instruct-lr0.0001-b16-r64-a32-lora-cthulhu\n"
          ]
        }
      ],
      "source": [
        "print(run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w7_rjs3yU13"
      },
      "outputs": [],
      "source": [
        "# Saving VRAM snapshots to Github\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning\n",
        "\n",
        "profiling_dir = f\"fine-tuning/profiling/{run_name}\"\n",
        "profiling_file = f\"{profiling_dir}/profile.pkl\"\n",
        "profiling_viz_file = f\"{profiling_dir}/trace.html\"\n",
        "\n",
        "# saving memory profile file to local file system\n",
        "Path(profiling_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "torch.cuda.memory._dump_snapshot(profiling_file)\n",
        "torch.cuda.memory._record_memory_history(enabled=None)\n",
        "\n",
        "# generating memory profile visualuzation files in local file system\n",
        "! wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/cuda/_memory_viz.py\n",
        "! python _memory_viz.py trace_plot {profiling_file} -o {profiling_viz_file}\n",
        "\n",
        "\n",
        "# do the following the first time to push safetensors to github lfs\n",
        "##! git lfs install\n",
        "##! git lfs track \"*safetensors\"\n",
        "##! git add .gitattributes\n",
        "\n",
        "# pushing profiling files to github\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add {profiling_dir}\n",
        "! git commit -m \"added new profiling files\"\n",
        "! git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR7cTKm1TLoC",
        "outputId": "49dea509-b4dd-4b61-9087-8f96f67fc074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training time: 2490.6 s\n",
            "Memory used for original model (GB): 7.2716\n",
            "Memory used for LoRA model (GB): 0.1562\n",
            "Memory used for training (GB): 5.2251\n"
          ]
        }
      ],
      "source": [
        "# Print VRAM memory usage\n",
        "\n",
        "import torch\n",
        "\n",
        "mem_training = torch.cuda.max_memory_allocated(device)\n",
        "mem_used_training = round((mem_training - mem_lora_model_load) / 1024/1024/1024, 4)\n",
        "\n",
        "\n",
        "print(f\"training time: {(time_end_training - time_start_training):.1f} s\")\n",
        "print(f\"Memory used for original model (GB): {mem_used_model}\")\n",
        "print(f\"Memory used for LoRA model (GB): {mem_used_lora_model}\")\n",
        "print(f\"Memory used for training (GB): {mem_used_training}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHEwOBX4eMVT"
      },
      "source": [
        "## Sample inference\n",
        "\n",
        "- Load best fine-tuned model checkpoint\n",
        "- Perform inference on sample user question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7SjTSloL7oQ"
      },
      "outputs": [],
      "source": [
        "# TEST BEST MODEL\n",
        "#\n",
        "# Let's load the best model and do some generation.\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "tmp_model = AutoModelForCausalLM.from_pretrained(f\"{out_dir}/best-model\").to(device)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How does a player fight an enemy?\"},\n",
        "    #{\"role\": \"user\", \"content\": \"Who invented Cthulhu?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(device)\n",
        "\n",
        "print(\"*\"*20)\n",
        "print(\"Tokenized input:\")\n",
        "print(tokenizer.decode(inputs[0]).strip())\n",
        "print(\"*\"*20)\n",
        "print(\"\\nAnswer:\")\n",
        "\n",
        "_ = tmp_model.generate(\n",
        "    input_ids=inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "    max_new_tokens = 256,\n",
        "    temperature = 0.1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMUdX4k-0uiz"
      },
      "source": [
        "# Utils to check GPU status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9UKbFI-HDdK"
      },
      "outputs": [],
      "source": [
        "!  nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLYOJ8SbIe-A"
      },
      "outputs": [],
      "source": [
        "! fuser -v /dev/nvidia*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ywm3q8PIkYb"
      },
      "outputs": [],
      "source": [
        "! sudo kill -9 9203"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqoDpdIMMlQ2"
      },
      "source": [
        "# Old code (deprecated)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtMLK2Ubum3h"
      },
      "outputs": [],
      "source": [
        "# SAVING TO GITHUB: BEST MODEL, LAST MODEL, PROFILING FILES\n",
        "\n",
        "%cd /content/cthulhu_fine_tuning/\n",
        "\n",
        "print(f\"Best model at step: {trainer.state.best_global_step}\")\n",
        "print(f\"Best model checkpoint: {trainer.state.best_model_checkpoint}\")\n",
        "print(f\"Metric at best step (on eval set): {trainer.state.best_metric}\")\n",
        "\n",
        "best_model_dir = f\"{out_dir}/best-model\"\n",
        "best_model_state_file = f\"{out_dir}/best-model-trainer_state.json\"\n",
        "last_model_dir = f\"{out_dir}/checkpoint-{trainer.state.max_steps}\"\n",
        "\n",
        "trainer.save_model(best_model_dir)\n",
        "trainer.state.save_to_json(best_model_state_file)\n",
        "\n",
        "#! git lfs install\n",
        "#! git lfs track \"*safetensors\"\n",
        "#! git add .gitattributes\n",
        "! git config --global user.email \"marco.pennacchiotti@gmail.com\"\n",
        "! git add {best_model_dir}\n",
        "! git add {best_model_state_file}\n",
        "! git add {last_model_dir}/*json\n",
        "! git add {last_model_dir}/*md\n",
        "! git add {last_model_dir}/*safetensors\n",
        "! git add {last_model_dir}/*jinja\n",
        "! git add {profiling_dir}\n",
        "! git commit -m \"added new model\"\n",
        "! git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNHsL7G4ufRU"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "def upload_to_hub(api, hub_repo_id, model_local_path, checkpoint):\n",
        "  api.create_repo(repo_id=f\"mpenna77/{hub_repo_id}\", private=True, exist_ok=True)\n",
        "  api.upload_folder(\n",
        "      folder_path=f\"{model_local_path}/{checkpoint}\",\n",
        "      repo_id=f\"mpenna77/{hub_repo_id}\",\n",
        "      path_in_repo=checkpoint,\n",
        "      repo_type=\"model\",\n",
        "  )\n",
        "\n",
        "api = HfApi(token=userdata.get('HF_TOKEN_WRITE'))\n",
        "\n",
        "upload_to_hub(api, run_name, out_dir, checkpoint)\n",
        "\n",
        "#hub_model_name_last = f\"{run_name}-checkpoint-{trainer.state.max_steps}\"\n",
        "#model_dir_last = f\"{out_dir}/checkpoint-{trainer.state.max_steps}\"\n",
        "#upload_to_hub(api, hub_model_name_last, model_dir_last)\n",
        "\n",
        "#hub_model_name_best = f\"{run_name}-best-model\"\n",
        "#model_dir_best = f\"{out_dir}/best-model\"\n",
        "#upload_to_hub(api, hub_model_name_best, model_dir_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWRj9FrC25Cb"
      },
      "source": [
        "## Torch Cuda Profiler\n",
        "\n",
        "This was code used to print a profile of the memory but unfortunately it breaks with LoRa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVEp4wfM2gqV"
      },
      "outputs": [],
      "source": [
        "# TRAINING WITH PROFILER\n",
        "# Profiling with torch.profiler:\n",
        "# - https://huggingface.co/docs/accelerate/usage_guides/profiler\n",
        "# - https://zdevito.github.io/2022/12/09/memory-traces.html\n",
        "# - https://discuss.pytorch.org/t/is-there-a-pytorch-profiler-integration-with-huggingface-trainer/160497/2\n",
        "\n",
        "\n",
        "# class to log memory state at each training step\n",
        "class ProfCallback(TrainerCallback):\n",
        "    def __init__(self, prof):\n",
        "        self.prof = prof\n",
        "\n",
        "    # will execute at every step (i.e. batch) of training\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        self.prof.step()\n",
        "\n",
        "results = None\n",
        "with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU,\n",
        "                                        torch.profiler.ProfilerActivity.CUDA],\n",
        "                            on_trace_ready=torch.profiler.tensorboard_trace_handler('tmp-hf-training-trainer'),\n",
        "                            schedule=torch.profiler.schedule(skip_first=1, wait=5, warmup=1, active=3, repeat=2),\n",
        "                            profile_memory=True,\n",
        "                            with_stack=True,\n",
        "                            record_shapes=True) as prof:\n",
        "\n",
        "    trainer.add_callback(ProfCallback(prof=prof))\n",
        "    results = trainer.train()\n",
        "\n",
        "\n",
        "# SAVING FILE\n",
        "\n",
        "profiling_file = f\"fine-tuning/profiling/{run_name}/output.html\"\n",
        "Path(profiling_file).parent.mkdir(exist_ok=True, parents=True)\n",
        "with open(profiling_file, \"w\") as f:\n",
        "  f.write(torch.cuda._memory_viz.profile_plot(prof))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjHFJFxUW2sc"
      },
      "source": [
        "## Basic tokenizer and model usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjHT4QxS23Rd"
      },
      "source": [
        "This code was used to experiment with the transformer library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqwsYOzgwoue"
      },
      "outputs": [],
      "source": [
        "# loading model and its tokenizer\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model_causal = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74EIoBTE_OtY"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer(\"Today it is a beatiful day in\", return_tensors=\"pt\")\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_VX0iGNEqXU"
      },
      "outputs": [],
      "source": [
        "model_output = model.forward(**input_ids)\n",
        "model_output[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc20BI-dMOQ7"
      },
      "outputs": [],
      "source": [
        "model_causal_output = model_causal.forward(**input_ids)\n",
        "model_causal_output[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBtaad_8GCUG"
      },
      "outputs": [],
      "source": [
        "model_causal_generated = model_causal.generate(**input_ids, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjtoczfrGqTJ"
      },
      "outputs": [],
      "source": [
        "model_causal_generated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43hVzzLlG4SE"
      },
      "outputs": [],
      "source": [
        "model_causal_generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iPUN5SQG7JQ"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(model_causal_generated[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foK8JoJsE6x_"
      },
      "source": [
        "## DataCollator and padding\n",
        "\n",
        "this code shows how the DataCollator (i.e. the processor that provides batches to the Trainer during training or fine-tuning), not only creates batches of a given size from the dataset, but also ensures that each example in the batch has the same sequence length by padding with the pad token of the tokenizer.\n",
        "See: https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2/\n",
        "\n",
        "Note also the different behaviour of `DataCollatorForLanguageModeling` and `DataCollatorForSeq2Seq`.\n",
        "\n",
        "The `DataCollatorForLanguageModeling` automatically creates the **Label** field to pass to the Trainer.\n",
        "'Labels' are used by the loss function to compute the loss.\\\n",
        "Specifically, at training time, in the forward pass, for a specific token in the sequence (say in position K), the Trainer gets a prob distribution over the token vocabulary. It then takes the token with highest probability (say 12344) and compares it with the ground truth token at position K in the Label field (say the token id is 23433) and computes the loss accordingly.\\\n",
        "Note that when a Label has -100 as a value, that token position wont be considered in computing the loss. This is indeed the case when doing padding: when a token is a padding token, `DataCollatorForLanguageModeling` set its label to -100 and its attention mask to 0, because we do not want to consider pad tokens in computing the loss, as they are not meaningful for the task. See the example below to see this.\n",
        "\n",
        "Summarizing `DataCollatorForLanguageModeling` does the following:\n",
        "- Gets all examples in the batch\n",
        "- Computes the longest token sequence of all examples. Pads all other example to the right up to the longest token sequence, using the padding token. Now all examples have the same token sequence length, which can be used in the NN.\n",
        "- Creates automatically a `Label` field, which is actually a copy of the tokens (i.e. input_ids). However, it replaces padding tokens with -100, so that this is ignored when computing the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxM8ZmD8E4p8"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def tokenize(dataset, tokenizer: AutoTokenizer):\n",
        "  \"\"\"\n",
        "  Tokenizes the dataset. If the number of tokens of an example exceed the max\n",
        "  context length, the example will be truncated.\n",
        "  \"\"\"\n",
        "  return tokenizer(\n",
        "      dataset[\"chatml\"],\n",
        "      truncation=True,\n",
        "      padding=False,\n",
        "      )\n",
        "\n",
        "\n",
        "tmp_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "tmp_dataset_tokenized = dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=[\"file_index\", \"chatml\"],\n",
        "    batched=True,\n",
        "    fn_kwargs={\"tokenizer\": tmp_tokenizer},\n",
        "  )\n",
        "\n",
        "collate_fn = DataCollatorForLanguageModeling(tokenizer=tmp_tokenizer, mlm=False)\n",
        "dataloader = torch.utils.data.DataLoader(tmp_dataset_tokenized[\"train\"], collate_fn=collate_fn, batch_size=2)\n",
        "\n",
        "i = 0\n",
        "for batch in dataloader:\n",
        "      print(f\"Tensors length: {len(batch['input_ids'][0])}\")\n",
        "      print(f\"Non zero elements in sentence 1: {len(batch['input_ids'][0].nonzero())}\")\n",
        "      print(f\"Non zero elements in sentence 2: {len(batch['input_ids'][1].nonzero())}\")\n",
        "      print(f\"Zero elements in sentence 1: {len(batch['input_ids'][0]) - len(batch['input_ids'][0].nonzero())}\")\n",
        "      print(f\"Zero elements in sentence 2: {len(batch['input_ids'][1]) - len(batch['input_ids'][1].nonzero())}\")\n",
        "      print(batch)\n",
        "      if i == 2:\n",
        "        break\n",
        "      i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9dpcz7hce2y"
      },
      "source": [
        "Let's now see what `DataCollatorForSeq2Seq` does.\n",
        "It does as the other DataCollator, but does not create the Label field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeHDPHj1dnJl"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "collate_fn = DataCollatorForSeq2Seq(tokenizer=tmp_tokenizer)\n",
        "dataloader = torch.utils.data.DataLoader(tmp_dataset_tokenized[\"train\"], collate_fn=collate_fn, batch_size=2)\n",
        "\n",
        "i = 0\n",
        "for batch in dataloader:\n",
        "      print(f\"Tensors length: {len(batch['input_ids'][0])}\")\n",
        "      print(f\"Non zero elements in sentence 1: {len(batch['input_ids'][0].nonzero())}\")\n",
        "      print(f\"Non zero elements in sentence 2: {len(batch['input_ids'][1].nonzero())}\")\n",
        "      print(f\"Zero elements in sentence 1: {len(batch['input_ids'][0]) - len(batch['input_ids'][0].nonzero())}\")\n",
        "      print(f\"Zero elements in sentence 2: {len(batch['input_ids'][1]) - len(batch['input_ids'][1].nonzero())}\")\n",
        "      print(batch)\n",
        "      if i == 2:\n",
        "        break\n",
        "      i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUjXYkWTQKu4"
      },
      "source": [
        "This example is taken from from the \"in-context\" QA tutorial from HF (https://huggingface.co/docs/transformers/tasks/question_answering, https://huggingface.co/learn/llm-course/chapter7/7). This is not a generative task, but a classification task: the model has to identify in which position of the text context that is given, is the answer to a specific question.\\\n",
        "Each example containts a context text, a question and the correct answer, which is contained in the context. The task for the LLM is to learn to answer the question by finding the answer in the context and generate as is, in its answer.\n",
        "Fundamentally one passes as trainable tokens (i.e. not masked) the follwoing \"[CLS]question [SEP] context [SEP]\". These \"trainable tokens\" are those that will be passed to the QA loss function to compute the correct answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zSmSifGKSNs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
        "squad = squad.train_test_split(test_size=0.2)\n",
        "\n",
        "tmp2_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tmp2_tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co6lAzFkK29l"
      },
      "outputs": [],
      "source": [
        "from itertools import compress\n",
        "\n",
        "example = squad[\"train\"][0]\n",
        "tokenized_example = tokenized_squad[\"train\"][0]\n",
        "\n",
        "print(\"Example:\")\n",
        "print(example)\n",
        "print(\"*\"*20)\n",
        "print(\"Tokenized example:\")\n",
        "print(tokenized_example)\n",
        "print(\"*\"*20)\n",
        "print(\"Decoded tokenized example:\")\n",
        "print(tmp2_tokenizer.decode(tokenized_example[\"input_ids\"]))\n",
        "print(\"*\"*20)\n",
        "print(\"Decoded tokenized example (only tokens between start_positions and end_positions):\")\n",
        "print(tmp2_tokenizer.decode(tokenized_example[\"input_ids\"][tokenized_example[\"start_positions\"]:tokenized_example[\"end_positions\"]]))\n",
        "print(\"*\"*20)\n",
        "print(\"Decoded tokenized example (only tokens that are not masked):\")\n",
        "print(tmp2_tokenizer.decode(list(compress(tokenized_example[\"input_ids\"], tokenized_example[\"attention_mask\"]))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FhV2wUpW7Xj"
      },
      "source": [
        "## Chat templates\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY9hAM2GXAlr"
      },
      "source": [
        "Few shot example using Llama 3.1 chat model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThBaLbCPXHWf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "\n",
        "model_chat = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"auto\")\n",
        "tokenizer_chat = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "device = model_chat.device # puts the model in the best available unit. If GPU available it will be GPU, otherwise CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvRg3o2-SAal"
      },
      "outputs": [],
      "source": [
        "# set the few-shot prompt. The 'system' message given instructions on what to do. The examples are provided as pairs of 'user' and 'ai' messages.\n",
        "# the prompt ends with the requested user question.\n",
        "prompt_few_shot = [\n",
        "    { \"role\": \"system\", \"content\": \"You are a helpful assistant that classifies an input text as having either positive or negative sentiment.\\n Use the following examples as a guide.\"},\n",
        "    { \"role\": \"user\", \"content\": \"Today is such a beatiful day, the sky is blue and the temperature is warm.\"},\n",
        "    { \"role\": \"ai\", \"content\": \"positive\"},\n",
        "    { \"role\": \"user\", \"content\": \"This is the worst match AS Lazio has ever played\"},\n",
        "    { \"role\": \"ai\", \"content\": \"negative\"},\n",
        "    { \"role\": \"user\", \"content\": \"The cake that he prepared yesterday was yummie!\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WSpAxfIcR6w"
      },
      "outputs": [],
      "source": [
        " messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "       {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "       {\"role\": \"assistant\", \"content\": \"The LA Dodgers won the World Series in 2020.\"},\n",
        "       {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "   ]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\" : \"Summarize this message in max 10 words.\"},\n",
        "    {\"role\": \"user\", \"content\" : \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter. When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows, and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"},\n",
        "    {\"role\": \"assistant\", \"content\" : \"Jupiter is the largest planet and very bright in the sky.\"}\n",
        "  ]\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\" : \"Complete the following code.\"},\n",
        "    {\"role\": \"user\", \"content\" : \"def fibonacci(num):\"}\n",
        "    {\"role\": \"assistant\", \"content\" : \"if num <= 0:\\n    return []\\nelif num == 1:\\n    return [0]\\nelif num == 2:\\n    return [0, 1]\\nelse:\\n    fib_list = [0, 1]\\n    for i in range(2, num):\\n        fib_list.append(fib_list[i - 1] + fib_list[i - 2])\\n    return fib_list\"},\n",
        "  ]\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\" : \"You translate English to Italian.\"},\n",
        "      {\"role\": \"user\", \"content\" : \"Today is a beautiful day.\"},\n",
        "      {\"role\": \"assistant\", \"content\" : \"Oggi e' una bella giornata.\"},\n",
        "\n",
        "\n",
        "messages = [\n",
        "      {\"role\": \"system\", \"content\" : \"You translate corporate jargon into plain English.\"},\n",
        "      {\"role\": \"user\", \"content\" : \"New synergies will help drive top-line growth.\"},\n",
        "      {\"role\": \"assistant\", \"content\" : \"Working well together will make more money.\"},\n",
        "      {\"role\": \"user\", \"content\" : \"Let’s circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "      {\"role\": \"assistant\", \"content\" : \"When we’re less busy, let’s talk about how to do better.\"},\n",
        "      {\"role\": \"user\", \"content\" : \"LThis late pivot means we don’t have time to boil the ocean for the client deliverable.\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"This sudden change in direction means we don't have enough time to do everything for the client's request.\"}\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
        "]\n",
        "\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n        {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n        {{- '\\\"parameters\\\": ' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \\\"}\\\" }}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFNgzRgfaQ7k"
      },
      "outputs": [],
      "source": [
        "# tokenize our few-shot prompt\n",
        "# docs: https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L1530\n",
        "input_ids = tokenizer_chat.apply_chat_template(\n",
        "    conversation=prompt_few_shot,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        "    ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bEJSUn9c_CJ"
      },
      "outputs": [],
      "source": [
        "# show the prompt format used by Llama for chat conversations\n",
        "tokenizer_chat.decode(input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtfrnZWWdFb5"
      },
      "outputs": [],
      "source": [
        "# ask the model to generate the sentiment of the last text entered in the prompt\n",
        "generated_ids = model_chat.generate(input_ids, max_new_tokens=10)\n",
        "generated_text = tokenizer_chat.decode(generated_ids[0])\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_mhhYJyRXqKO",
        "S1xVR0kyX2b8",
        "3Ms1NKZ1YznC",
        "ep00j1zEZ2IE",
        "ytMI7s3xcvuy",
        "bHEwOBX4eMVT",
        "fMUdX4k-0uiz",
        "dqoDpdIMMlQ2",
        "FWRj9FrC25Cb",
        "ZjHFJFxUW2sc",
        "foK8JoJsE6x_",
        "6FhV2wUpW7Xj"
      ],
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyOZ3YNDrJ3yPbcMaHAEKVWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}